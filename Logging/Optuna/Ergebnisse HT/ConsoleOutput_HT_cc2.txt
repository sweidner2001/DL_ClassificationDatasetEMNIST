nohup: Eingabe wird ignoriert
2025-06-24 19:29:37.259485: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:477] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered
WARNING: All log messages before absl::InitializeLog() is called are written to STDERR
E0000 00:00:1750786177.275573  285709 cuda_dnn.cc:8310] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered
E0000 00:00:1750786177.280539  285709 cuda_blas.cc:1418] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered
2025-06-24 19:29:37.297278: I tensorflow/core/platform/cpu_feature_guard.cc:210] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.
To enable the following instructions: AVX2 FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.
mean: [0.485, 0.456, 0.406], std: [0.229, 0.224, 0.225]
Länge: 697932
---rebalance_test_from_train()---
100%|██████████| 36/36 [00:00<00:00, 1384.59it/s]
100%|██████████| 36/36 [00:00<00:00, 6231.74it/s]
Prepare Test-Dataset: 100%|██████████| 36/36 [00:00<00:00, 367.95label/s]
Klasse 0: Test 1000 - original Test 5778
Klasse 1: Test 1000 - original Test 6330
Klasse 2: Test 1000 - original Test 5869
Klasse 3: Test 1000 - original Test 5969
Klasse 4: Test 1000 - original Test 5619
Klasse 5: Test 1000 - original Test 5190
Klasse 6: Test 1000 - original Test 5705
Klasse 7: Test 1000 - original Test 6139
Klasse 8: Test 1000 - original Test 5633
Klasse 9: Test 1000 - original Test 5686
Klasse A: Test 1000 - original Test 1062
Klasse B: Test 648 - Train 352
Klasse C: Test 1000 - original Test 1739
Klasse D: Test 779 - Train 221
Klasse E: Test 851 - Train 149
Klasse F: Test 1000 - original Test 1440
Klasse G: Test 447 - Train 553
Klasse H: Test 521 - Train 479
Klasse I: Test 1000 - original Test 2048
Klasse J: Test 626 - Train 374
Klasse K: Test 382 - Train 618
Klasse L: Test 810 - Train 190
Klasse M: Test 1000 - original Test 1485
Klasse a: Test 1000 - original Test 1644
Klasse b: Test 853 - Train 147
Klasse c: Test 432 - Train 568
Klasse d: Test 1000 - original Test 1683
Klasse e: Test 1000 - original Test 4092
Klasse f: Test 400 - Train 600
Klasse g: Test 589 - Train 411
Klasse h: Test 1000 - original Test 1479
Klasse i: Test 427 - Train 573
Klasse j: Test 317 - Train 683
Klasse k: Test 466 - Train 534
Klasse l: Test 1000 - original Test 2535
Klasse m: Test 464 - Train 536
Länge danch: 507942
100%|██████████| 36/36 [00:00<00:00, 2278.69it/s]
Prepare Train-Dataset: 100%|██████████| 36/36 [00:00<00:00, 83.73label/s]
[I 2025-06-24 19:29:40,360] A new study created in RDB with name: HyperparameterTuning_1
/home/c611/.local/lib/python3.11/site-packages/optuna/trial/_trial.py:682: RuntimeWarning: Inconsistent parameter values for distribution with name "sched_factor"! This might be a configuration mistake. Optuna allows to call the same distribution with the same name more than once in a trial. When the parameter values are inconsistent optuna only uses the values of the first call and ignores all following. Using these values: {'step': None, 'low': 0.0, 'high': 0.4, 'log': False}
  warnings.warn(
Label 0: Train 5000 orig 5000
Trainingsdaten nach Label 0: 5000
Label 1: Train 5000 orig 5000
Trainingsdaten nach Label 1: 10000
Label 2: Train 5000 orig 5000
Trainingsdaten nach Label 2: 15000
Label 3: Train 5000 orig 5000
Trainingsdaten nach Label 3: 20000
Label 4: Train 5000 orig 5000
Trainingsdaten nach Label 4: 25000
Label 5: Train 5000 orig 5000
Trainingsdaten nach Label 5: 30000
Label 6: Train 5000 orig 5000
Trainingsdaten nach Label 6: 35000
Label 7: Train 5000 orig 5000
Trainingsdaten nach Label 7: 40000
Label 8: Train 5000 orig 5000
Trainingsdaten nach Label 8: 45000
Label 9: Train 5000 orig 5000
Trainingsdaten nach Label 9: 50000
Label A: Train 5000 orig 5000
Trainingsdaten nach Label A: 55000
Label B: Train 3526 augmentated 1474
Trainingsdaten nach Label B: 60000
Label C: Train 5000 orig 5000
Trainingsdaten nach Label C: 65000
Label D: Train 4341 augmentated 659
Trainingsdaten nach Label D: 70000
Label E: Train 4785 augmentated 215
Trainingsdaten nach Label E: 75000
Label F: Train 5000 orig 5000
Trainingsdaten nach Label F: 80000
Label G: Train 1964 augmentated 3036
Trainingsdaten nach Label G: 85000
Label H: Train 2673 augmentated 2327
Trainingsdaten nach Label H: 90000
Label I: Train 5000 orig 5000
Trainingsdaten nach Label I: 95000
Label J: Train 3388 augmentated 1612
Trainingsdaten nach Label J: 100000
Label K: Train 1850 augmentated 3150
Trainingsdaten nach Label K: 105000
Label L: Train 4886 augmentated 114
Trainingsdaten nach Label L: 110000
Label M: Train 5000 orig 5000
Trainingsdaten nach Label M: 115000
Label a: Train 5000 orig 5000
Trainingsdaten nach Label a: 120000
Label b: Train 5000 orig 5000
Trainingsdaten nach Label b: 125000
Label c: Train 2286 augmentated 2714
Trainingsdaten nach Label c: 130000
Label d: Train 5000 orig 5000
Trainingsdaten nach Label d: 135000
Label e: Train 5000 orig 5000
Trainingsdaten nach Label e: 140000
Label f: Train 1961 augmentated 3039
Trainingsdaten nach Label f: 145000
Label g: Train 3276 augmentated 1724
Trainingsdaten nach Label g: 150000
Label h: Train 5000 orig 5000
Trainingsdaten nach Label h: 155000
Label i: Train 2152 augmentated 2848
Trainingsdaten nach Label i: 160000
Label j: Train 1213 augmentated 3787
Trainingsdaten nach Label j: 165000
Label k: Train 1957 augmentated 3043
Trainingsdaten nach Label k: 170000
Label l: Train 5000 orig 5000
Trainingsdaten nach Label l: 175000
Label m: Train 2109 augmentated 2891
Trainingsdaten nach Label m: 180000
{'lr': 0.006879819457145932, 'weight_decay': 0.004818126873617189, 'num_epochs': 20, 'batch_size': 32, 'only_train_classifier': False, 'sched_patience': 4, 'sched_factor': 0.12713634978376773, 'sched_min_lr': 1e-05, 'train_with_CombinedClassifier': True, 'probabilities': [0.19291648011977414, 0.9819545733301013, 0.6329866539512424], 'config_CombinedClassifier': {'linear_out_1': 512, 'linear_out_2': 256, 'dropout': 0.12713634978376773}}


-------------- Start New Training: ---------------


Epoch 1/20: 100%|██████████| 4500/4500 [01:51<00:00, 40.45batch/s]

Model validation...
100%|██████████| 1125/1125 [00:23<00:00, 47.79batch/s]
top1_score    0.030489
top3_score    0.083763
top5_score    0.140581
dtype: float64
valid_loss:  3.5835189819335938
Learning rate after epoch 1: [0.006879819457145932]
Epoch 2/20: 100%|██████████| 4500/4500 [01:48<00:00, 41.53batch/s]

Model validation...
100%|██████████| 1125/1125 [00:23<00:00, 47.86batch/s]
top1_score    0.032971
top3_score    0.087449
top5_score    0.143808
dtype: float64
valid_loss:  3.5835189819335938
Learning rate after epoch 2: [0.006879819457145932]
Epoch 3/20: 100%|██████████| 4500/4500 [01:50<00:00, 40.58batch/s]

Model validation...
100%|██████████| 1125/1125 [00:23<00:00, 47.95batch/s]
top1_score    0.027975
top3_score    0.080536
top5_score    0.134144
dtype: float64
valid_loss:  3.5835189819335938
Learning rate after epoch 3: [0.006879819457145932]
Epoch 4/20: 100%|██████████| 4500/4500 [01:50<00:00, 40.58batch/s]

Model validation...
100%|██████████| 1125/1125 [00:23<00:00, 47.84batch/s]
top1_score    0.027556
top3_score    0.083113
top5_score    0.138899
dtype: float64
valid_loss:  3.5835189819335938
Learning rate after epoch 4: [0.006879819457145932]
Epoch 5/20: 100%|██████████| 4500/4500 [01:48<00:00, 41.32batch/s]

Model validation...
100%|██████████| 1125/1125 [00:23<00:00, 47.89batch/s]
top1_score    0.027778
top3_score    0.083333
top5_score    0.138889
dtype: float64
valid_loss:  3.5835189819335938
Learning rate after epoch 5: [0.006879819457145932]
Epoch 6/20: 100%|██████████| 4500/4500 [01:50<00:00, 40.58batch/s]

Model validation...
100%|██████████| 1125/1125 [00:23<00:00, 47.77batch/s]
top1_score    0.027778
top3_score    0.083333
top5_score    0.138889
dtype: float64
valid_loss:  3.5835189819335938
Learning rate after epoch 6: [0.0008746751329528762]
Epoch 7/20: 100%|██████████| 4500/4500 [01:50<00:00, 40.59batch/s]

Model validation...
100%|██████████| 1125/1125 [00:23<00:00, 47.89batch/s]
top1_score    0.027778
top3_score    0.083333
top5_score    0.138889
dtype: float64
valid_loss:  3.5835189819335938
Learning rate after epoch 7: [0.0008746751329528762]
Epoch 8/20: 100%|██████████| 4500/4500 [01:50<00:00, 40.77batch/s]

Model validation...
100%|██████████| 1125/1125 [00:22<00:00, 49.54batch/s]
top1_score    0.027778
top3_score    0.083333
top5_score    0.138889
dtype: float64
valid_loss:  3.5835189819335938
Learning rate after epoch 8: [0.0008746751329528762]
Epoch 9/20: 100%|██████████| 4500/4500 [01:50<00:00, 40.58batch/s]

Model validation...
100%|██████████| 1125/1125 [00:23<00:00, 48.27batch/s]
top1_score    0.027778
top3_score    0.083333
top5_score    0.138889
dtype: float64
valid_loss:  3.5835189819335938
Learning rate after epoch 9: [0.0008746751329528762]
Epoch 10/20: 100%|██████████| 4500/4500 [01:50<00:00, 40.57batch/s]

Model validation...
100%|██████████| 1125/1125 [00:23<00:00, 47.77batch/s]
top1_score    0.027778
top3_score    0.083333
top5_score    0.138889
dtype: float64
valid_loss:  3.5835189819335938
Learning rate after epoch 10: [0.0008746751329528762]
Epoch 11/20: 100%|██████████| 4500/4500 [01:50<00:00, 40.56batch/s]

Model validation...
100%|██████████| 1125/1125 [00:23<00:00, 47.99batch/s]
top1_score    0.027778
top3_score    0.083333
top5_score    0.138889
dtype: float64
valid_loss:  3.5835189819335938
Learning rate after epoch 11: [0.00011120300365026042]
Epoch 12/20: 100%|██████████| 4500/4500 [01:48<00:00, 41.40batch/s]

Model validation...
100%|██████████| 1125/1125 [00:23<00:00, 47.79batch/s]
top1_score    0.027778
top3_score    0.083333
top5_score    0.138889
dtype: float64
valid_loss:  3.5835189819335938
Learning rate after epoch 12: [0.00011120300365026042]
Epoch 13/20: 100%|██████████| 4500/4500 [01:50<00:00, 40.56batch/s]

Model validation...
100%|██████████| 1125/1125 [00:23<00:00, 47.72batch/s]
top1_score    0.027778
top3_score    0.083333
top5_score    0.138889
dtype: float64
valid_loss:  3.5835189819335938
Learning rate after epoch 13: [0.00011120300365026042]
Epoch 14/20: 100%|██████████| 4500/4500 [01:50<00:00, 40.56batch/s]

Model validation...
100%|██████████| 1125/1125 [00:23<00:00, 48.00batch/s]
top1_score    0.027778
top3_score    0.083333
top5_score    0.138889
dtype: float64
valid_loss:  3.5835189819335938
Learning rate after epoch 14: [0.00011120300365026042]
Epoch 15/20: 100%|██████████| 4500/4500 [01:48<00:00, 41.35batch/s]

Model validation...
100%|██████████| 1125/1125 [00:23<00:00, 48.14batch/s]
top1_score    0.027778
top3_score    0.083333
top5_score    0.138889
dtype: float64
valid_loss:  3.5835189819335938
Learning rate after epoch 15: [0.00011120300365026042]
Epoch 16/20: 100%|██████████| 4500/4500 [01:50<00:00, 40.56batch/s]

Model validation...
100%|██████████| 1125/1125 [00:23<00:00, 47.86batch/s]
top1_score    0.027778
top3_score    0.083333
top5_score    0.138889
dtype: float64
valid_loss:  3.5835189819335938
Learning rate after epoch 16: [1.4137943969085108e-05]
Epoch 17/20: 100%|██████████| 4500/4500 [01:50<00:00, 40.56batch/s]

Model validation...
100%|██████████| 1125/1125 [00:23<00:00, 47.71batch/s]
top1_score    0.027778
top3_score    0.083333
top5_score    0.138889
dtype: float64
valid_loss:  3.5835189819335938
Learning rate after epoch 17: [1.4137943969085108e-05]
Epoch 18/20: 100%|██████████| 4500/4500 [01:49<00:00, 40.97batch/s]

Model validation...
100%|██████████| 1125/1125 [00:23<00:00, 48.63batch/s]
top1_score    0.027778
top3_score    0.083333
top5_score    0.138889
dtype: float64
valid_loss:  3.5835189819335938
Learning rate after epoch 18: [1.4137943969085108e-05]
Epoch 19/20: 100%|██████████| 4500/4500 [01:50<00:00, 40.55batch/s]

Model validation...
100%|██████████| 1125/1125 [00:23<00:00, 47.91batch/s]
top1_score    0.027778
top3_score    0.083333
top5_score    0.138889
dtype: float64
valid_loss:  3.5835189819335938
Learning rate after epoch 19: [1.4137943969085108e-05]
Epoch 20/20: 100%|██████████| 4500/4500 [01:50<00:00, 40.56batch/s]

Model validation...
100%|██████████| 1125/1125 [00:23<00:00, 47.78batch/s]
[I 2025-06-24 20:14:19,335] Trial 0 finished with value: 0.03297093138098717 and parameters: {'linear_out_1': 512, 'linear_out_2': 256, 'sched_factor': 0.12713634978376773, 'lr': 0.006879819457145932, 'weight_decay': 0.004818126873617189, 'batch_size': 32, 'sched_patience': 4, 'sched_min_lr': 1e-05, 'a1': 0.19291648011977414, 'a2': 0.9819545733301013, 'a3': 0.6329866539512424}. Best is trial 0 with value: 0.03297093138098717.
/home/c611/.local/lib/python3.11/site-packages/optuna/trial/_trial.py:682: RuntimeWarning: Inconsistent parameter values for distribution with name "sched_factor"! This might be a configuration mistake. Optuna allows to call the same distribution with the same name more than once in a trial. When the parameter values are inconsistent optuna only uses the values of the first call and ignores all following. Using these values: {'step': None, 'low': 0.0, 'high': 0.4, 'log': False}
  warnings.warn(
top1_score    0.027778
top3_score    0.083333
top5_score    0.138889
dtype: float64
valid_loss:  3.5835189819335938
Learning rate after epoch 20: [1.4137943969085108e-05]
Training finished!
{'lr': 0.0012148886987550394, 'weight_decay': 0.0018666350100419351, 'num_epochs': 20, 'batch_size': 16, 'only_train_classifier': False, 'sched_patience': 5, 'sched_factor': 0.18750219795719927, 'sched_min_lr': 1e-06, 'train_with_CombinedClassifier': True, 'probabilities': [0.9082413069037405, 0.7928377382319802, 0.66311201938152], 'config_CombinedClassifier': {'linear_out_1': 512, 'linear_out_2': 64, 'dropout': 0.18750219795719927}}


-------------- Start New Training: ---------------


Epoch 1/20: 100%|██████████| 9000/9000 [02:49<00:00, 53.14batch/s]

Model validation...
100%|██████████| 2250/2250 [00:38<00:00, 58.20batch/s]
top1_score    0.034691
top3_score    0.081984
top5_score    0.138435
dtype: float64
valid_loss:  3.583516455438402
Learning rate after epoch 1: [0.0012148886987550394]
Epoch 2/20: 100%|██████████| 9000/9000 [02:52<00:00, 52.27batch/s]

Model validation...
100%|██████████| 2250/2250 [00:39<00:00, 56.89batch/s]
top1_score    0.027696
top3_score    0.083066
top5_score    0.139850
dtype: float64
valid_loss:  3.5835189819335938
Learning rate after epoch 2: [0.0012148886987550394]
Epoch 3/20: 100%|██████████| 9000/9000 [02:14<00:00, 66.75batch/s]

Model validation...
100%|██████████| 2250/2250 [00:14<00:00, 151.03batch/s]
top1_score    0.027668
top3_score    0.083039
top5_score    0.140019
dtype: float64
valid_loss:  3.5835189819335938
Learning rate after epoch 3: [0.0012148886987550394]
Epoch 4/20: 100%|██████████| 9000/9000 [01:31<00:00, 98.60batch/s]

Model validation...
100%|██████████| 2250/2250 [00:14<00:00, 151.92batch/s]
top1_score    0.027705
top3_score    0.082841
top5_score    0.140214
dtype: float64
valid_loss:  3.5835189819335938
Learning rate after epoch 4: [0.0012148886987550394]
Epoch 5/20: 100%|██████████| 9000/9000 [01:31<00:00, 98.43batch/s]

Model validation...
100%|██████████| 2250/2250 [00:15<00:00, 148.72batch/s]
top1_score    0.027592
top3_score    0.083141
top5_score    0.138827
dtype: float64
valid_loss:  3.5835189819335938
Learning rate after epoch 5: [0.0012148886987550394]
Epoch 6/20: 100%|██████████| 9000/9000 [01:31<00:00, 98.21batch/s]

Model validation...
100%|██████████| 2250/2250 [00:15<00:00, 147.41batch/s]
top1_score    0.027890
top3_score    0.083328
top5_score    0.139110
dtype: float64
valid_loss:  3.583518983417087
Learning rate after epoch 6: [0.0012148886987550394]
Epoch 7/20: 100%|██████████| 9000/9000 [01:32<00:00, 97.40batch/s]

Model validation...
100%|██████████| 2250/2250 [00:15<00:00, 149.35batch/s]
top1_score    0.028142
top3_score    0.089306
top5_score    0.147546
dtype: float64
valid_loss:  3.583518917295668
Learning rate after epoch 7: [0.00022779430128993164]
Epoch 8/20: 100%|██████████| 9000/9000 [01:32<00:00, 97.43batch/s]

Model validation...
100%|██████████| 2250/2250 [00:14<00:00, 152.25batch/s]
top1_score    0.056125
top3_score    0.087361
top5_score    0.145750
dtype: float64
valid_loss:  3.5377802040312027
Learning rate after epoch 8: [0.00022779430128993164]
Epoch 9/20: 100%|██████████| 9000/9000 [01:32<00:00, 97.81batch/s]

Model validation...
100%|██████████| 2250/2250 [00:14<00:00, 151.23batch/s]
top1_score    0.086313
top3_score    0.112279
top5_score    0.149611
dtype: float64
valid_loss:  3.5132956341637507
Learning rate after epoch 9: [0.00022779430128993164]
Epoch 10/20: 100%|██████████| 9000/9000 [01:31<00:00, 98.32batch/s]

Model validation...
100%|██████████| 2250/2250 [00:15<00:00, 149.93batch/s]
top1_score    0.091035
top3_score    0.118517
top5_score    0.144742
dtype: float64
valid_loss:  3.5120421019660104
Learning rate after epoch 10: [0.00022779430128993164]
Epoch 11/20: 100%|██████████| 9000/9000 [01:31<00:00, 98.24batch/s]

Model validation...
100%|██████████| 2250/2250 [00:14<00:00, 151.64batch/s]
top1_score    0.118448
top3_score    0.159523
top5_score    0.175959
dtype: float64
valid_loss:  3.4878693721559313
Learning rate after epoch 11: [0.00022779430128993164]
Epoch 12/20: 100%|██████████| 9000/9000 [01:31<00:00, 98.46batch/s]

Model validation...
100%|██████████| 2250/2250 [00:14<00:00, 152.56batch/s]
top1_score    0.128856
top3_score    0.158146
top5_score    0.191746
dtype: float64
valid_loss:  3.4879045004314846
Learning rate after epoch 12: [0.00022779430128993164]
Epoch 13/20: 100%|██████████| 9000/9000 [01:32<00:00, 97.62batch/s]

Model validation...
100%|██████████| 2250/2250 [00:14<00:00, 150.93batch/s]
top1_score    0.122423
top3_score    0.172250
top5_score    0.208711
dtype: float64
valid_loss:  3.4864309203889636
Learning rate after epoch 13: [0.00022779430128993164]
Epoch 14/20: 100%|██████████| 9000/9000 [01:31<00:00, 98.56batch/s]

Model validation...
100%|██████████| 2250/2250 [00:14<00:00, 150.13batch/s]
top1_score    0.127121
top3_score    0.183312
top5_score    0.226595
dtype: float64
valid_loss:  3.4869386501312256
Learning rate after epoch 14: [0.00022779430128993164]
Epoch 15/20: 100%|██████████| 9000/9000 [01:33<00:00, 96.53batch/s]

Model validation...
100%|██████████| 2250/2250 [00:14<00:00, 152.57batch/s]
top1_score    0.131812
top3_score    0.191082
top5_score    0.245400
dtype: float64
valid_loss:  3.486153075006273
Learning rate after epoch 15: [0.00022779430128993164]
Epoch 16/20: 100%|██████████| 9000/9000 [01:33<00:00, 96.77batch/s]

Model validation...
100%|██████████| 2250/2250 [00:14<00:00, 151.73batch/s]
top1_score    0.138411
top3_score    0.192970
top5_score    0.239351
dtype: float64
valid_loss:  3.486629859076606
Learning rate after epoch 16: [0.00022779430128993164]
Epoch 17/20: 100%|██████████| 9000/9000 [01:31<00:00, 98.06batch/s]

Model validation...
100%|██████████| 2250/2250 [00:14<00:00, 152.02batch/s]
top1_score    0.287563
top3_score    0.301136
top5_score    0.306304
dtype: float64
valid_loss:  3.3287884328630235
Learning rate after epoch 17: [0.00022779430128993164]
Epoch 18/20: 100%|██████████| 9000/9000 [01:31<00:00, 98.34batch/s]

Model validation...
100%|██████████| 2250/2250 [00:15<00:00, 149.58batch/s]
top1_score    0.321877
top3_score    0.327609
top5_score    0.332276
dtype: float64
valid_loss:  3.293772733264499
Learning rate after epoch 18: [0.00022779430128993164]
Epoch 19/20: 100%|██████████| 9000/9000 [01:32<00:00, 97.57batch/s]

Model validation...
100%|██████████| 2250/2250 [00:14<00:00, 151.80batch/s]
top1_score    0.324257
top3_score    0.333576
top5_score    0.339432
dtype: float64
valid_loss:  3.2922596175935532
Learning rate after epoch 19: [0.00022779430128993164]
Epoch 20/20: 100%|██████████| 9000/9000 [01:31<00:00, 98.29batch/s]

Model validation...
100%|██████████| 2250/2250 [00:14<00:00, 151.55batch/s]
[I 2025-06-24 20:54:05,896] Trial 1 finished with value: 0.33892202377319336 and parameters: {'linear_out_1': 512, 'linear_out_2': 64, 'sched_factor': 0.18750219795719927, 'lr': 0.0012148886987550394, 'weight_decay': 0.0018666350100419351, 'batch_size': 16, 'sched_patience': 5, 'sched_min_lr': 1e-06, 'a1': 0.9082413069037405, 'a2': 0.7928377382319802, 'a3': 0.66311201938152}. Best is trial 1 with value: 0.33892202377319336.
/home/c611/.local/lib/python3.11/site-packages/optuna/trial/_trial.py:682: RuntimeWarning: Inconsistent parameter values for distribution with name "sched_factor"! This might be a configuration mistake. Optuna allows to call the same distribution with the same name more than once in a trial. When the parameter values are inconsistent optuna only uses the values of the first call and ignores all following. Using these values: {'step': None, 'low': 0.0, 'high': 0.4, 'log': False}
  warnings.warn(
top1_score    0.338922
top3_score    0.345050
top5_score    0.348386
dtype: float64
valid_loss:  3.2758892839219835
Learning rate after epoch 20: [0.00022779430128993164]
Training finished!
{'lr': 0.003052471219587521, 'weight_decay': 0.009479996458085239, 'num_epochs': 20, 'batch_size': 16, 'only_train_classifier': False, 'sched_patience': 5, 'sched_factor': 0.2200750634561533, 'sched_min_lr': 1e-06, 'train_with_CombinedClassifier': True, 'probabilities': [0.3907110414655486, 0.5258118447517279, 0.5096103020897927], 'config_CombinedClassifier': {'linear_out_1': 128, 'linear_out_2': 128, 'dropout': 0.2200750634561533}}


-------------- Start New Training: ---------------


Epoch 1/20: 100%|██████████| 9000/9000 [01:33<00:00, 96.03batch/s]

Model validation...
100%|██████████| 2250/2250 [00:15<00:00, 149.80batch/s]
top1_score    0.033160
top3_score    0.087568
top5_score    0.149441
dtype: float64
valid_loss:  3.5835189819335938
Learning rate after epoch 1: [0.003052471219587521]
Epoch 2/20: 100%|██████████| 9000/9000 [01:32<00:00, 97.15batch/s]

Model validation...
100%|██████████| 2250/2250 [00:14<00:00, 151.64batch/s]
top1_score    0.026078
top3_score    0.079951
top5_score    0.138073
dtype: float64
valid_loss:  3.5835189819335938
Learning rate after epoch 2: [0.003052471219587521]
Epoch 3/20: 100%|██████████| 9000/9000 [01:33<00:00, 96.58batch/s]

Model validation...
100%|██████████| 2250/2250 [00:15<00:00, 147.79batch/s]
top1_score    0.031222
top3_score    0.086971
top5_score    0.128543
dtype: float64
valid_loss:  3.5835189645555285
Learning rate after epoch 3: [0.003052471219587521]
Epoch 4/20: 100%|██████████| 9000/9000 [01:33<00:00, 96.27batch/s]

Model validation...
100%|██████████| 2250/2250 [00:14<00:00, 151.27batch/s]
top1_score    0.022931
top3_score    0.088148
top5_score    0.131480
dtype: float64
valid_loss:  3.5835189446343314
Learning rate after epoch 4: [0.003052471219587521]
Epoch 5/20: 100%|██████████| 9000/9000 [01:33<00:00, 96.25batch/s]

Model validation...
100%|██████████| 2250/2250 [00:14<00:00, 152.83batch/s]
top1_score    0.040634
top3_score    0.100651
top5_score    0.137224
dtype: float64
valid_loss:  3.5835189819335938
Learning rate after epoch 5: [0.003052471219587521]
Epoch 6/20: 100%|██████████| 9000/9000 [01:32<00:00, 96.86batch/s]

Model validation...
100%|██████████| 2250/2250 [00:14<00:00, 151.32batch/s]
top1_score    0.009663
top3_score    0.068717
top5_score    0.129360
dtype: float64
valid_loss:  3.5835189819335938
Learning rate after epoch 6: [0.003052471219587521]
Epoch 7/20: 100%|██████████| 9000/9000 [01:31<00:00, 97.87batch/s]

Model validation...
100%|██████████| 2250/2250 [00:14<00:00, 150.24batch/s]
top1_score    0.026918
top3_score    0.088974
top5_score    0.140370
dtype: float64
valid_loss:  3.5835189819335938
Learning rate after epoch 7: [0.0006717727973488053]
Epoch 8/20: 100%|██████████| 9000/9000 [01:33<00:00, 95.98batch/s]

Model validation...
100%|██████████| 2250/2250 [00:14<00:00, 151.80batch/s]
top1_score    0.028370
top3_score    0.066006
top5_score    0.122758
dtype: float64
valid_loss:  3.5835189819335938
Learning rate after epoch 8: [0.0006717727973488053]
Epoch 9/20: 100%|██████████| 9000/9000 [01:32<00:00, 97.21batch/s]

Model validation...
100%|██████████| 2250/2250 [00:14<00:00, 151.85batch/s]
top1_score    0.044696
top3_score    0.083723
top5_score    0.139144
dtype: float64
valid_loss:  3.5524296871821086
Learning rate after epoch 9: [0.0006717727973488053]
Epoch 10/20: 100%|██████████| 9000/9000 [01:33<00:00, 96.57batch/s]

Model validation...
100%|██████████| 2250/2250 [00:15<00:00, 147.59batch/s]
top1_score    0.077621
top3_score    0.097627
top5_score    0.138915
dtype: float64
valid_loss:  3.5227133567598132
Learning rate after epoch 10: [0.0006717727973488053]
Epoch 11/20: 100%|██████████| 9000/9000 [01:33<00:00, 95.84batch/s]

Model validation...
100%|██████████| 2250/2250 [00:14<00:00, 150.64batch/s]
top1_score    0.108343
top3_score    0.110890
top5_score    0.149402
dtype: float64
valid_loss:  3.486244380315145
Learning rate after epoch 11: [0.0006717727973488053]
Epoch 12/20: 100%|██████████| 9000/9000 [01:33<00:00, 96.17batch/s]

Model validation...
100%|██████████| 2250/2250 [00:15<00:00, 149.94batch/s]
top1_score    0.107569
top3_score    0.111032
top5_score    0.170299
dtype: float64
valid_loss:  3.4861654619640774
Learning rate after epoch 12: [0.0006717727973488053]
Epoch 13/20: 100%|██████████| 9000/9000 [01:33<00:00, 96.29batch/s]

Model validation...
100%|██████████| 2250/2250 [00:14<00:00, 151.62batch/s]
top1_score    0.108539
top3_score    0.116756
top5_score    0.166084
dtype: float64
valid_loss:  3.485102108001709
Learning rate after epoch 13: [0.0006717727973488053]
Epoch 14/20: 100%|██████████| 9000/9000 [01:35<00:00, 94.67batch/s]

Model validation...
100%|██████████| 2250/2250 [00:15<00:00, 148.69batch/s]
top1_score    0.108659
top3_score    0.111564
top5_score    0.173468
dtype: float64
valid_loss:  3.4849774278004966
Learning rate after epoch 14: [0.0006717727973488053]
Epoch 15/20: 100%|██████████| 9000/9000 [01:32<00:00, 97.20batch/s]

Model validation...
100%|██████████| 2250/2250 [00:14<00:00, 150.73batch/s]
top1_score    0.108430
top3_score    0.127551
top5_score    0.182270
dtype: float64
valid_loss:  3.4853116930855643
Learning rate after epoch 15: [0.0006717727973488053]
Epoch 16/20: 100%|██████████| 9000/9000 [01:32<00:00, 97.64batch/s]

Model validation...
100%|██████████| 2250/2250 [00:15<00:00, 148.39batch/s]
top1_score    0.110064
top3_score    0.140215
top5_score    0.189065
dtype: float64
valid_loss:  3.4844245187971326
Learning rate after epoch 16: [0.0006717727973488053]
Epoch 17/20: 100%|██████████| 9000/9000 [01:33<00:00, 96.01batch/s]

Model validation...
100%|██████████| 2250/2250 [00:14<00:00, 151.71batch/s]
top1_score    0.107885
top3_score    0.140019
top5_score    0.181601
dtype: float64
valid_loss:  3.4862828922271727
Learning rate after epoch 17: [0.0006717727973488053]
Epoch 18/20: 100%|██████████| 9000/9000 [01:32<00:00, 97.03batch/s]

Model validation...
100%|██████████| 2250/2250 [00:14<00:00, 150.88batch/s]
top1_score    0.109273
top3_score    0.131051
top5_score    0.169133
dtype: float64
valid_loss:  3.4845193083021377
Learning rate after epoch 18: [0.0006717727973488053]
Epoch 19/20: 100%|██████████| 9000/9000 [01:33<00:00, 96.73batch/s]

Model validation...
100%|██████████| 2250/2250 [00:15<00:00, 149.11batch/s]
top1_score    0.128951
top3_score    0.186201
top5_score    0.222248
dtype: float64
valid_loss:  3.484751835823059
Learning rate after epoch 19: [0.0006717727973488053]
Epoch 20/20: 100%|██████████| 9000/9000 [01:33<00:00, 96.72batch/s]

Model validation...
100%|██████████| 2250/2250 [00:14<00:00, 151.50batch/s]
[I 2025-06-24 21:30:10,102] Trial 2 finished with value: 0.1321241557598114 and parameters: {'linear_out_1': 128, 'linear_out_2': 128, 'sched_factor': 0.2200750634561533, 'lr': 0.003052471219587521, 'weight_decay': 0.009479996458085239, 'batch_size': 16, 'sched_patience': 5, 'sched_min_lr': 1e-06, 'a1': 0.3907110414655486, 'a2': 0.5258118447517279, 'a3': 0.5096103020897927}. Best is trial 1 with value: 0.33892202377319336.
/home/c611/.local/lib/python3.11/site-packages/optuna/trial/_trial.py:682: RuntimeWarning: Inconsistent parameter values for distribution with name "sched_factor"! This might be a configuration mistake. Optuna allows to call the same distribution with the same name more than once in a trial. When the parameter values are inconsistent optuna only uses the values of the first call and ignores all following. Using these values: {'step': None, 'low': 0.0, 'high': 0.4, 'log': False}
  warnings.warn(
top1_score    0.132124
top3_score    0.191549
top5_score    0.246279
dtype: float64
valid_loss:  3.484101428985596
Learning rate after epoch 20: [0.0006717727973488053]
Training finished!
{'lr': 2.4055482956377946e-05, 'weight_decay': 0.0003594967842856646, 'num_epochs': 20, 'batch_size': 64, 'only_train_classifier': False, 'sched_patience': 5, 'sched_factor': 0.286353050346915, 'sched_min_lr': 1e-06, 'train_with_CombinedClassifier': True, 'probabilities': [0.5382997848840297, 0.6877996879862077, 0.7444415699075089], 'config_CombinedClassifier': {'linear_out_1': 256, 'linear_out_2': 256, 'dropout': 0.286353050346915}}


-------------- Start New Training: ---------------


Epoch 1/20: 100%|██████████| 2250/2250 [00:43<00:00, 51.31batch/s]

Model validation...
100%|██████████| 563/563 [00:09<00:00, 59.63batch/s]
top1_score    0.705495
top3_score    0.721576
top5_score    0.725362
dtype: float64
valid_loss:  2.930889350694619
Learning rate after epoch 1: [2.4055482956377946e-05]
Epoch 2/20: 100%|██████████| 2250/2250 [00:43<00:00, 51.90batch/s]

Model validation...
100%|██████████| 563/563 [00:09<00:00, 59.62batch/s]
top1_score    0.754754
top3_score    0.770094
top5_score    0.771946
dtype: float64
valid_loss:  2.87919072952406
Learning rate after epoch 2: [2.4055482956377946e-05]
Epoch 3/20: 100%|██████████| 2250/2250 [00:43<00:00, 51.56batch/s]

Model validation...
100%|██████████| 563/563 [00:09<00:00, 59.40batch/s]
top1_score    0.758977
top3_score    0.771479
top5_score    0.773044
dtype: float64
valid_loss:  2.8737810221277376
Learning rate after epoch 3: [2.4055482956377946e-05]
Epoch 4/20: 100%|██████████| 2250/2250 [00:43<00:00, 51.66batch/s]

Model validation...
100%|██████████| 563/563 [00:09<00:00, 59.44batch/s]
top1_score    0.776991
top3_score    0.789946
top5_score    0.792661
dtype: float64
valid_loss:  2.8557757289540917
Learning rate after epoch 4: [2.4055482956377946e-05]
Epoch 5/20: 100%|██████████| 2250/2250 [00:43<00:00, 51.65batch/s]

Model validation...
100%|██████████| 563/563 [00:09<00:00, 59.76batch/s]
top1_score    0.779676
top3_score    0.793197
top5_score    0.794998
dtype: float64
valid_loss:  2.8526409400716344
Learning rate after epoch 5: [2.4055482956377946e-05]
Epoch 6/20: 100%|██████████| 2250/2250 [00:43<00:00, 51.61batch/s]

Model validation...
100%|██████████| 563/563 [00:09<00:00, 58.93batch/s]
top1_score    0.797523
top3_score    0.810638
top5_score    0.812645
dtype: float64
valid_loss:  2.8343788518787068
Learning rate after epoch 6: [2.4055482956377946e-05]
Epoch 7/20: 100%|██████████| 2250/2250 [00:43<00:00, 51.61batch/s]

Model validation...
100%|██████████| 563/563 [00:09<00:00, 59.28batch/s]
top1_score    0.799034
top3_score    0.813737
top5_score    0.816798
dtype: float64
valid_loss:  2.8343541944005772
Learning rate after epoch 7: [2.4055482956377946e-05]
Epoch 8/20: 100%|██████████| 2250/2250 [00:43<00:00, 51.39batch/s]

Model validation...
100%|██████████| 563/563 [00:09<00:00, 59.42batch/s]
top1_score    0.808370
top3_score    0.823148
top5_score    0.824944
dtype: float64
valid_loss:  2.8245455109204958
Learning rate after epoch 8: [2.4055482956377946e-05]
Epoch 9/20: 100%|██████████| 2250/2250 [00:43<00:00, 51.92batch/s]

Model validation...
100%|██████████| 563/563 [00:09<00:00, 59.12batch/s]
top1_score    0.809627
top3_score    0.823606
top5_score    0.825176
dtype: float64
valid_loss:  2.823328826186077
Learning rate after epoch 9: [2.4055482956377946e-05]
Epoch 10/20: 100%|██████████| 2250/2250 [00:43<00:00, 51.65batch/s]

Model validation...
100%|██████████| 563/563 [00:09<00:00, 59.68batch/s]
top1_score    0.811488
top3_score    0.824436
top5_score    0.826189
dtype: float64
valid_loss:  2.820462910362501
Learning rate after epoch 10: [2.4055482956377946e-05]
Epoch 11/20: 100%|██████████| 2250/2250 [00:43<00:00, 51.90batch/s]

Model validation...
100%|██████████| 563/563 [00:09<00:00, 59.53batch/s]
top1_score    0.810862
top3_score    0.822204
top5_score    0.823941
dtype: float64
valid_loss:  2.8217998457209044
Learning rate after epoch 11: [2.4055482956377946e-05]
Epoch 12/20: 100%|██████████| 2250/2250 [00:43<00:00, 51.60batch/s]

Model validation...
100%|██████████| 563/563 [00:09<00:00, 59.35batch/s]
top1_score    0.810421
top3_score    0.823121
top5_score    0.824692
dtype: float64
valid_loss:  2.8215267747164194
Learning rate after epoch 12: [2.4055482956377946e-05]
Epoch 13/20: 100%|██████████| 2250/2250 [00:43<00:00, 51.42batch/s]

Model validation...
100%|██████████| 563/563 [00:09<00:00, 59.31batch/s]
top1_score    0.812288
top3_score    0.823194
top5_score    0.824975
dtype: float64
valid_loss:  2.8193549936236963
Learning rate after epoch 13: [2.4055482956377946e-05]
Epoch 14/20: 100%|██████████| 2250/2250 [00:43<00:00, 51.23batch/s]

Model validation...
100%|██████████| 563/563 [00:09<00:00, 59.02batch/s]
top1_score    0.813480
top3_score    0.825364
top5_score    0.826734
dtype: float64
valid_loss:  2.8183654646255194
Learning rate after epoch 14: [2.4055482956377946e-05]
Epoch 15/20: 100%|██████████| 2250/2250 [00:43<00:00, 51.37batch/s]

Model validation...
100%|██████████| 563/563 [00:09<00:00, 59.43batch/s]
top1_score    0.813840
top3_score    0.824577
top5_score    0.825911
dtype: float64
valid_loss:  2.8176723674177913
Learning rate after epoch 15: [2.4055482956377946e-05]
Epoch 16/20: 100%|██████████| 2250/2250 [00:43<00:00, 51.74batch/s]

Model validation...
100%|██████████| 563/563 [00:09<00:00, 59.42batch/s]
top1_score    0.814277
top3_score    0.825084
top5_score    0.826327
dtype: float64
valid_loss:  2.8174939905136136
Learning rate after epoch 16: [2.4055482956377946e-05]
Epoch 17/20: 100%|██████████| 2250/2250 [00:43<00:00, 51.45batch/s]

Model validation...
100%|██████████| 563/563 [00:09<00:00, 58.79batch/s]
top1_score    0.816590
top3_score    0.826927
top5_score    0.828440
dtype: float64
valid_loss:  2.8149153775474316
Learning rate after epoch 17: [2.4055482956377946e-05]
Epoch 18/20: 100%|██████████| 2250/2250 [00:43<00:00, 51.34batch/s]

Model validation...
100%|██████████| 563/563 [00:09<00:00, 59.39batch/s]
top1_score    0.811832
top3_score    0.823356
top5_score    0.825066
dtype: float64
valid_loss:  2.8193200786414407
Learning rate after epoch 18: [2.4055482956377946e-05]
Epoch 19/20: 100%|██████████| 2250/2250 [00:43<00:00, 51.69batch/s]

Model validation...
100%|██████████| 563/563 [00:09<00:00, 59.31batch/s]
top1_score    0.810954
top3_score    0.821231
top5_score    0.822655
dtype: float64
valid_loss:  2.820562482515406
Learning rate after epoch 19: [2.4055482956377946e-05]
Epoch 20/20: 100%|██████████| 2250/2250 [00:43<00:00, 51.34batch/s]

Model validation...
100%|██████████| 563/563 [00:09<00:00, 59.23batch/s]
[I 2025-06-24 21:47:53,271] Trial 3 finished with value: 0.8165901899337769 and parameters: {'linear_out_1': 256, 'linear_out_2': 256, 'sched_factor': 0.286353050346915, 'lr': 2.4055482956377946e-05, 'weight_decay': 0.0003594967842856646, 'batch_size': 64, 'sched_patience': 5, 'sched_min_lr': 1e-06, 'a1': 0.5382997848840297, 'a2': 0.6877996879862077, 'a3': 0.7444415699075089}. Best is trial 3 with value: 0.8165901899337769.
/home/c611/.local/lib/python3.11/site-packages/optuna/trial/_trial.py:682: RuntimeWarning: Inconsistent parameter values for distribution with name "sched_factor"! This might be a configuration mistake. Optuna allows to call the same distribution with the same name more than once in a trial. When the parameter values are inconsistent optuna only uses the values of the first call and ignores all following. Using these values: {'step': None, 'low': 0.0, 'high': 0.4, 'log': False}
  warnings.warn(
top1_score    0.814691
top3_score    0.824546
top5_score    0.826137
dtype: float64
valid_loss:  2.816725041895946
Learning rate after epoch 20: [2.4055482956377946e-05]
Training finished!
{'lr': 2.358706812459391e-05, 'weight_decay': 0.002740057434703994, 'num_epochs': 20, 'batch_size': 32, 'only_train_classifier': False, 'sched_patience': 10, 'sched_factor': 0.04300774714032847, 'sched_min_lr': 1e-06, 'train_with_CombinedClassifier': True, 'probabilities': [0.8124714789721101, 0.8625762559273166, 0.15265706107994276], 'config_CombinedClassifier': {'linear_out_1': 256, 'linear_out_2': 128, 'dropout': 0.04300774714032847}}


-------------- Start New Training: ---------------


Epoch 1/20: 100%|██████████| 4500/4500 [00:57<00:00, 78.39batch/s]

Model validation...
100%|██████████| 1125/1125 [00:10<00:00, 103.35batch/s]
top1_score    0.745291
top3_score    0.760517
top5_score    0.762990
dtype: float64
valid_loss:  2.8913277272118463
Learning rate after epoch 1: [2.358706812459391e-05]
Epoch 2/20: 100%|██████████| 4500/4500 [00:57<00:00, 77.73batch/s]

Model validation...
100%|██████████| 1125/1125 [00:10<00:00, 104.80batch/s]
top1_score    0.755941
top3_score    0.765442
top5_score    0.767495
dtype: float64
valid_loss:  2.8785058528052434
Learning rate after epoch 2: [2.358706812459391e-05]
Epoch 3/20: 100%|██████████| 4500/4500 [00:56<00:00, 79.11batch/s]

Model validation...
100%|██████████| 1125/1125 [00:10<00:00, 103.28batch/s]
top1_score    0.762702
top3_score    0.776321
top5_score    0.778460
dtype: float64
valid_loss:  2.87305349583096
Learning rate after epoch 3: [2.358706812459391e-05]
Epoch 4/20: 100%|██████████| 4500/4500 [00:57<00:00, 78.63batch/s]

Model validation...
100%|██████████| 1125/1125 [00:10<00:00, 104.13batch/s]
top1_score    0.771490
top3_score    0.782756
top5_score    0.784394
dtype: float64
valid_loss:  2.8618530023362903
Learning rate after epoch 4: [2.358706812459391e-05]
Epoch 5/20: 100%|██████████| 4500/4500 [00:57<00:00, 78.48batch/s]

Model validation...
100%|██████████| 1125/1125 [00:10<00:00, 103.99batch/s]
top1_score    0.774948
top3_score    0.785859
top5_score    0.787273
dtype: float64
valid_loss:  2.858283383263482
Learning rate after epoch 5: [2.358706812459391e-05]
Epoch 6/20: 100%|██████████| 4500/4500 [00:57<00:00, 78.49batch/s]

Model validation...
100%|██████████| 1125/1125 [00:10<00:00, 103.25batch/s]
top1_score    0.778191
top3_score    0.789586
top5_score    0.790879
dtype: float64
valid_loss:  2.8548823324839274
Learning rate after epoch 6: [2.358706812459391e-05]
Epoch 7/20: 100%|██████████| 4500/4500 [00:57<00:00, 78.11batch/s]

Model validation...
100%|██████████| 1125/1125 [00:10<00:00, 104.38batch/s]
top1_score    0.777249
top3_score    0.787523
top5_score    0.789040
dtype: float64
valid_loss:  2.8553283591800267
Learning rate after epoch 7: [2.358706812459391e-05]
Epoch 8/20: 100%|██████████| 4500/4500 [00:57<00:00, 78.16batch/s]

Model validation...
100%|██████████| 1125/1125 [00:10<00:00, 104.26batch/s]
top1_score    0.779476
top3_score    0.810978
top5_score    0.813957
dtype: float64
valid_loss:  2.8536311923133004
Learning rate after epoch 8: [2.358706812459391e-05]
Epoch 9/20: 100%|██████████| 4500/4500 [00:57<00:00, 78.31batch/s]

Model validation...
100%|██████████| 1125/1125 [00:10<00:00, 103.26batch/s]
top1_score    0.791664
top3_score    0.811103
top5_score    0.813282
dtype: float64
valid_loss:  2.8415333292219374
Learning rate after epoch 9: [2.358706812459391e-05]
Epoch 10/20: 100%|██████████| 4500/4500 [00:57<00:00, 78.72batch/s]

Model validation...
100%|██████████| 1125/1125 [00:10<00:00, 103.75batch/s]
top1_score    0.813466
top3_score    0.830495
top5_score    0.832909
dtype: float64
valid_loss:  2.819631669998169
Learning rate after epoch 10: [2.358706812459391e-05]
Epoch 11/20: 100%|██████████| 4500/4500 [00:56<00:00, 79.01batch/s]

Model validation...
100%|██████████| 1125/1125 [00:10<00:00, 104.02batch/s]
top1_score    0.813974
top3_score    0.831759
top5_score    0.834452
dtype: float64
valid_loss:  2.819169229719374
Learning rate after epoch 11: [2.358706812459391e-05]
Epoch 12/20: 100%|██████████| 4500/4500 [00:57<00:00, 78.80batch/s]

Model validation...
100%|██████████| 1125/1125 [00:10<00:00, 103.18batch/s]
top1_score    0.811195
top3_score    0.828414
top5_score    0.830315
dtype: float64
valid_loss:  2.8213405265808107
Learning rate after epoch 12: [2.358706812459391e-05]
Epoch 13/20: 100%|██████████| 4500/4500 [00:57<00:00, 78.62batch/s]

Model validation...
100%|██████████| 1125/1125 [00:10<00:00, 103.82batch/s]
top1_score    0.810822
top3_score    0.830568
top5_score    0.832741
dtype: float64
valid_loss:  2.82141195763482
Learning rate after epoch 13: [2.358706812459391e-05]
Epoch 14/20: 100%|██████████| 4500/4500 [00:57<00:00, 78.61batch/s]

Model validation...
100%|██████████| 1125/1125 [00:10<00:00, 102.68batch/s]
top1_score    0.815843
top3_score    0.832495
top5_score    0.834224
dtype: float64
valid_loss:  2.8169557209014893
Learning rate after epoch 14: [2.358706812459391e-05]
Epoch 15/20: 100%|██████████| 4500/4500 [00:57<00:00, 78.48batch/s]

Model validation...
100%|██████████| 1125/1125 [00:10<00:00, 104.56batch/s]
top1_score    0.811435
top3_score    0.829331
top5_score    0.831657
dtype: float64
valid_loss:  2.820721300125122
Learning rate after epoch 15: [2.358706812459391e-05]
Epoch 16/20: 100%|██████████| 4500/4500 [00:56<00:00, 79.78batch/s]

Model validation...
100%|██████████| 1125/1125 [00:10<00:00, 104.39batch/s]
top1_score    0.815890
top3_score    0.829894
top5_score    0.831866
dtype: float64
valid_loss:  2.8168043994903567
Learning rate after epoch 16: [2.358706812459391e-05]
Epoch 17/20: 100%|██████████| 4500/4500 [00:56<00:00, 79.30batch/s]

Model validation...
100%|██████████| 1125/1125 [00:10<00:00, 104.11batch/s]
top1_score    0.818245
top3_score    0.830549
top5_score    0.832633
dtype: float64
valid_loss:  2.8140560537974038
Learning rate after epoch 17: [2.358706812459391e-05]
Epoch 18/20: 100%|██████████| 4500/4500 [00:57<00:00, 78.64batch/s]

Model validation...
100%|██████████| 1125/1125 [00:10<00:00, 105.12batch/s]
top1_score    0.816885
top3_score    0.828617
top5_score    0.830239
dtype: float64
valid_loss:  2.8151191590627036
Learning rate after epoch 18: [2.358706812459391e-05]
Epoch 19/20: 100%|██████████| 4500/4500 [00:57<00:00, 78.82batch/s]

Model validation...
100%|██████████| 1125/1125 [00:10<00:00, 104.32batch/s]
top1_score    0.814416
top3_score    0.830250
top5_score    0.831939
dtype: float64
valid_loss:  2.8182364296383327
Learning rate after epoch 19: [2.358706812459391e-05]
Epoch 20/20: 100%|██████████| 4500/4500 [00:57<00:00, 78.75batch/s]

Model validation...
100%|██████████| 1125/1125 [00:10<00:00, 104.73batch/s]
[I 2025-06-24 22:10:34,820] Trial 4 finished with value: 0.8182450532913208 and parameters: {'linear_out_1': 256, 'linear_out_2': 128, 'sched_factor': 0.04300774714032847, 'lr': 2.358706812459391e-05, 'weight_decay': 0.002740057434703994, 'batch_size': 32, 'sched_patience': 10, 'sched_min_lr': 1e-06, 'a1': 0.8124714789721101, 'a2': 0.8625762559273166, 'a3': 0.15265706107994276}. Best is trial 4 with value: 0.8182450532913208.
/home/c611/.local/lib/python3.11/site-packages/optuna/trial/_trial.py:682: RuntimeWarning: Inconsistent parameter values for distribution with name "sched_factor"! This might be a configuration mistake. Optuna allows to call the same distribution with the same name more than once in a trial. When the parameter values are inconsistent optuna only uses the values of the first call and ignores all following. Using these values: {'step': None, 'low': 0.0, 'high': 0.4, 'log': False}
  warnings.warn(
top1_score    0.815165
top3_score    0.830261
top5_score    0.831803
dtype: float64
valid_loss:  2.816109472486708
Learning rate after epoch 20: [2.358706812459391e-05]
Training finished!
{'lr': 0.00018978593759913664, 'weight_decay': 0.000284839266837128, 'num_epochs': 20, 'batch_size': 64, 'only_train_classifier': False, 'sched_patience': 3, 'sched_factor': 0.10595998592311169, 'sched_min_lr': 1e-06, 'train_with_CombinedClassifier': True, 'probabilities': [0.7159115256071766, 0.12764448726214978, 0.6531874278730978], 'config_CombinedClassifier': {'linear_out_1': 512, 'linear_out_2': 64, 'dropout': 0.10595998592311169}}


-------------- Start New Training: ---------------


Epoch 1/20: 100%|██████████| 2250/2250 [00:43<00:00, 51.60batch/s]

Model validation...
100%|██████████| 563/563 [00:09<00:00, 59.41batch/s]
top1_score    0.729923
top3_score    0.740108
top5_score    0.741468
dtype: float64
valid_loss:  2.903307095725837
Learning rate after epoch 1: [0.00018978593759913664]
Epoch 2/20: 100%|██████████| 2250/2250 [00:43<00:00, 51.53batch/s]

Model validation...
100%|██████████| 563/563 [00:09<00:00, 58.40batch/s]
top1_score    0.754891
top3_score    0.767626
top5_score    0.769210
dtype: float64
valid_loss:  2.8778374123107477
Learning rate after epoch 2: [0.00018978593759913664]
Epoch 3/20: 100%|██████████| 2250/2250 [00:43<00:00, 51.19batch/s]

Model validation...
100%|██████████| 563/563 [00:09<00:00, 59.39batch/s]
top1_score    0.756907
top3_score    0.768986
top5_score    0.770968
dtype: float64
valid_loss:  2.876089373447967
Learning rate after epoch 3: [0.00018978593759913664]
Epoch 4/20: 100%|██████████| 2250/2250 [00:43<00:00, 51.33batch/s]

Model validation...
100%|██████████| 563/563 [00:09<00:00, 59.16batch/s]
top1_score    0.774082
top3_score    0.790691
top5_score    0.792424
dtype: float64
valid_loss:  2.8580848899658364
Learning rate after epoch 4: [0.00018978593759913664]
Epoch 5/20: 100%|██████████| 2250/2250 [00:44<00:00, 51.11batch/s]

Model validation...
100%|██████████| 563/563 [00:09<00:00, 59.37batch/s]
top1_score    0.777394
top3_score    0.793713
top5_score    0.795827
dtype: float64
valid_loss:  2.8551979196219843
Learning rate after epoch 5: [0.00018978593759913664]
Epoch 6/20: 100%|██████████| 2250/2250 [00:43<00:00, 51.70batch/s]

Model validation...
100%|██████████| 563/563 [00:09<00:00, 59.55batch/s]
top1_score    0.795339
top3_score    0.810413
top5_score    0.811891
dtype: float64
valid_loss:  2.8372755867973316
Learning rate after epoch 6: [0.00018978593759913664]
Epoch 7/20: 100%|██████████| 2250/2250 [00:43<00:00, 51.64batch/s]

Model validation...
100%|██████████| 563/563 [00:09<00:00, 58.55batch/s]
top1_score    0.790250
top3_score    0.804390
top5_score    0.806112
dtype: float64
valid_loss:  2.842593636860009
Learning rate after epoch 7: [0.00018978593759913664]
Epoch 8/20: 100%|██████████| 2250/2250 [00:43<00:00, 51.51batch/s]

Model validation...
100%|██████████| 563/563 [00:09<00:00, 59.25batch/s]
top1_score    0.789765
top3_score    0.803330
top5_score    0.804810
dtype: float64
valid_loss:  2.842262354032608
Learning rate after epoch 8: [0.00018978593759913664]
Epoch 9/20: 100%|██████████| 2250/2250 [00:43<00:00, 51.29batch/s]

Model validation...
100%|██████████| 563/563 [00:09<00:00, 59.08batch/s]
top1_score    0.789564
top3_score    0.802943
top5_score    0.805018
dtype: float64
valid_loss:  2.8428954294689173
Learning rate after epoch 9: [0.00018978593759913664]
Epoch 10/20: 100%|██████████| 2250/2250 [00:43<00:00, 51.17batch/s]

Model validation...
100%|██████████| 563/563 [00:09<00:00, 59.38batch/s]
top1_score    0.783883
top3_score    0.796978
top5_score    0.798685
dtype: float64
valid_loss:  2.8490562853974093
Learning rate after epoch 10: [2.0109715276409073e-05]
Epoch 11/20: 100%|██████████| 2250/2250 [00:43<00:00, 51.47batch/s]

Model validation...
100%|██████████| 563/563 [00:09<00:00, 59.20batch/s]
top1_score    0.812510
top3_score    0.821697
top5_score    0.823182
dtype: float64
valid_loss:  2.8189512302778117
Learning rate after epoch 11: [2.0109715276409073e-05]
Epoch 12/20: 100%|██████████| 2250/2250 [00:43<00:00, 51.40batch/s]

Model validation...
100%|██████████| 563/563 [00:09<00:00, 58.55batch/s]
top1_score    0.815689
top3_score    0.825140
top5_score    0.827030
dtype: float64
valid_loss:  2.8156753191939465
Learning rate after epoch 12: [2.0109715276409073e-05]
Epoch 13/20: 100%|██████████| 2250/2250 [00:43<00:00, 51.43batch/s]

Model validation...
100%|██████████| 563/563 [00:09<00:00, 59.47batch/s]
top1_score    0.818820
top3_score    0.827537
top5_score    0.829041
dtype: float64
valid_loss:  2.812758378406613
Learning rate after epoch 13: [2.0109715276409073e-05]
Epoch 14/20: 100%|██████████| 2250/2250 [00:43<00:00, 51.54batch/s]

Model validation...
100%|██████████| 563/563 [00:09<00:00, 59.15batch/s]
top1_score    0.819135
top3_score    0.828196
top5_score    0.829848
dtype: float64
valid_loss:  2.8124972694085293
Learning rate after epoch 14: [2.0109715276409073e-05]
Epoch 15/20: 100%|██████████| 2250/2250 [00:43<00:00, 51.40batch/s]

Model validation...
100%|██████████| 563/563 [00:09<00:00, 59.00batch/s]
top1_score    0.820419
top3_score    0.828586
top5_score    0.830117
dtype: float64
valid_loss:  2.8110834167439815
Learning rate after epoch 15: [2.0109715276409073e-05]
Epoch 16/20: 100%|██████████| 2250/2250 [00:43<00:00, 51.56batch/s]

Model validation...
100%|██████████| 563/563 [00:09<00:00, 59.43batch/s]
top1_score    0.820195
top3_score    0.828176
top5_score    0.829753
dtype: float64
valid_loss:  2.8116492390844368
Learning rate after epoch 16: [2.0109715276409073e-05]
Epoch 17/20: 100%|██████████| 2250/2250 [00:43<00:00, 51.39batch/s]

Model validation...
100%|██████████| 563/563 [00:09<00:00, 59.14batch/s]
top1_score    0.820915
top3_score    0.829947
top5_score    0.831179
dtype: float64
valid_loss:  2.810648624681029
Learning rate after epoch 17: [2.0109715276409073e-05]
Epoch 18/20: 100%|██████████| 2250/2250 [00:43<00:00, 51.42batch/s]

Model validation...
100%|██████████| 563/563 [00:09<00:00, 59.32batch/s]
top1_score    0.823534
top3_score    0.831155
top5_score    0.832359
dtype: float64
valid_loss:  2.8083776542602594
Learning rate after epoch 18: [2.0109715276409073e-05]
Epoch 19/20: 100%|██████████| 2250/2250 [00:43<00:00, 51.45batch/s]

Model validation...
100%|██████████| 563/563 [00:09<00:00, 59.34batch/s]
top1_score    0.823759
top3_score    0.832194
top5_score    0.833401
dtype: float64
valid_loss:  2.8077700709788456
Learning rate after epoch 19: [2.0109715276409073e-05]
Epoch 20/20: 100%|██████████| 2250/2250 [00:43<00:00, 51.29batch/s]

Model validation...
100%|██████████| 563/563 [00:09<00:00, 59.04batch/s]
[I 2025-06-24 22:28:21,097] Trial 5 finished with value: 0.8237589597702026 and parameters: {'linear_out_1': 512, 'linear_out_2': 64, 'sched_factor': 0.10595998592311169, 'lr': 0.00018978593759913664, 'weight_decay': 0.000284839266837128, 'batch_size': 64, 'sched_patience': 3, 'sched_min_lr': 1e-06, 'a1': 0.7159115256071766, 'a2': 0.12764448726214978, 'a3': 0.6531874278730978}. Best is trial 5 with value: 0.8237589597702026.
/home/c611/.local/lib/python3.11/site-packages/optuna/trial/_trial.py:682: RuntimeWarning: Inconsistent parameter values for distribution with name "sched_factor"! This might be a configuration mistake. Optuna allows to call the same distribution with the same name more than once in a trial. When the parameter values are inconsistent optuna only uses the values of the first call and ignores all following. Using these values: {'step': None, 'low': 0.0, 'high': 0.4, 'log': False}
  warnings.warn(
top1_score    0.823105
top3_score    0.831121
top5_score    0.832601
dtype: float64
valid_loss:  2.808422660319462
Learning rate after epoch 20: [2.0109715276409073e-05]
Training finished!
{'lr': 5.043282495182633e-05, 'weight_decay': 0.006897363596454951, 'num_epochs': 20, 'batch_size': 64, 'only_train_classifier': False, 'sched_patience': 4, 'sched_factor': 0.2793630814831351, 'sched_min_lr': 1e-05, 'train_with_CombinedClassifier': True, 'probabilities': [0.699908835302261, 0.18670807741731738, 0.1398873061025779], 'config_CombinedClassifier': {'linear_out_1': 128, 'linear_out_2': 128, 'dropout': 0.2793630814831351}}


-------------- Start New Training: ---------------


Epoch 1/20: 100%|██████████| 2250/2250 [00:43<00:00, 51.81batch/s]

Model validation...
100%|██████████| 563/563 [00:09<00:00, 59.89batch/s]
top1_score    0.740722
top3_score    0.752889
top5_score    0.755403
dtype: float64
valid_loss:  2.89455973487976
Learning rate after epoch 1: [5.043282495182633e-05]
Epoch 2/20: 100%|██████████| 2250/2250 [00:43<00:00, 51.80batch/s]

Model validation...
100%|██████████| 563/563 [00:09<00:00, 59.31batch/s]
top1_score    0.755850
top3_score    0.767380
top5_score    0.769021
dtype: float64
valid_loss:  2.8792875973411816
Learning rate after epoch 2: [5.043282495182633e-05]
Epoch 3/20: 100%|██████████| 2250/2250 [00:43<00:00, 51.71batch/s]

Model validation...
100%|██████████| 563/563 [00:09<00:00, 59.68batch/s]
top1_score    0.773325
top3_score    0.784660
top5_score    0.786303
dtype: float64
valid_loss:  2.8612679846545097
Learning rate after epoch 3: [5.043282495182633e-05]
Epoch 4/20: 100%|██████████| 2250/2250 [00:43<00:00, 52.30batch/s]

Model validation...
100%|██████████| 563/563 [00:09<00:00, 59.98batch/s]
top1_score    0.788944
top3_score    0.800262
top5_score    0.801697
dtype: float64
valid_loss:  2.8447239754678515
Learning rate after epoch 4: [5.043282495182633e-05]
Epoch 5/20: 100%|██████████| 2250/2250 [00:43<00:00, 51.84batch/s]

Model validation...
100%|██████████| 563/563 [00:09<00:00, 59.11batch/s]
top1_score    0.788630
top3_score    0.797991
top5_score    0.799455
dtype: float64
valid_loss:  2.8457475491992956
Learning rate after epoch 5: [5.043282495182633e-05]
Epoch 6/20: 100%|██████████| 2250/2250 [00:43<00:00, 51.95batch/s]

Model validation...
100%|██████████| 563/563 [00:09<00:00, 59.99batch/s]
top1_score    0.786159
top3_score    0.795291
top5_score    0.796401
dtype: float64
valid_loss:  2.8474691913564083
Learning rate after epoch 6: [5.043282495182633e-05]
Epoch 7/20: 100%|██████████| 2250/2250 [00:43<00:00, 51.67batch/s]

Model validation...
100%|██████████| 563/563 [00:09<00:00, 59.70batch/s]
top1_score    0.788574
top3_score    0.797588
top5_score    0.798667
dtype: float64
valid_loss:  2.8448290820655346
Learning rate after epoch 7: [5.043282495182633e-05]
Epoch 8/20: 100%|██████████| 2250/2250 [00:43<00:00, 52.17batch/s]

Model validation...
100%|██████████| 563/563 [00:09<00:00, 59.59batch/s]
top1_score    0.790337
top3_score    0.798940
top5_score    0.803329
dtype: float64
valid_loss:  2.843226013979734
Learning rate after epoch 8: [5.043282495182633e-05]
Epoch 9/20: 100%|██████████| 2250/2250 [00:43<00:00, 51.95batch/s]

Model validation...
100%|██████████| 563/563 [00:09<00:00, 59.90batch/s]
top1_score    0.792280
top3_score    0.804553
top5_score    0.806827
dtype: float64
valid_loss:  2.841591598297309
Learning rate after epoch 9: [5.043282495182633e-05]
Epoch 10/20: 100%|██████████| 2250/2250 [00:43<00:00, 52.27batch/s]

Model validation...
100%|██████████| 563/563 [00:09<00:00, 59.22batch/s]
top1_score    0.799285
top3_score    0.810573
top5_score    0.812383
dtype: float64
valid_loss:  2.83389019415603
Learning rate after epoch 10: [5.043282495182633e-05]
Epoch 11/20: 100%|██████████| 2250/2250 [00:43<00:00, 52.01batch/s]

Model validation...
100%|██████████| 563/563 [00:09<00:00, 59.82batch/s]
top1_score    0.803023
top3_score    0.814143
top5_score    0.815733
dtype: float64
valid_loss:  2.830784242487716
Learning rate after epoch 11: [5.043282495182633e-05]
Epoch 12/20: 100%|██████████| 2250/2250 [00:43<00:00, 52.15batch/s]

Model validation...
100%|██████████| 563/563 [00:09<00:00, 60.24batch/s]
top1_score    0.809095
top3_score    0.823179
top5_score    0.825503
dtype: float64
valid_loss:  2.8252542378847383
Learning rate after epoch 12: [5.043282495182633e-05]
Epoch 13/20: 100%|██████████| 2250/2250 [00:42<00:00, 52.52batch/s]

Model validation...
100%|██████████| 563/563 [00:09<00:00, 60.06batch/s]
top1_score    0.808462
top3_score    0.820599
top5_score    0.821860
dtype: float64
valid_loss:  2.8251803264313016
Learning rate after epoch 13: [5.043282495182633e-05]
Epoch 14/20: 100%|██████████| 2250/2250 [00:43<00:00, 52.02batch/s]

Model validation...
100%|██████████| 563/563 [00:09<00:00, 60.00batch/s]
top1_score    0.811621
top3_score    0.822277
top5_score    0.823680
dtype: float64
valid_loss:  2.8214316664538206
Learning rate after epoch 14: [5.043282495182633e-05]
Epoch 15/20: 100%|██████████| 2250/2250 [00:43<00:00, 52.16batch/s]

Model validation...
100%|██████████| 563/563 [00:09<00:00, 59.93batch/s]
top1_score    0.811976
top3_score    0.822082
top5_score    0.823920
dtype: float64
valid_loss:  2.8209806466822513
Learning rate after epoch 15: [5.043282495182633e-05]
Epoch 16/20: 100%|██████████| 2250/2250 [00:43<00:00, 51.85batch/s]

Model validation...
100%|██████████| 563/563 [00:09<00:00, 60.18batch/s]
top1_score    0.809915
top3_score    0.819920
top5_score    0.821103
dtype: float64
valid_loss:  2.822992478975394
Learning rate after epoch 16: [5.043282495182633e-05]
Epoch 17/20: 100%|██████████| 2250/2250 [00:43<00:00, 52.01batch/s]

Model validation...
100%|██████████| 563/563 [00:09<00:00, 59.98batch/s]
top1_score    0.810118
top3_score    0.820222
top5_score    0.821791
dtype: float64
valid_loss:  2.8234309286249677
Learning rate after epoch 17: [5.043282495182633e-05]
Epoch 18/20: 100%|██████████| 2250/2250 [00:42<00:00, 52.43batch/s]

Model validation...
100%|██████████| 563/563 [00:09<00:00, 60.15batch/s]
top1_score    0.810844
top3_score    0.820453
top5_score    0.821871
dtype: float64
valid_loss:  2.8223075248420133
Learning rate after epoch 18: [5.043282495182633e-05]
Epoch 19/20: 100%|██████████| 2250/2250 [00:43<00:00, 52.24batch/s]

Model validation...
100%|██████████| 563/563 [00:09<00:00, 59.95batch/s]
top1_score    0.816827
top3_score    0.825530
top5_score    0.826667
dtype: float64
valid_loss:  2.8164212004964773
Learning rate after epoch 19: [5.043282495182633e-05]
Epoch 20/20: 100%|██████████| 2250/2250 [00:43<00:00, 51.92batch/s]

Model validation...
100%|██████████| 563/563 [00:09<00:00, 60.32batch/s]
[I 2025-06-24 22:45:54,797] Trial 6 finished with value: 0.8168268203735352 and parameters: {'linear_out_1': 128, 'linear_out_2': 128, 'sched_factor': 0.2793630814831351, 'lr': 5.043282495182633e-05, 'weight_decay': 0.006897363596454951, 'batch_size': 64, 'sched_patience': 4, 'sched_min_lr': 1e-05, 'a1': 0.699908835302261, 'a2': 0.18670807741731738, 'a3': 0.1398873061025779}. Best is trial 5 with value: 0.8237589597702026.
/home/c611/.local/lib/python3.11/site-packages/optuna/trial/_trial.py:682: RuntimeWarning: Inconsistent parameter values for distribution with name "sched_factor"! This might be a configuration mistake. Optuna allows to call the same distribution with the same name more than once in a trial. When the parameter values are inconsistent optuna only uses the values of the first call and ignores all following. Using these values: {'step': None, 'low': 0.0, 'high': 0.4, 'log': False}
  warnings.warn(
top1_score    0.816550
top3_score    0.824312
top5_score    0.825582
dtype: float64
valid_loss:  2.8167009959212415
Learning rate after epoch 20: [5.043282495182633e-05]
Training finished!
{'lr': 3.999800156857692e-05, 'weight_decay': 0.00042890981394079676, 'num_epochs': 20, 'batch_size': 64, 'only_train_classifier': False, 'sched_patience': 9, 'sched_factor': 0.14240806450083995, 'sched_min_lr': 1e-06, 'train_with_CombinedClassifier': True, 'probabilities': [0.5190828330814866, 0.7812252975382281, 0.3757568680344674], 'config_CombinedClassifier': {'linear_out_1': 256, 'linear_out_2': 128, 'dropout': 0.14240806450083995}}


-------------- Start New Training: ---------------


Epoch 1/20: 100%|██████████| 2250/2250 [00:43<00:00, 51.53batch/s]

Model validation...
100%|██████████| 563/563 [00:09<00:00, 59.81batch/s]
top1_score    0.767817
top3_score    0.787575
top5_score    0.790245
dtype: float64
valid_loss:  2.8690419383413204
Learning rate after epoch 1: [3.999800156857692e-05]
Epoch 2/20: 100%|██████████| 2250/2250 [00:43<00:00, 51.40batch/s]

Model validation...
100%|██████████| 563/563 [00:09<00:00, 59.12batch/s]
top1_score    0.789295
top3_score    0.813745
top5_score    0.817956
dtype: float64
valid_loss:  2.8463538679729345
Learning rate after epoch 2: [3.999800156857692e-05]
Epoch 3/20: 100%|██████████| 2250/2250 [00:43<00:00, 51.57batch/s]

Model validation...
100%|██████████| 563/563 [00:09<00:00, 59.99batch/s]
top1_score    0.805166
top3_score    0.821604
top5_score    0.824146
dtype: float64
valid_loss:  2.8292363469172965
Learning rate after epoch 3: [3.999800156857692e-05]
Epoch 4/20: 100%|██████████| 2250/2250 [00:43<00:00, 51.58batch/s]

Model validation...
100%|██████████| 563/563 [00:09<00:00, 59.02batch/s]
top1_score    0.800655
top3_score    0.816618
top5_score    0.818994
dtype: float64
valid_loss:  2.8341066130101153
Learning rate after epoch 4: [3.999800156857692e-05]
Epoch 5/20: 100%|██████████| 2250/2250 [00:43<00:00, 51.60batch/s]

Model validation...
100%|██████████| 563/563 [00:09<00:00, 59.50batch/s]
top1_score    0.811566
top3_score    0.828067
top5_score    0.829907
dtype: float64
valid_loss:  2.821663737932276
Learning rate after epoch 5: [3.999800156857692e-05]
Epoch 6/20: 100%|██████████| 2250/2250 [00:43<00:00, 51.55batch/s]

Model validation...
100%|██████████| 563/563 [00:09<00:00, 59.79batch/s]
top1_score    0.812252
top3_score    0.824441
top5_score    0.826252
dtype: float64
valid_loss:  2.820688812177939
Learning rate after epoch 6: [3.999800156857692e-05]
Epoch 7/20: 100%|██████████| 2250/2250 [00:43<00:00, 51.71batch/s]

Model validation...
100%|██████████| 563/563 [00:09<00:00, 59.84batch/s]
top1_score    0.811527
top3_score    0.824687
top5_score    0.826156
dtype: float64
valid_loss:  2.821092786416279
Learning rate after epoch 7: [3.999800156857692e-05]
Epoch 8/20: 100%|██████████| 2250/2250 [00:43<00:00, 51.94batch/s]

Model validation...
100%|██████████| 563/563 [00:09<00:00, 59.79batch/s]
top1_score    0.813342
top3_score    0.827670
top5_score    0.829712
dtype: float64
valid_loss:  2.819160719747865
Learning rate after epoch 8: [3.999800156857692e-05]
Epoch 9/20: 100%|██████████| 2250/2250 [00:43<00:00, 52.25batch/s]

Model validation...
100%|██████████| 563/563 [00:09<00:00, 59.96batch/s]
top1_score    0.815507
top3_score    0.828021
top5_score    0.829149
dtype: float64
valid_loss:  2.8171804027489618
Learning rate after epoch 9: [3.999800156857692e-05]
Epoch 10/20: 100%|██████████| 2250/2250 [00:43<00:00, 52.13batch/s]

Model validation...
100%|██████████| 563/563 [00:09<00:00, 59.76batch/s]
top1_score    0.823310
top3_score    0.838973
top5_score    0.840739
dtype: float64
valid_loss:  2.809653226266322
Learning rate after epoch 10: [3.999800156857692e-05]
Epoch 11/20: 100%|██████████| 2250/2250 [00:43<00:00, 51.68batch/s]

Model validation...
100%|██████████| 563/563 [00:09<00:00, 59.64batch/s]
top1_score    0.834763
top3_score    0.849428
top5_score    0.851439
dtype: float64
valid_loss:  2.798378825822901
Learning rate after epoch 11: [3.999800156857692e-05]
Epoch 12/20: 100%|██████████| 2250/2250 [00:43<00:00, 51.69batch/s]

Model validation...
100%|██████████| 563/563 [00:09<00:00, 59.73batch/s]
top1_score    0.839532
top3_score    0.854243
top5_score    0.855928
dtype: float64
valid_loss:  2.793834228075207
Learning rate after epoch 12: [3.999800156857692e-05]
Epoch 13/20: 100%|██████████| 2250/2250 [00:43<00:00, 51.70batch/s]

Model validation...
100%|██████████| 563/563 [00:09<00:00, 59.94batch/s]
top1_score    0.838111
top3_score    0.853783
top5_score    0.855425
dtype: float64
valid_loss:  2.7951929052600217
Learning rate after epoch 13: [3.999800156857692e-05]
Epoch 14/20: 100%|██████████| 2250/2250 [00:43<00:00, 51.94batch/s]

Model validation...
100%|██████████| 563/563 [00:09<00:00, 59.35batch/s]
top1_score    0.833321
top3_score    0.847275
top5_score    0.849099
dtype: float64
valid_loss:  2.800138748771979
Learning rate after epoch 14: [3.999800156857692e-05]
Epoch 15/20: 100%|██████████| 2250/2250 [00:43<00:00, 51.58batch/s]

Model validation...
100%|██████████| 563/563 [00:09<00:00, 59.52batch/s]
top1_score    0.842268
top3_score    0.853644
top5_score    0.854661
dtype: float64
valid_loss:  2.7910554620766512
Learning rate after epoch 15: [3.999800156857692e-05]
Epoch 16/20: 100%|██████████| 2250/2250 [00:43<00:00, 51.77batch/s]

Model validation...
100%|██████████| 563/563 [00:09<00:00, 59.72batch/s]
top1_score    0.835258
top3_score    0.850433
top5_score    0.852018
dtype: float64
valid_loss:  2.7974635695056
Learning rate after epoch 16: [3.999800156857692e-05]
Epoch 17/20: 100%|██████████| 2250/2250 [00:43<00:00, 51.68batch/s]

Model validation...
100%|██████████| 563/563 [00:09<00:00, 60.03batch/s]
top1_score    0.841299
top3_score    0.853596
top5_score    0.854807
dtype: float64
valid_loss:  2.791211040998225
Learning rate after epoch 17: [3.999800156857692e-05]
Epoch 18/20: 100%|██████████| 2250/2250 [00:43<00:00, 52.04batch/s]

Model validation...
100%|██████████| 563/563 [00:09<00:00, 59.42batch/s]
top1_score    0.843295
top3_score    0.856932
top5_score    0.858405
dtype: float64
valid_loss:  2.789478969404583
Learning rate after epoch 18: [3.999800156857692e-05]
Epoch 19/20: 100%|██████████| 2250/2250 [00:43<00:00, 51.46batch/s]

Model validation...
100%|██████████| 563/563 [00:09<00:00, 59.74batch/s]
top1_score    0.842996
top3_score    0.855197
top5_score    0.856625
dtype: float64
valid_loss:  2.7895416222499487
Learning rate after epoch 19: [3.999800156857692e-05]
Epoch 20/20: 100%|██████████| 2250/2250 [00:43<00:00, 51.81batch/s]

Model validation...
100%|██████████| 563/563 [00:09<00:00, 59.18batch/s]
[I 2025-06-24 23:03:34,281] Trial 7 finished with value: 0.8432953357696533 and parameters: {'linear_out_1': 256, 'linear_out_2': 128, 'sched_factor': 0.14240806450083995, 'lr': 3.999800156857692e-05, 'weight_decay': 0.00042890981394079676, 'batch_size': 64, 'sched_patience': 9, 'sched_min_lr': 1e-06, 'a1': 0.5190828330814866, 'a2': 0.7812252975382281, 'a3': 0.3757568680344674}. Best is trial 7 with value: 0.8432953357696533.
/home/c611/.local/lib/python3.11/site-packages/optuna/trial/_trial.py:682: RuntimeWarning: Inconsistent parameter values for distribution with name "sched_factor"! This might be a configuration mistake. Optuna allows to call the same distribution with the same name more than once in a trial. When the parameter values are inconsistent optuna only uses the values of the first call and ignores all following. Using these values: {'step': None, 'low': 0.0, 'high': 0.4, 'log': False}
  warnings.warn(
top1_score    0.835263
top3_score    0.850117
top5_score    0.851348
dtype: float64
valid_loss:  2.7967869138632957
Learning rate after epoch 20: [3.999800156857692e-05]
Training finished!
{'lr': 2.783989378560212e-05, 'weight_decay': 0.00017713116347813733, 'num_epochs': 20, 'batch_size': 16, 'only_train_classifier': False, 'sched_patience': 7, 'sched_factor': 0.13769602712138176, 'sched_min_lr': 1e-06, 'train_with_CombinedClassifier': True, 'probabilities': [0.787967159150076, 0.1601712355971677, 0.14960027093110626], 'config_CombinedClassifier': {'linear_out_1': 512, 'linear_out_2': 256, 'dropout': 0.13769602712138176}}


-------------- Start New Training: ---------------


Epoch 1/20: 100%|██████████| 9000/9000 [01:29<00:00, 100.18batch/s]

Model validation...
100%|██████████| 2250/2250 [00:14<00:00, 151.77batch/s]
top1_score    0.735635
top3_score    0.747591
top5_score    0.755969
dtype: float64
valid_loss:  2.8987735889222885
Learning rate after epoch 1: [2.783989378560212e-05]
Epoch 2/20: 100%|██████████| 9000/9000 [01:30<00:00, 99.55batch/s]

Model validation...
100%|██████████| 2250/2250 [00:14<00:00, 152.18batch/s]
top1_score    0.741497
top3_score    0.750494
top5_score    0.759019
dtype: float64
valid_loss:  2.8906903676986695
Learning rate after epoch 2: [2.783989378560212e-05]
Epoch 3/20: 100%|██████████| 9000/9000 [01:29<00:00, 100.60batch/s]

Model validation...
100%|██████████| 2250/2250 [00:14<00:00, 151.05batch/s]
top1_score    0.765171
top3_score    0.778902
top5_score    0.780255
dtype: float64
valid_loss:  2.8665507558186847
Learning rate after epoch 3: [2.783989378560212e-05]
Epoch 4/20: 100%|██████████| 9000/9000 [01:30<00:00, 99.81batch/s]

Model validation...
100%|██████████| 2250/2250 [00:14<00:00, 151.14batch/s]
top1_score    0.765139
top3_score    0.777262
top5_score    0.778144
dtype: float64
valid_loss:  2.866064097934299
Learning rate after epoch 4: [2.783989378560212e-05]
Epoch 5/20: 100%|██████████| 9000/9000 [01:30<00:00, 99.96batch/s]

Model validation...
100%|██████████| 2250/2250 [00:14<00:00, 152.07batch/s]
top1_score    0.768434
top3_score    0.779241
top5_score    0.780553
dtype: float64
valid_loss:  2.8625872802734373
Learning rate after epoch 5: [2.783989378560212e-05]
Epoch 6/20: 100%|██████████| 9000/9000 [01:29<00:00, 100.24batch/s]

Model validation...
100%|██████████| 2250/2250 [00:14<00:00, 150.92batch/s]
[I 2025-06-24 23:14:03,459] Trial 8 pruned.
/home/c611/.local/lib/python3.11/site-packages/optuna/trial/_trial.py:682: RuntimeWarning: Inconsistent parameter values for distribution with name "sched_factor"! This might be a configuration mistake. Optuna allows to call the same distribution with the same name more than once in a trial. When the parameter values are inconsistent optuna only uses the values of the first call and ignores all following. Using these values: {'step': None, 'low': 0.0, 'high': 0.4, 'log': False}
  warnings.warn(
top1_score    0.771165
top3_score    0.783461
top5_score    0.784452
dtype: float64
valid_loss:  2.86009218788147
Learning rate after epoch 6: [2.783989378560212e-05]
{'lr': 7.086391174222633e-05, 'weight_decay': 0.0001537820093131612, 'num_epochs': 20, 'batch_size': 32, 'only_train_classifier': False, 'sched_patience': 3, 'sched_factor': 0.004267746602565081, 'sched_min_lr': 1e-06, 'train_with_CombinedClassifier': True, 'probabilities': [0.8183586074661165, 0.8156962267122799, 0.9527988440590834], 'config_CombinedClassifier': {'linear_out_1': 512, 'linear_out_2': 64, 'dropout': 0.004267746602565081}}


-------------- Start New Training: ---------------


Epoch 1/20: 100%|██████████| 4500/4500 [00:57<00:00, 77.89batch/s]

Model validation...
100%|██████████| 1125/1125 [00:11<00:00, 102.19batch/s]
top1_score    0.716944
top3_score    0.732194
top5_score    0.734079
dtype: float64
valid_loss:  2.9150889059702556
Learning rate after epoch 1: [7.086391174222633e-05]
Epoch 2/20: 100%|██████████| 4500/4500 [00:58<00:00, 77.35batch/s]

Model validation...
100%|██████████| 1125/1125 [00:10<00:00, 102.82batch/s]
top1_score    0.733292
top3_score    0.749348
top5_score    0.751377
dtype: float64
valid_loss:  2.89803473493788
Learning rate after epoch 2: [7.086391174222633e-05]
Epoch 3/20: 100%|██████████| 4500/4500 [00:58<00:00, 77.52batch/s]

Model validation...
100%|██████████| 1125/1125 [00:10<00:00, 102.46batch/s]
[I 2025-06-24 23:17:30,691] Trial 9 pruned.
/home/c611/.local/lib/python3.11/site-packages/optuna/trial/_trial.py:682: RuntimeWarning: Inconsistent parameter values for distribution with name "sched_factor"! This might be a configuration mistake. Optuna allows to call the same distribution with the same name more than once in a trial. When the parameter values are inconsistent optuna only uses the values of the first call and ignores all following. Using these values: {'step': None, 'low': 0.0, 'high': 0.4, 'log': False}
  warnings.warn(
top1_score    0.732760
top3_score    0.745901
top5_score    0.747749
dtype: float64
valid_loss:  2.897865497589111
Learning rate after epoch 3: [7.086391174222633e-05]
{'lr': 0.0003234677955294704, 'weight_decay': 0.0007454225954124511, 'num_epochs': 20, 'batch_size': 64, 'only_train_classifier': False, 'sched_patience': 10, 'sched_factor': 0.3978609880236772, 'sched_min_lr': 1e-05, 'train_with_CombinedClassifier': True, 'probabilities': [0.4141066890256485, 0.5006610894914043, 0.3866412647677004], 'config_CombinedClassifier': {'linear_out_1': 256, 'linear_out_2': 128, 'dropout': 0.3978609880236772}}


-------------- Start New Training: ---------------


Epoch 1/20: 100%|██████████| 2250/2250 [00:43<00:00, 51.42batch/s]

Model validation...
100%|██████████| 563/563 [00:09<00:00, 58.90batch/s]
top1_score    0.672788
top3_score    0.700291
top5_score    0.702854
dtype: float64
valid_loss:  2.959813335225595
Learning rate after epoch 1: [0.0003234677955294704]
Epoch 2/20: 100%|██████████| 2250/2250 [00:43<00:00, 51.37batch/s]

Model validation...
100%|██████████| 563/563 [00:09<00:00, 59.39batch/s]
top1_score    0.733180
top3_score    0.756353
top5_score    0.762953
dtype: float64
valid_loss:  2.8982684476760947
Learning rate after epoch 2: [0.0003234677955294704]
Epoch 3/20: 100%|██████████| 2250/2250 [00:43<00:00, 51.67batch/s]

Model validation...
100%|██████████| 563/563 [00:09<00:00, 59.71batch/s]
[I 2025-06-24 23:20:10,583] Trial 10 pruned.
/home/c611/.local/lib/python3.11/site-packages/optuna/trial/_trial.py:682: RuntimeWarning: Inconsistent parameter values for distribution with name "sched_factor"! This might be a configuration mistake. Optuna allows to call the same distribution with the same name more than once in a trial. When the parameter values are inconsistent optuna only uses the values of the first call and ignores all following. Using these values: {'step': None, 'low': 0.0, 'high': 0.4, 'log': False}
  warnings.warn(
top1_score    0.743820
top3_score    0.763391
top5_score    0.766256
dtype: float64
valid_loss:  2.886460469416996
Learning rate after epoch 3: [0.0003234677955294704]
{'lr': 0.00018553413651173673, 'weight_decay': 0.0004978701727977567, 'num_epochs': 20, 'batch_size': 64, 'only_train_classifier': False, 'sched_patience': 8, 'sched_factor': 0.07454508908989581, 'sched_min_lr': 1e-06, 'train_with_CombinedClassifier': True, 'probabilities': [0.6083287170872044, 0.3768921774910501, 0.3973705177599233], 'config_CombinedClassifier': {'linear_out_1': 256, 'linear_out_2': 64, 'dropout': 0.07454508908989581}}


-------------- Start New Training: ---------------


Epoch 1/20: 100%|██████████| 2250/2250 [00:43<00:00, 51.36batch/s]

Model validation...
100%|██████████| 563/563 [00:09<00:00, 59.53batch/s]
top1_score    0.722108
top3_score    0.740448
top5_score    0.746731
dtype: float64
valid_loss:  2.911114664314907
Learning rate after epoch 1: [0.00018553413651173673]
Epoch 2/20: 100%|██████████| 2250/2250 [00:43<00:00, 51.39batch/s]

Model validation...
100%|██████████| 563/563 [00:09<00:00, 59.25batch/s]
top1_score    0.720503
top3_score    0.733288
top5_score    0.738205
dtype: float64
valid_loss:  2.9114555605351398
Learning rate after epoch 2: [0.00018553413651173673]
Epoch 3/20: 100%|██████████| 2250/2250 [00:43<00:00, 51.66batch/s]

Model validation...
100%|██████████| 563/563 [00:09<00:00, 58.92batch/s]
[I 2025-06-24 23:22:50,556] Trial 11 pruned.
/home/c611/.local/lib/python3.11/site-packages/optuna/trial/_trial.py:682: RuntimeWarning: Inconsistent parameter values for distribution with name "sched_factor"! This might be a configuration mistake. Optuna allows to call the same distribution with the same name more than once in a trial. When the parameter values are inconsistent optuna only uses the values of the first call and ignores all following. Using these values: {'step': None, 'low': 0.0, 'high': 0.4, 'log': False}
  warnings.warn(
top1_score    0.726507
top3_score    0.738395
top5_score    0.742036
dtype: float64
valid_loss:  2.905439325166638
Learning rate after epoch 3: [0.00018553413651173673]
{'lr': 1.0094053439404893e-05, 'weight_decay': 0.00031899749982284843, 'num_epochs': 20, 'batch_size': 64, 'only_train_classifier': False, 'sched_patience': 8, 'sched_factor': 0.10626209994205815, 'sched_min_lr': 1e-06, 'train_with_CombinedClassifier': True, 'probabilities': [0.4974986990588113, 0.32371358601351863, 0.8709401155126775], 'config_CombinedClassifier': {'linear_out_1': 512, 'linear_out_2': 64, 'dropout': 0.10626209994205815}}


-------------- Start New Training: ---------------


Epoch 1/20: 100%|██████████| 2250/2250 [00:43<00:00, 51.15batch/s]

Model validation...
100%|██████████| 563/563 [00:09<00:00, 59.02batch/s]
top1_score    0.664475
top3_score    0.681835
top5_score    0.685097
dtype: float64
valid_loss:  2.976194074691718
Learning rate after epoch 1: [1.0094053439404893e-05]
Epoch 2/20: 100%|██████████| 2250/2250 [00:43<00:00, 51.36batch/s]

Model validation...
100%|██████████| 563/563 [00:09<00:00, 58.65batch/s]
top1_score    0.723493
top3_score    0.744305
top5_score    0.747965
dtype: float64
valid_loss:  2.914402423912848
Learning rate after epoch 2: [1.0094053439404893e-05]
Epoch 3/20: 100%|██████████| 2250/2250 [00:44<00:00, 50.95batch/s]

Model validation...
100%|██████████| 563/563 [00:09<00:00, 58.55batch/s]
[I 2025-06-24 23:25:31,574] Trial 12 pruned.
/home/c611/.local/lib/python3.11/site-packages/optuna/trial/_trial.py:682: RuntimeWarning: Inconsistent parameter values for distribution with name "sched_factor"! This might be a configuration mistake. Optuna allows to call the same distribution with the same name more than once in a trial. When the parameter values are inconsistent optuna only uses the values of the first call and ignores all following. Using these values: {'step': None, 'low': 0.0, 'high': 0.4, 'log': False}
  warnings.warn(
top1_score    0.755811
top3_score    0.772666
top5_score    0.774988
dtype: float64
valid_loss:  2.88137279691747
Learning rate after epoch 3: [1.0094053439404893e-05]
{'lr': 0.00027216907123399657, 'weight_decay': 0.0012713982487896174, 'num_epochs': 20, 'batch_size': 64, 'only_train_classifier': False, 'sched_patience': 9, 'sched_factor': 0.1857705762251196, 'sched_min_lr': 1e-06, 'train_with_CombinedClassifier': True, 'probabilities': [0.23961190840944047, 0.6469050835432602, 0.3514961926373278], 'config_CombinedClassifier': {'linear_out_1': 256, 'linear_out_2': 128, 'dropout': 0.1857705762251196}}


-------------- Start New Training: ---------------


Epoch 1/20: 100%|██████████| 2250/2250 [00:43<00:00, 51.78batch/s]

Model validation...
100%|██████████| 563/563 [00:09<00:00, 59.58batch/s]
top1_score    0.678796
top3_score    0.709199
top5_score    0.713007
dtype: float64
valid_loss:  2.954829866466895
Learning rate after epoch 1: [0.00027216907123399657]
Epoch 2/20: 100%|██████████| 2250/2250 [00:43<00:00, 51.81batch/s]

Model validation...
100%|██████████| 563/563 [00:09<00:00, 59.92batch/s]
top1_score    0.704788
top3_score    0.727025
top5_score    0.731514
dtype: float64
valid_loss:  2.9253692461796383
Learning rate after epoch 2: [0.00027216907123399657]
Epoch 3/20: 100%|██████████| 2250/2250 [00:43<00:00, 51.32batch/s]

Model validation...
100%|██████████| 563/563 [00:09<00:00, 59.77batch/s]
[I 2025-06-24 23:28:10,878] Trial 13 pruned.
/home/c611/.local/lib/python3.11/site-packages/optuna/trial/_trial.py:682: RuntimeWarning: Inconsistent parameter values for distribution with name "sched_factor"! This might be a configuration mistake. Optuna allows to call the same distribution with the same name more than once in a trial. When the parameter values are inconsistent optuna only uses the values of the first call and ignores all following. Using these values: {'step': None, 'low': 0.0, 'high': 0.4, 'log': False}
  warnings.warn(
top1_score    0.713820
top3_score    0.734835
top5_score    0.736794
dtype: float64
valid_loss:  2.915965205610963
Learning rate after epoch 3: [0.00027216907123399657]
{'lr': 0.00013463875075697736, 'weight_decay': 0.00011624311678626046, 'num_epochs': 20, 'batch_size': 64, 'only_train_classifier': False, 'sched_patience': 7, 'sched_factor': 0.2536858615531026, 'sched_min_lr': 1e-05, 'train_with_CombinedClassifier': True, 'probabilities': [0.98765725317393, 0.3698006159991696, 0.4999043075388836], 'config_CombinedClassifier': {'linear_out_1': 128, 'linear_out_2': 64, 'dropout': 0.2536858615531026}}


-------------- Start New Training: ---------------


Epoch 1/20: 100%|██████████| 2250/2250 [00:43<00:00, 51.46batch/s]

Model validation...
100%|██████████| 563/563 [00:09<00:00, 59.80batch/s]
top1_score    0.704641
top3_score    0.716428
top5_score    0.719137
dtype: float64
valid_loss:  2.9272772139384946
Learning rate after epoch 1: [0.00013463875075697736]
Epoch 2/20: 100%|██████████| 2250/2250 [00:43<00:00, 51.66batch/s]

Model validation...
100%|██████████| 563/563 [00:09<00:00, 59.84batch/s]
top1_score    0.731656
top3_score    0.740878
top5_score    0.742496
dtype: float64
valid_loss:  2.899459627974732
Learning rate after epoch 2: [0.00013463875075697736]
Epoch 3/20: 100%|██████████| 2250/2250 [00:43<00:00, 51.25batch/s]

Model validation...
100%|██████████| 563/563 [00:09<00:00, 59.49batch/s]
[I 2025-06-24 23:30:50,657] Trial 14 pruned.
/home/c611/.local/lib/python3.11/site-packages/optuna/trial/_trial.py:682: RuntimeWarning: Inconsistent parameter values for distribution with name "sched_factor"! This might be a configuration mistake. Optuna allows to call the same distribution with the same name more than once in a trial. When the parameter values are inconsistent optuna only uses the values of the first call and ignores all following. Using these values: {'step': None, 'low': 0.0, 'high': 0.4, 'log': False}
  warnings.warn(
top1_score    0.735390
top3_score    0.753851
top5_score    0.755830
dtype: float64
valid_loss:  2.896217884856473
Learning rate after epoch 3: [0.00013463875075697736]
{'lr': 0.0008177807225795572, 'weight_decay': 0.00027271799238884705, 'num_epochs': 20, 'batch_size': 64, 'only_train_classifier': False, 'sched_patience': 6, 'sched_factor': 0.1592788204457131, 'sched_min_lr': 1e-06, 'train_with_CombinedClassifier': True, 'probabilities': [0.6523804214151515, 0.9766230661047433, 0.2829873494353498], 'config_CombinedClassifier': {'linear_out_1': 512, 'linear_out_2': 128, 'dropout': 0.1592788204457131}}


-------------- Start New Training: ---------------


Epoch 1/20: 100%|██████████| 2250/2250 [00:43<00:00, 51.46batch/s]

Model validation...
100%|██████████| 563/563 [00:09<00:00, 59.26batch/s]
top1_score    0.389814
top3_score    0.442935
top5_score    0.451438
dtype: float64
valid_loss:  3.238859443003924
Learning rate after epoch 1: [0.0008177807225795572]
Epoch 2/20: 100%|██████████| 2250/2250 [00:43<00:00, 51.94batch/s]

Model validation...
100%|██████████| 563/563 [00:09<00:00, 59.93batch/s]
top1_score    0.326608
top3_score    0.360396
top5_score    0.370535
dtype: float64
valid_loss:  3.3006782231068317
Learning rate after epoch 2: [0.0008177807225795572]
Epoch 3/20: 100%|██████████| 2250/2250 [00:43<00:00, 51.67batch/s]

Model validation...
100%|██████████| 563/563 [00:09<00:00, 59.77batch/s]
[I 2025-06-24 23:33:29,873] Trial 15 pruned.
/home/c611/.local/lib/python3.11/site-packages/optuna/trial/_trial.py:682: RuntimeWarning: Inconsistent parameter values for distribution with name "sched_factor"! This might be a configuration mistake. Optuna allows to call the same distribution with the same name more than once in a trial. When the parameter values are inconsistent optuna only uses the values of the first call and ignores all following. Using these values: {'step': None, 'low': 0.0, 'high': 0.4, 'log': False}
  warnings.warn(
top1_score    0.465898
top3_score    0.495556
top5_score    0.499296
dtype: float64
valid_loss:  3.1598555414968867
Learning rate after epoch 3: [0.0008177807225795572]
{'lr': 8.737174674429033e-05, 'weight_decay': 0.0007209551271317763, 'num_epochs': 20, 'batch_size': 64, 'only_train_classifier': False, 'sched_patience': 3, 'sched_factor': 0.07391249373589093, 'sched_min_lr': 1e-06, 'train_with_CombinedClassifier': True, 'probabilities': [0.3973270323692488, 0.690072469891156, 0.7751231802691462], 'config_CombinedClassifier': {'linear_out_1': 256, 'linear_out_2': 64, 'dropout': 0.07391249373589093}}


-------------- Start New Training: ---------------


Epoch 1/20: 100%|██████████| 2250/2250 [00:43<00:00, 51.18batch/s]

Model validation...
100%|██████████| 563/563 [00:09<00:00, 59.12batch/s]
top1_score    0.753617
top3_score    0.776307
top5_score    0.778539
dtype: float64
valid_loss:  2.88123140834788
Learning rate after epoch 1: [8.737174674429033e-05]
Epoch 2/20: 100%|██████████| 2250/2250 [00:43<00:00, 51.16batch/s]

Model validation...
100%|██████████| 563/563 [00:09<00:00, 59.17batch/s]
top1_score    0.747441
top3_score    0.772273
top5_score    0.774604
dtype: float64
valid_loss:  2.8861206534912487
Learning rate after epoch 2: [8.737174674429033e-05]
Epoch 3/20: 100%|██████████| 2250/2250 [00:44<00:00, 51.11batch/s]

Model validation...
100%|██████████| 563/563 [00:09<00:00, 58.63batch/s]
top1_score    0.773416
top3_score    0.791878
top5_score    0.794951
dtype: float64
valid_loss:  2.8591832449762267
Learning rate after epoch 3: [8.737174674429033e-05]
Epoch 4/20: 100%|██████████| 2250/2250 [00:43<00:00, 51.41batch/s]

Model validation...
100%|██████████| 563/563 [00:09<00:00, 59.39batch/s]
top1_score    0.778151
top3_score    0.796418
top5_score    0.799253
dtype: float64
valid_loss:  2.853804093694602
Learning rate after epoch 4: [8.737174674429033e-05]
Epoch 5/20: 100%|██████████| 2250/2250 [00:43<00:00, 51.50batch/s]

Model validation...
100%|██████████| 563/563 [00:09<00:00, 59.30batch/s]
top1_score    0.780415
top3_score    0.798307
top5_score    0.800652
dtype: float64
valid_loss:  2.851467290102483
Learning rate after epoch 5: [8.737174674429033e-05]
Epoch 6/20: 100%|██████████| 2250/2250 [00:43<00:00, 51.36batch/s]

Model validation...
100%|██████████| 563/563 [00:09<00:00, 59.18batch/s]
[I 2025-06-24 23:38:50,631] Trial 16 pruned.
/home/c611/.local/lib/python3.11/site-packages/optuna/trial/_trial.py:682: RuntimeWarning: Inconsistent parameter values for distribution with name "sched_factor"! This might be a configuration mistake. Optuna allows to call the same distribution with the same name more than once in a trial. When the parameter values are inconsistent optuna only uses the values of the first call and ignores all following. Using these values: {'step': None, 'low': 0.0, 'high': 0.4, 'log': False}
  warnings.warn(
top1_score    0.772305
top3_score    0.792152
top5_score    0.796609
dtype: float64
valid_loss:  2.859576304894781
Learning rate after epoch 6: [8.737174674429033e-05]
{'lr': 0.0005887578005299481, 'weight_decay': 0.0011004507999497217, 'num_epochs': 20, 'batch_size': 64, 'only_train_classifier': False, 'sched_patience': 9, 'sched_factor': 0.3645415599198094, 'sched_min_lr': 1e-06, 'train_with_CombinedClassifier': True, 'probabilities': [0.28743555869315895, 0.10743994655713376, 0.5649226390840605], 'config_CombinedClassifier': {'linear_out_1': 256, 'linear_out_2': 128, 'dropout': 0.3645415599198094}}


-------------- Start New Training: ---------------


Epoch 1/20: 100%|██████████| 2250/2250 [00:43<00:00, 51.30batch/s]

Model validation...
100%|██████████| 563/563 [00:09<00:00, 58.35batch/s]
top1_score    0.619913
top3_score    0.637354
top5_score    0.640033
dtype: float64
valid_loss:  3.0128548153763766
Learning rate after epoch 1: [0.0005887578005299481]
Epoch 2/20: 100%|██████████| 2250/2250 [00:44<00:00, 50.82batch/s]

Model validation...
100%|██████████| 563/563 [00:09<00:00, 58.86batch/s]
top1_score    0.648086
top3_score    0.664114
top5_score    0.667896
dtype: float64
valid_loss:  2.984942466708737
Learning rate after epoch 2: [0.0005887578005299481]
Epoch 3/20: 100%|██████████| 2250/2250 [00:44<00:00, 50.87batch/s]

Model validation...
100%|██████████| 563/563 [00:09<00:00, 58.86batch/s]
[I 2025-06-24 23:41:32,157] Trial 17 pruned.
/home/c611/.local/lib/python3.11/site-packages/optuna/trial/_trial.py:682: RuntimeWarning: Inconsistent parameter values for distribution with name "sched_factor"! This might be a configuration mistake. Optuna allows to call the same distribution with the same name more than once in a trial. When the parameter values are inconsistent optuna only uses the values of the first call and ignores all following. Using these values: {'step': None, 'low': 0.0, 'high': 0.4, 'log': False}
  warnings.warn(
top1_score    0.661044
top3_score    0.674980
top5_score    0.679557
dtype: float64
valid_loss:  2.9720020937877267
Learning rate after epoch 3: [0.0005887578005299481]
{'lr': 1.1652296623436675e-05, 'weight_decay': 0.00022686506382767094, 'num_epochs': 20, 'batch_size': 32, 'only_train_classifier': False, 'sched_patience': 6, 'sched_factor': 0.09099355841693307, 'sched_min_lr': 1e-05, 'train_with_CombinedClassifier': True, 'probabilities': [0.7068148894406593, 0.2901762166893156, 0.2407553361897768], 'config_CombinedClassifier': {'linear_out_1': 512, 'linear_out_2': 64, 'dropout': 0.09099355841693307}}


-------------- Start New Training: ---------------


Epoch 1/20: 100%|██████████| 4500/4500 [00:57<00:00, 78.71batch/s]

Model validation...
100%|██████████| 1125/1125 [00:10<00:00, 103.58batch/s]
top1_score    0.740247
top3_score    0.762825
top5_score    0.765980
dtype: float64
valid_loss:  2.90059499168396
Learning rate after epoch 1: [1.1652296623436675e-05]
Epoch 2/20: 100%|██████████| 4500/4500 [00:57<00:00, 78.46batch/s]

Model validation...
100%|██████████| 1125/1125 [00:10<00:00, 103.19batch/s]
top1_score    0.769207
top3_score    0.786197
top5_score    0.788192
dtype: float64
valid_loss:  2.8681514934963652
Learning rate after epoch 2: [1.1652296623436675e-05]
Epoch 3/20: 100%|██████████| 4500/4500 [00:57<00:00, 78.11batch/s]

Model validation...
100%|██████████| 1125/1125 [00:10<00:00, 103.58batch/s]
top1_score    0.773841
top3_score    0.788751
top5_score    0.790439
dtype: float64
valid_loss:  2.8614900529649523
Learning rate after epoch 3: [1.1652296623436675e-05]
Epoch 4/20: 100%|██████████| 4500/4500 [00:57<00:00, 78.59batch/s]

Model validation...
100%|██████████| 1125/1125 [00:10<00:00, 103.18batch/s]
top1_score    0.775828
top3_score    0.789346
top5_score    0.790681
dtype: float64
valid_loss:  2.8592928451961943
Learning rate after epoch 4: [1.1652296623436675e-05]
Epoch 5/20: 100%|██████████| 4500/4500 [00:57<00:00, 78.31batch/s]

Model validation...
100%|██████████| 1125/1125 [00:10<00:00, 103.82batch/s]
top1_score    0.791314
top3_score    0.809404
top5_score    0.813287
dtype: float64
valid_loss:  2.8438433344099256
Learning rate after epoch 5: [1.1652296623436675e-05]
Epoch 6/20: 100%|██████████| 4500/4500 [00:57<00:00, 78.49batch/s]

Model validation...
100%|██████████| 1125/1125 [00:10<00:00, 103.38batch/s]
top1_score    0.799846
top3_score    0.816262
top5_score    0.818572
dtype: float64
valid_loss:  2.83500398657057
Learning rate after epoch 6: [1.1652296623436675e-05]
Epoch 7/20: 100%|██████████| 4500/4500 [00:57<00:00, 78.77batch/s]

Model validation...
100%|██████████| 1125/1125 [00:10<00:00, 103.35batch/s]
top1_score    0.801503
top3_score    0.817397
top5_score    0.820013
dtype: float64
valid_loss:  2.832951510535346
Learning rate after epoch 7: [1.1652296623436675e-05]
Epoch 8/20: 100%|██████████| 4500/4500 [00:57<00:00, 78.41batch/s]

Model validation...
100%|██████████| 1125/1125 [00:10<00:00, 102.62batch/s]
top1_score    0.816515
top3_score    0.831392
top5_score    0.833489
dtype: float64
valid_loss:  2.8188924600813126
Learning rate after epoch 8: [1.1652296623436675e-05]
Epoch 9/20: 100%|██████████| 4500/4500 [00:57<00:00, 78.16batch/s]

Model validation...
100%|██████████| 1125/1125 [00:10<00:00, 103.04batch/s]
top1_score    0.815351
top3_score    0.830461
top5_score    0.833154
dtype: float64
valid_loss:  2.8193689200083414
Learning rate after epoch 9: [1.1652296623436675e-05]
Epoch 10/20: 100%|██████████| 4500/4500 [00:57<00:00, 78.18batch/s]

Model validation...
100%|██████████| 1125/1125 [00:10<00:00, 103.93batch/s]
top1_score    0.818235
top3_score    0.832637
top5_score    0.834320
dtype: float64
valid_loss:  2.815504813724094
Learning rate after epoch 10: [1.1652296623436675e-05]
Epoch 11/20: 100%|██████████| 4500/4500 [00:57<00:00, 77.87batch/s]

Model validation...
100%|██████████| 1125/1125 [00:10<00:00, 103.01batch/s]
top1_score    0.821527
top3_score    0.835234
top5_score    0.836970
dtype: float64
valid_loss:  2.813158459133572
Learning rate after epoch 11: [1.1652296623436675e-05]
Epoch 12/20: 100%|██████████| 4500/4500 [00:57<00:00, 78.46batch/s]

Model validation...
100%|██████████| 1125/1125 [00:10<00:00, 103.66batch/s]
top1_score    0.820020
top3_score    0.833299
top5_score    0.835279
dtype: float64
valid_loss:  2.8136523030598957
Learning rate after epoch 12: [1.1652296623436675e-05]
Epoch 13/20: 100%|██████████| 4500/4500 [00:57<00:00, 77.80batch/s]

Model validation...
100%|██████████| 1125/1125 [00:10<00:00, 102.62batch/s]
top1_score    0.821519
top3_score    0.834682
top5_score    0.836364
dtype: float64
valid_loss:  2.8123528785705565
Learning rate after epoch 13: [1.1652296623436675e-05]
Epoch 14/20: 100%|██████████| 4500/4500 [00:57<00:00, 78.36batch/s]

Model validation...
100%|██████████| 1125/1125 [00:10<00:00, 104.06batch/s]
top1_score    0.823374
top3_score    0.836381
top5_score    0.837869
dtype: float64
valid_loss:  2.8105726799435087
Learning rate after epoch 14: [1.1652296623436675e-05]
Epoch 15/20: 100%|██████████| 4500/4500 [00:57<00:00, 78.11batch/s]

Model validation...
100%|██████████| 1125/1125 [00:11<00:00, 102.02batch/s]
top1_score    0.822732
top3_score    0.834101
top5_score    0.835535
dtype: float64
valid_loss:  2.810956379360623
Learning rate after epoch 15: [1.1652296623436675e-05]
Epoch 16/20: 100%|██████████| 4500/4500 [00:57<00:00, 78.50batch/s]

Model validation...
100%|██████████| 1125/1125 [00:10<00:00, 103.15batch/s]
top1_score    0.824170
top3_score    0.836252
top5_score    0.837490
dtype: float64
valid_loss:  2.8092637793223063
Learning rate after epoch 16: [1.1652296623436675e-05]
Epoch 17/20: 100%|██████████| 4500/4500 [00:57<00:00, 78.21batch/s]

Model validation...
100%|██████████| 1125/1125 [00:10<00:00, 103.81batch/s]
top1_score    0.824316
top3_score    0.835614
top5_score    0.837036
dtype: float64
valid_loss:  2.8090970397525363
Learning rate after epoch 17: [1.1652296623436675e-05]
Epoch 18/20: 100%|██████████| 4500/4500 [00:57<00:00, 78.61batch/s]

Model validation...
100%|██████████| 1125/1125 [00:10<00:00, 103.69batch/s]
top1_score    0.824775
top3_score    0.836296
top5_score    0.837771
dtype: float64
valid_loss:  2.808773140801324
Learning rate after epoch 18: [1.1652296623436675e-05]
Epoch 19/20: 100%|██████████| 4500/4500 [00:57<00:00, 78.24batch/s]

Model validation...
100%|██████████| 1125/1125 [00:10<00:00, 103.48batch/s]
top1_score    0.825246
top3_score    0.836666
top5_score    0.838219
dtype: float64
valid_loss:  2.808189716127184
Learning rate after epoch 19: [1.1652296623436675e-05]
Epoch 20/20: 100%|██████████| 4500/4500 [00:57<00:00, 78.17batch/s]

Model validation...
100%|██████████| 1125/1125 [00:10<00:00, 103.47batch/s]
[I 2025-06-25 00:04:19,761] Trial 18 finished with value: 0.8252463936805725 and parameters: {'linear_out_1': 512, 'linear_out_2': 64, 'sched_factor': 0.09099355841693307, 'lr': 1.1652296623436675e-05, 'weight_decay': 0.00022686506382767094, 'batch_size': 32, 'sched_patience': 6, 'sched_min_lr': 1e-05, 'a1': 0.7068148894406593, 'a2': 0.2901762166893156, 'a3': 0.2407553361897768}. Best is trial 7 with value: 0.8432953357696533.
/home/c611/.local/lib/python3.11/site-packages/optuna/trial/_trial.py:682: RuntimeWarning: Inconsistent parameter values for distribution with name "sched_factor"! This might be a configuration mistake. Optuna allows to call the same distribution with the same name more than once in a trial. When the parameter values are inconsistent optuna only uses the values of the first call and ignores all following. Using these values: {'step': None, 'low': 0.0, 'high': 0.4, 'log': False}
  warnings.warn(
top1_score    0.822830
top3_score    0.833558
top5_score    0.834812
dtype: float64
valid_loss:  2.8107614998287627
Learning rate after epoch 20: [1.1652296623436675e-05]
Training finished!
{'lr': 1.1165313560627098e-05, 'weight_decay': 0.0005232943643857126, 'num_epochs': 20, 'batch_size': 32, 'only_train_classifier': False, 'sched_patience': 6, 'sched_factor': 0.007746669985587776, 'sched_min_lr': 1e-05, 'train_with_CombinedClassifier': True, 'probabilities': [0.11597900121093135, 0.2779726185594881, 0.2582623944533247], 'config_CombinedClassifier': {'linear_out_1': 128, 'linear_out_2': 256, 'dropout': 0.007746669985587776}}


-------------- Start New Training: ---------------


Epoch 1/20: 100%|██████████| 4500/4500 [00:57<00:00, 78.13batch/s]

Model validation...
100%|██████████| 1125/1125 [00:10<00:00, 102.63batch/s]
top1_score    0.725665
top3_score    0.751301
top5_score    0.754850
dtype: float64
valid_loss:  2.912136598587036
Learning rate after epoch 1: [1.1165313560627098e-05]
Epoch 2/20: 100%|██████████| 4500/4500 [00:57<00:00, 78.50batch/s]

Model validation...
100%|██████████| 1125/1125 [00:10<00:00, 103.55batch/s]
top1_score    0.735347
top3_score    0.756573
top5_score    0.758571
dtype: float64
valid_loss:  2.897212997012668
Learning rate after epoch 2: [1.1165313560627098e-05]
Epoch 3/20: 100%|██████████| 4500/4500 [00:57<00:00, 78.35batch/s]

Model validation...
100%|██████████| 1125/1125 [00:10<00:00, 102.45batch/s]
top1_score    0.761155
top3_score    0.781823
top5_score    0.784058
dtype: float64
valid_loss:  2.8709103815290664
Learning rate after epoch 3: [1.1165313560627098e-05]
Epoch 4/20: 100%|██████████| 4500/4500 [00:57<00:00, 78.17batch/s]

Model validation...
100%|██████████| 1125/1125 [00:10<00:00, 102.77batch/s]
top1_score    0.784584
top3_score    0.807594
top5_score    0.811046
dtype: float64
valid_loss:  2.8483559197319877
Learning rate after epoch 4: [1.1165313560627098e-05]
Epoch 5/20: 100%|██████████| 4500/4500 [00:57<00:00, 77.85batch/s]

Model validation...
100%|██████████| 1125/1125 [00:10<00:00, 103.85batch/s]
top1_score    0.787026
top3_score    0.809166
top5_score    0.813315
dtype: float64
valid_loss:  2.8446768385569254
Learning rate after epoch 5: [1.1165313560627098e-05]
Epoch 6/20: 100%|██████████| 4500/4500 [00:57<00:00, 78.06batch/s]

Model validation...
100%|██████████| 1125/1125 [00:11<00:00, 100.43batch/s]
top1_score    0.810533
top3_score    0.835179
top5_score    0.839438
dtype: float64
valid_loss:  2.8231578278011744
Learning rate after epoch 6: [1.1165313560627098e-05]
Epoch 7/20: 100%|██████████| 4500/4500 [00:57<00:00, 78.07batch/s]

Model validation...
100%|██████████| 1125/1125 [00:11<00:00, 101.85batch/s]
top1_score    0.819030
top3_score    0.841537
top5_score    0.844740
dtype: float64
valid_loss:  2.813900599161784
Learning rate after epoch 7: [1.1165313560627098e-05]
Epoch 8/20: 100%|██████████| 4500/4500 [00:57<00:00, 77.87batch/s]

Model validation...
100%|██████████| 1125/1125 [00:10<00:00, 102.74batch/s]
top1_score    0.819203
top3_score    0.839667
top5_score    0.842303
dtype: float64
valid_loss:  2.8137449997795954
Learning rate after epoch 8: [1.1165313560627098e-05]
Epoch 9/20: 100%|██████████| 4500/4500 [00:57<00:00, 77.95batch/s]

Model validation...
100%|██████████| 1125/1125 [00:10<00:00, 102.55batch/s]
top1_score    0.821830
top3_score    0.843171
top5_score    0.845098
dtype: float64
valid_loss:  2.8111871174706353
Learning rate after epoch 9: [1.1165313560627098e-05]
Epoch 10/20: 100%|██████████| 4500/4500 [00:57<00:00, 77.76batch/s]

Model validation...
100%|██████████| 1125/1125 [00:10<00:00, 103.14batch/s]
top1_score    0.822356
top3_score    0.842396
top5_score    0.844813
dtype: float64
valid_loss:  2.8094856537712944
Learning rate after epoch 10: [1.1165313560627098e-05]
Epoch 11/20: 100%|██████████| 4500/4500 [00:57<00:00, 77.74batch/s]

Model validation...
100%|██████████| 1125/1125 [00:10<00:00, 102.79batch/s]
top1_score    0.822851
top3_score    0.840506
top5_score    0.842966
dtype: float64
valid_loss:  2.8090671026441787
Learning rate after epoch 11: [1.1165313560627098e-05]
Epoch 12/20: 100%|██████████| 4500/4500 [00:57<00:00, 77.91batch/s]

Model validation...
100%|██████████| 1125/1125 [00:10<00:00, 102.75batch/s]
top1_score    0.823717
top3_score    0.842069
top5_score    0.844758
dtype: float64
valid_loss:  2.8079896420372856
Learning rate after epoch 12: [1.1165313560627098e-05]
Epoch 13/20: 100%|██████████| 4500/4500 [00:57<00:00, 78.15batch/s]

Model validation...
100%|██████████| 1125/1125 [00:11<00:00, 101.26batch/s]
top1_score    0.835803
top3_score    0.855166
top5_score    0.858014
dtype: float64
valid_loss:  2.7961716611650256
Learning rate after epoch 13: [1.1165313560627098e-05]
Epoch 14/20: 100%|██████████| 4500/4500 [00:57<00:00, 77.90batch/s]

Model validation...
100%|██████████| 1125/1125 [00:10<00:00, 102.59batch/s]
top1_score    0.834892
top3_score    0.855111
top5_score    0.857454
dtype: float64
valid_loss:  2.7969189817640516
Learning rate after epoch 14: [1.1165313560627098e-05]
Epoch 15/20: 100%|██████████| 4500/4500 [00:57<00:00, 77.79batch/s]

Model validation...
100%|██████████| 1125/1125 [00:10<00:00, 102.38batch/s]
top1_score    0.837461
top3_score    0.856990
top5_score    0.859430
dtype: float64
valid_loss:  2.794024668375651
Learning rate after epoch 15: [1.1165313560627098e-05]
Epoch 16/20: 100%|██████████| 4500/4500 [00:57<00:00, 77.98batch/s]

Model validation...
100%|██████████| 1125/1125 [00:10<00:00, 102.90batch/s]
top1_score    0.837659
top3_score    0.854318
top5_score    0.856755
dtype: float64
valid_loss:  2.7938184627956812
Learning rate after epoch 16: [1.1165313560627098e-05]
Epoch 17/20: 100%|██████████| 4500/4500 [00:57<00:00, 78.18batch/s]

Model validation...
100%|██████████| 1125/1125 [00:10<00:00, 103.28batch/s]
top1_score    0.837058
top3_score    0.856047
top5_score    0.858380
dtype: float64
valid_loss:  2.7947017262776694
Learning rate after epoch 17: [1.1165313560627098e-05]
Epoch 18/20: 100%|██████████| 4500/4500 [00:57<00:00, 77.90batch/s]

Model validation...
100%|██████████| 1125/1125 [00:10<00:00, 102.47batch/s]
top1_score    0.836556
top3_score    0.854201
top5_score    0.856816
dtype: float64
valid_loss:  2.795310170915392
Learning rate after epoch 18: [1.1165313560627098e-05]
Epoch 19/20: 100%|██████████| 4500/4500 [00:57<00:00, 78.16batch/s]

Model validation...
100%|██████████| 1125/1125 [00:10<00:00, 103.32batch/s]
top1_score    0.839379
top3_score    0.856773
top5_score    0.859027
dtype: float64
valid_loss:  2.791951058069865
Learning rate after epoch 19: [1.1165313560627098e-05]
Epoch 20/20: 100%|██████████| 4500/4500 [00:57<00:00, 78.04batch/s]

Model validation...
100%|██████████| 1125/1125 [00:10<00:00, 102.96batch/s]
[I 2025-06-25 00:27:13,295] Trial 19 finished with value: 0.839378833770752 and parameters: {'linear_out_1': 128, 'linear_out_2': 256, 'sched_factor': 0.007746669985587776, 'lr': 1.1165313560627098e-05, 'weight_decay': 0.0005232943643857126, 'batch_size': 32, 'sched_patience': 6, 'sched_min_lr': 1e-05, 'a1': 0.11597900121093135, 'a2': 0.2779726185594881, 'a3': 0.2582623944533247}. Best is trial 7 with value: 0.8432953357696533.
/home/c611/.local/lib/python3.11/site-packages/optuna/trial/_trial.py:682: RuntimeWarning: Inconsistent parameter values for distribution with name "sched_factor"! This might be a configuration mistake. Optuna allows to call the same distribution with the same name more than once in a trial. When the parameter values are inconsistent optuna only uses the values of the first call and ignores all following. Using these values: {'step': None, 'low': 0.0, 'high': 0.4, 'log': False}
  warnings.warn(
top1_score    0.837977
top3_score    0.855385
top5_score    0.857538
dtype: float64
valid_loss:  2.7931140467325846
Learning rate after epoch 20: [1.1165313560627098e-05]
Training finished!
{'lr': 3.6855481640118005e-05, 'weight_decay': 0.0005711677639915223, 'num_epochs': 20, 'batch_size': 32, 'only_train_classifier': False, 'sched_patience': 8, 'sched_factor': 0.0430134172834553, 'sched_min_lr': 1e-05, 'train_with_CombinedClassifier': True, 'probabilities': [0.1669883275818467, 0.4480602266252337, 0.26143213740380605], 'config_CombinedClassifier': {'linear_out_1': 128, 'linear_out_2': 256, 'dropout': 0.0430134172834553}}


-------------- Start New Training: ---------------


Epoch 1/20: 100%|██████████| 4500/4500 [00:57<00:00, 77.92batch/s]

Model validation...
100%|██████████| 1125/1125 [00:10<00:00, 104.03batch/s]
top1_score    0.769076
top3_score    0.793488
top5_score    0.795974
dtype: float64
valid_loss:  2.8658248767852785
Learning rate after epoch 1: [3.6855481640118005e-05]
Epoch 2/20: 100%|██████████| 4500/4500 [00:57<00:00, 78.32batch/s]

Model validation...
100%|██████████| 1125/1125 [00:11<00:00, 101.76batch/s]
top1_score    0.795916
top3_score    0.820252
top5_score    0.823579
dtype: float64
valid_loss:  2.83748720826043
Learning rate after epoch 2: [3.6855481640118005e-05]
Epoch 3/20: 100%|██████████| 4500/4500 [00:57<00:00, 77.73batch/s]

Model validation...
100%|██████████| 1125/1125 [00:10<00:00, 103.46batch/s]
top1_score    0.803324
top3_score    0.824417
top5_score    0.826730
dtype: float64
valid_loss:  2.830108512878418
Learning rate after epoch 3: [3.6855481640118005e-05]
Epoch 4/20: 100%|██████████| 4500/4500 [00:58<00:00, 77.09batch/s]

Model validation...
100%|██████████| 1125/1125 [00:11<00:00, 101.84batch/s]
top1_score    0.806635
top3_score    0.829600
top5_score    0.832155
dtype: float64
valid_loss:  2.825459806230333
Learning rate after epoch 4: [3.6855481640118005e-05]
Epoch 5/20: 100%|██████████| 4500/4500 [00:57<00:00, 78.41batch/s]

Model validation...
100%|██████████| 1125/1125 [00:10<00:00, 102.86batch/s]
top1_score    0.818343
top3_score    0.837586
top5_score    0.840163
dtype: float64
valid_loss:  2.8137179938422308
Learning rate after epoch 5: [3.6855481640118005e-05]
Epoch 6/20: 100%|██████████| 4500/4500 [00:57<00:00, 77.95batch/s]

Model validation...
100%|██████████| 1125/1125 [00:10<00:00, 102.55batch/s]
top1_score    0.822277
top3_score    0.843649
top5_score    0.845909
dtype: float64
valid_loss:  2.8103296012878416
Learning rate after epoch 6: [3.6855481640118005e-05]
Epoch 7/20: 100%|██████████| 4500/4500 [00:58<00:00, 76.56batch/s]

Model validation...
100%|██████████| 1125/1125 [00:10<00:00, 103.25batch/s]
top1_score    0.821711
top3_score    0.841825
top5_score    0.843396
dtype: float64
valid_loss:  2.810846177207099
Learning rate after epoch 7: [3.6855481640118005e-05]
Epoch 8/20: 100%|██████████| 4500/4500 [00:57<00:00, 78.18batch/s]

Model validation...
100%|██████████| 1125/1125 [00:10<00:00, 104.30batch/s]
top1_score    0.819184
top3_score    0.839369
top5_score    0.841346
dtype: float64
valid_loss:  2.813261263317532
Learning rate after epoch 8: [3.6855481640118005e-05]
Epoch 9/20: 100%|██████████| 4500/4500 [00:57<00:00, 78.89batch/s]

Model validation...
100%|██████████| 1125/1125 [00:10<00:00, 102.79batch/s]
top1_score    0.826319
top3_score    0.844294
top5_score    0.846265
dtype: float64
valid_loss:  2.805480943467882
Learning rate after epoch 9: [3.6855481640118005e-05]
Epoch 10/20: 100%|██████████| 4500/4500 [00:57<00:00, 77.99batch/s]

Model validation...
100%|██████████| 1125/1125 [00:10<00:00, 103.93batch/s]
top1_score    0.826558
top3_score    0.842570
top5_score    0.844304
dtype: float64
valid_loss:  2.805312009599474
Learning rate after epoch 10: [3.6855481640118005e-05]
Epoch 11/20: 100%|██████████| 4500/4500 [00:57<00:00, 78.22batch/s]

Model validation...
100%|██████████| 1125/1125 [00:10<00:00, 103.12batch/s]
top1_score    0.825777
top3_score    0.841739
top5_score    0.843581
dtype: float64
valid_loss:  2.8056217488182917
Learning rate after epoch 11: [3.6855481640118005e-05]
Epoch 12/20: 100%|██████████| 4500/4500 [00:57<00:00, 78.31batch/s]

Model validation...
100%|██████████| 1125/1125 [00:10<00:00, 103.57batch/s]
top1_score    0.825062
top3_score    0.842536
top5_score    0.844833
dtype: float64
valid_loss:  2.807052552329169
Learning rate after epoch 12: [3.6855481640118005e-05]
Epoch 13/20: 100%|██████████| 4500/4500 [00:57<00:00, 78.15batch/s]

Model validation...
100%|██████████| 1125/1125 [00:10<00:00, 104.12batch/s]
top1_score    0.824497
top3_score    0.841792
top5_score    0.843515
dtype: float64
valid_loss:  2.8074012518988716
Learning rate after epoch 13: [3.6855481640118005e-05]
Epoch 14/20: 100%|██████████| 4500/4500 [00:57<00:00, 78.18batch/s]

Model validation...
100%|██████████| 1125/1125 [00:10<00:00, 102.69batch/s]
top1_score    0.829887
top3_score    0.843832
top5_score    0.845849
dtype: float64
valid_loss:  2.801185474395752
Learning rate after epoch 14: [3.6855481640118005e-05]
Epoch 15/20: 100%|██████████| 4500/4500 [00:58<00:00, 77.57batch/s]

Model validation...
100%|██████████| 1125/1125 [00:10<00:00, 102.73batch/s]
top1_score    0.830054
top3_score    0.844998
top5_score    0.847025
dtype: float64
valid_loss:  2.801312892489963
Learning rate after epoch 15: [3.6855481640118005e-05]
Epoch 16/20: 100%|██████████| 4500/4500 [00:57<00:00, 78.16batch/s]

Model validation...
100%|██████████| 1125/1125 [00:10<00:00, 102.88batch/s]
top1_score    0.828291
top3_score    0.843195
top5_score    0.844863
dtype: float64
valid_loss:  2.803247369978163
Learning rate after epoch 16: [3.6855481640118005e-05]
Epoch 17/20: 100%|██████████| 4500/4500 [00:57<00:00, 78.19batch/s]

Model validation...
100%|██████████| 1125/1125 [00:10<00:00, 104.25batch/s]
top1_score    0.830241
top3_score    0.844878
top5_score    0.846603
dtype: float64
valid_loss:  2.801820200390286
Learning rate after epoch 17: [3.6855481640118005e-05]
Epoch 18/20: 100%|██████████| 4500/4500 [00:57<00:00, 78.13batch/s]

Model validation...
100%|██████████| 1125/1125 [00:11<00:00, 102.10batch/s]
top1_score    0.816834
top3_score    0.833150
top5_score    0.835103
dtype: float64
valid_loss:  2.814762154897054
Learning rate after epoch 18: [3.6855481640118005e-05]
Epoch 19/20: 100%|██████████| 4500/4500 [00:57<00:00, 78.08batch/s]

Model validation...
100%|██████████| 1125/1125 [00:10<00:00, 103.62batch/s]
top1_score    0.824818
top3_score    0.837655
top5_score    0.839545
dtype: float64
valid_loss:  2.806871260960897
Learning rate after epoch 19: [3.6855481640118005e-05]
Epoch 20/20: 100%|██████████| 4500/4500 [00:57<00:00, 78.17batch/s]

Model validation...
100%|██████████| 1125/1125 [00:11<00:00, 102.02batch/s]
[I 2025-06-25 00:50:06,273] Trial 20 finished with value: 0.8307907581329346 and parameters: {'linear_out_1': 128, 'linear_out_2': 256, 'sched_factor': 0.0430134172834553, 'lr': 3.6855481640118005e-05, 'weight_decay': 0.0005711677639915223, 'batch_size': 32, 'sched_patience': 8, 'sched_min_lr': 1e-05, 'a1': 0.1669883275818467, 'a2': 0.4480602266252337, 'a3': 0.26143213740380605}. Best is trial 7 with value: 0.8432953357696533.
/home/c611/.local/lib/python3.11/site-packages/optuna/trial/_trial.py:682: RuntimeWarning: Inconsistent parameter values for distribution with name "sched_factor"! This might be a configuration mistake. Optuna allows to call the same distribution with the same name more than once in a trial. When the parameter values are inconsistent optuna only uses the values of the first call and ignores all following. Using these values: {'step': None, 'low': 0.0, 'high': 0.4, 'log': False}
  warnings.warn(
top1_score    0.830791
top3_score    0.845085
top5_score    0.846463
dtype: float64
valid_loss:  2.800588722017076
Learning rate after epoch 20: [3.6855481640118005e-05]
Training finished!
{'lr': 3.904480230971098e-05, 'weight_decay': 0.0005364554395343626, 'num_epochs': 20, 'batch_size': 32, 'only_train_classifier': False, 'sched_patience': 8, 'sched_factor': 0.016001856333092118, 'sched_min_lr': 1e-05, 'train_with_CombinedClassifier': True, 'probabilities': [0.12856366486647444, 0.44959584670428837, 0.2731960665475962], 'config_CombinedClassifier': {'linear_out_1': 128, 'linear_out_2': 256, 'dropout': 0.016001856333092118}}


-------------- Start New Training: ---------------


Epoch 1/20: 100%|██████████| 4500/4500 [00:57<00:00, 77.98batch/s]

Model validation...
100%|██████████| 1125/1125 [00:10<00:00, 102.78batch/s]
top1_score    0.753690
top3_score    0.774025
top5_score    0.777986
dtype: float64
valid_loss:  2.880661818822225
Learning rate after epoch 1: [3.904480230971098e-05]
Epoch 2/20: 100%|██████████| 4500/4500 [00:57<00:00, 77.61batch/s]

Model validation...
100%|██████████| 1125/1125 [00:10<00:00, 103.54batch/s]
top1_score    0.762023
top3_score    0.778929
top5_score    0.781004
dtype: float64
valid_loss:  2.870274804857042
Learning rate after epoch 2: [3.904480230971098e-05]
Epoch 3/20: 100%|██████████| 4500/4500 [00:58<00:00, 77.33batch/s]

Model validation...
100%|██████████| 1125/1125 [00:10<00:00, 102.99batch/s]
top1_score    0.763728
top3_score    0.779852
top5_score    0.782458
dtype: float64
valid_loss:  2.8688457344902885
Learning rate after epoch 3: [3.904480230971098e-05]
Epoch 4/20: 100%|██████████| 4500/4500 [00:57<00:00, 77.69batch/s]

Model validation...
100%|██████████| 1125/1125 [00:10<00:00, 102.57batch/s]
top1_score    0.780718
top3_score    0.800761
top5_score    0.802927
dtype: float64
valid_loss:  2.850917989518907
Learning rate after epoch 4: [3.904480230971098e-05]
Epoch 5/20: 100%|██████████| 4500/4500 [00:58<00:00, 77.44batch/s]

Model validation...
100%|██████████| 1125/1125 [00:10<00:00, 103.27batch/s]
top1_score    0.783239
top3_score    0.801770
top5_score    0.804088
dtype: float64
valid_loss:  2.848211779064602
Learning rate after epoch 5: [3.904480230971098e-05]
Epoch 6/20: 100%|██████████| 4500/4500 [00:57<00:00, 77.97batch/s]

Model validation...
100%|██████████| 1125/1125 [00:10<00:00, 103.25batch/s]
[I 2025-06-25 00:56:59,794] Trial 21 pruned.
/home/c611/.local/lib/python3.11/site-packages/optuna/trial/_trial.py:682: RuntimeWarning: Inconsistent parameter values for distribution with name "sched_factor"! This might be a configuration mistake. Optuna allows to call the same distribution with the same name more than once in a trial. When the parameter values are inconsistent optuna only uses the values of the first call and ignores all following. Using these values: {'step': None, 'low': 0.0, 'high': 0.4, 'log': False}
  warnings.warn(
top1_score    0.778647
top3_score    0.794044
top5_score    0.796698
dtype: float64
valid_loss:  2.8525492536756727
Learning rate after epoch 6: [3.904480230971098e-05]
{'lr': 1.301933600891142e-05, 'weight_decay': 0.00048818900261904857, 'num_epochs': 20, 'batch_size': 32, 'only_train_classifier': False, 'sched_patience': 9, 'sched_factor': 0.042138229085502556, 'sched_min_lr': 1e-05, 'train_with_CombinedClassifier': True, 'probabilities': [0.1296448857106429, 0.2487363186391227, 0.4311962805107584], 'config_CombinedClassifier': {'linear_out_1': 128, 'linear_out_2': 256, 'dropout': 0.042138229085502556}}


-------------- Start New Training: ---------------


Epoch 1/20: 100%|██████████| 4500/4500 [00:57<00:00, 77.74batch/s]

Model validation...
100%|██████████| 1125/1125 [00:10<00:00, 102.39batch/s]
top1_score    0.730252
top3_score    0.753196
top5_score    0.756058
dtype: float64
valid_loss:  2.9055863179100885
Learning rate after epoch 1: [1.301933600891142e-05]
Epoch 2/20: 100%|██████████| 4500/4500 [00:58<00:00, 77.48batch/s]

Model validation...
100%|██████████| 1125/1125 [00:10<00:00, 102.42batch/s]
top1_score    0.761433
top3_score    0.781935
top5_score    0.783829
dtype: float64
valid_loss:  2.8722782135009766
Learning rate after epoch 2: [1.301933600891142e-05]
Epoch 3/20: 100%|██████████| 4500/4500 [00:57<00:00, 77.77batch/s]

Model validation...
100%|██████████| 1125/1125 [00:11<00:00, 101.97batch/s]
top1_score    0.771034
top3_score    0.789855
top5_score    0.791392
dtype: float64
valid_loss:  2.8621121275160046
Learning rate after epoch 3: [1.301933600891142e-05]
Epoch 4/20: 100%|██████████| 4500/4500 [00:58<00:00, 77.53batch/s]

Model validation...
100%|██████████| 1125/1125 [00:11<00:00, 101.95batch/s]
top1_score    0.787809
top3_score    0.809446
top5_score    0.812167
dtype: float64
valid_loss:  2.8456159468756783
Learning rate after epoch 4: [1.301933600891142e-05]
Epoch 5/20: 100%|██████████| 4500/4500 [00:57<00:00, 78.01batch/s]

Model validation...
100%|██████████| 1125/1125 [00:10<00:00, 102.71batch/s]
top1_score    0.790550
top3_score    0.809353
top5_score    0.811350
dtype: float64
valid_loss:  2.8412467401292587
Learning rate after epoch 5: [1.301933600891142e-05]
Epoch 6/20: 100%|██████████| 4500/4500 [00:57<00:00, 77.93batch/s]

Model validation...
100%|██████████| 1125/1125 [00:11<00:00, 102.12batch/s]
[I 2025-06-25 01:03:53,495] Trial 22 pruned.
/home/c611/.local/lib/python3.11/site-packages/optuna/trial/_trial.py:682: RuntimeWarning: Inconsistent parameter values for distribution with name "sched_factor"! This might be a configuration mistake. Optuna allows to call the same distribution with the same name more than once in a trial. When the parameter values are inconsistent optuna only uses the values of the first call and ignores all following. Using these values: {'step': None, 'low': 0.0, 'high': 0.4, 'log': False}
  warnings.warn(
top1_score    0.792244
top3_score    0.810768
top5_score    0.812863
dtype: float64
valid_loss:  2.839198678546482
Learning rate after epoch 6: [1.301933600891142e-05]
{'lr': 1.9501143259182537e-05, 'weight_decay': 0.0007680210231330233, 'num_epochs': 20, 'batch_size': 32, 'only_train_classifier': False, 'sched_patience': 7, 'sched_factor': 0.040807336501997825, 'sched_min_lr': 1e-05, 'train_with_CombinedClassifier': True, 'probabilities': [0.3027856206241005, 0.6103024297036699, 0.2400782924930901], 'config_CombinedClassifier': {'linear_out_1': 128, 'linear_out_2': 256, 'dropout': 0.040807336501997825}}


-------------- Start New Training: ---------------


Epoch 1/20: 100%|██████████| 4500/4500 [00:57<00:00, 77.69batch/s]

Model validation...
100%|██████████| 1125/1125 [00:10<00:00, 103.45batch/s]
top1_score    0.752442
top3_score    0.771461
top5_score    0.774029
dtype: float64
valid_loss:  2.884387651867337
Learning rate after epoch 1: [1.9501143259182537e-05]
Epoch 2/20: 100%|██████████| 4500/4500 [00:58<00:00, 77.14batch/s]

Model validation...
100%|██████████| 1125/1125 [00:10<00:00, 103.88batch/s]
top1_score    0.761824
top3_score    0.774248
top5_score    0.775944
dtype: float64
valid_loss:  2.871661950853136
Learning rate after epoch 2: [1.9501143259182537e-05]
Epoch 3/20: 100%|██████████| 4500/4500 [00:58<00:00, 77.24batch/s]

Model validation...
100%|██████████| 1125/1125 [00:10<00:00, 102.31batch/s]
top1_score    0.767321
top3_score    0.780398
top5_score    0.782479
dtype: float64
valid_loss:  2.865929720984565
Learning rate after epoch 3: [1.9501143259182537e-05]
Epoch 4/20: 100%|██████████| 4500/4500 [00:57<00:00, 77.74batch/s]

Model validation...
100%|██████████| 1125/1125 [00:10<00:00, 103.92batch/s]
top1_score    0.787683
top3_score    0.801357
top5_score    0.803202
dtype: float64
valid_loss:  2.8448728811475967
Learning rate after epoch 4: [1.9501143259182537e-05]
Epoch 5/20: 100%|██████████| 4500/4500 [00:57<00:00, 77.88batch/s]

Model validation...
100%|██████████| 1125/1125 [00:11<00:00, 102.12batch/s]
top1_score    0.789615
top3_score    0.802727
top5_score    0.804416
dtype: float64
valid_loss:  2.842098528544108
Learning rate after epoch 5: [1.9501143259182537e-05]
Epoch 6/20: 100%|██████████| 4500/4500 [00:58<00:00, 77.38batch/s]

Model validation...
100%|██████████| 1125/1125 [00:10<00:00, 102.82batch/s]
[I 2025-06-25 01:10:47,712] Trial 23 pruned.
/home/c611/.local/lib/python3.11/site-packages/optuna/trial/_trial.py:682: RuntimeWarning: Inconsistent parameter values for distribution with name "sched_factor"! This might be a configuration mistake. Optuna allows to call the same distribution with the same name more than once in a trial. When the parameter values are inconsistent optuna only uses the values of the first call and ignores all following. Using these values: {'step': None, 'low': 0.0, 'high': 0.4, 'log': False}
  warnings.warn(
top1_score    0.790151
top3_score    0.801977
top5_score    0.803803
dtype: float64
valid_loss:  2.841486382166545
Learning rate after epoch 6: [1.9501143259182537e-05]
{'lr': 5.205787518314441e-05, 'weight_decay': 0.0015384417805352627, 'num_epochs': 20, 'batch_size': 32, 'only_train_classifier': False, 'sched_patience': 8, 'sched_factor': 0.008134317947394415, 'sched_min_lr': 1e-05, 'train_with_CombinedClassifier': True, 'probabilities': [0.1090538467645541, 0.44878863693895554, 0.3348809637313344], 'config_CombinedClassifier': {'linear_out_1': 128, 'linear_out_2': 256, 'dropout': 0.008134317947394415}}


-------------- Start New Training: ---------------


Epoch 1/20: 100%|██████████| 4500/4500 [00:57<00:00, 77.90batch/s]

Model validation...
100%|██████████| 1125/1125 [00:10<00:00, 103.24batch/s]
top1_score    0.735262
top3_score    0.746345
top5_score    0.748813
dtype: float64
valid_loss:  2.897616013844808
Learning rate after epoch 1: [5.205787518314441e-05]
Epoch 2/20: 100%|██████████| 4500/4500 [00:57<00:00, 77.72batch/s]

Model validation...
100%|██████████| 1125/1125 [00:11<00:00, 101.67batch/s]
top1_score    0.758037
top3_score    0.770021
top5_score    0.771429
dtype: float64
valid_loss:  2.873517612881131
Learning rate after epoch 2: [5.205787518314441e-05]
Epoch 3/20: 100%|██████████| 4500/4500 [00:57<00:00, 78.09batch/s]

Model validation...
100%|██████████| 1125/1125 [00:10<00:00, 102.84batch/s]
top1_score    0.772926
top3_score    0.784191
top5_score    0.785680
dtype: float64
valid_loss:  2.8591784261067708
Learning rate after epoch 3: [5.205787518314441e-05]
Epoch 4/20: 100%|██████████| 4500/4500 [00:57<00:00, 78.48batch/s]

Model validation...
100%|██████████| 1125/1125 [00:10<00:00, 102.78batch/s]
top1_score    0.791011
top3_score    0.814509
top5_score    0.816699
dtype: float64
valid_loss:  2.8410628473493786
Learning rate after epoch 4: [5.205787518314441e-05]
Epoch 5/20: 100%|██████████| 4500/4500 [00:58<00:00, 77.44batch/s]

Model validation...
100%|██████████| 1125/1125 [00:10<00:00, 102.68batch/s]
top1_score    0.793366
top3_score    0.811518
top5_score    0.815355
dtype: float64
valid_loss:  2.8387712885538736
Learning rate after epoch 5: [5.205787518314441e-05]
Epoch 6/20: 100%|██████████| 4500/4500 [00:58<00:00, 77.53batch/s]

Model validation...
100%|██████████| 1125/1125 [00:10<00:00, 102.74batch/s]
[I 2025-06-25 01:17:40,658] Trial 24 pruned.
/home/c611/.local/lib/python3.11/site-packages/optuna/trial/_trial.py:682: RuntimeWarning: Inconsistent parameter values for distribution with name "sched_factor"! This might be a configuration mistake. Optuna allows to call the same distribution with the same name more than once in a trial. When the parameter values are inconsistent optuna only uses the values of the first call and ignores all following. Using these values: {'step': None, 'low': 0.0, 'high': 0.4, 'log': False}
  warnings.warn(
top1_score    0.791484
top3_score    0.810208
top5_score    0.813123
dtype: float64
valid_loss:  2.8400591261121964
Learning rate after epoch 6: [5.205787518314441e-05]
{'lr': 0.00011387631445404469, 'weight_decay': 0.000429475155492944, 'num_epochs': 20, 'batch_size': 32, 'only_train_classifier': False, 'sched_patience': 9, 'sched_factor': 0.05716326368394173, 'sched_min_lr': 1e-05, 'train_with_CombinedClassifier': True, 'probabilities': [0.19681972387241592, 0.588756775040012, 0.4524962955328061], 'config_CombinedClassifier': {'linear_out_1': 128, 'linear_out_2': 256, 'dropout': 0.05716326368394173}}


-------------- Start New Training: ---------------


Epoch 1/20: 100%|██████████| 4500/4500 [00:57<00:00, 78.03batch/s]

Model validation...
100%|██████████| 1125/1125 [00:11<00:00, 100.81batch/s]
top1_score    0.702166
top3_score    0.724573
top5_score    0.729649
dtype: float64
valid_loss:  2.931716285917494
Learning rate after epoch 1: [0.00011387631445404469]
Epoch 2/20: 100%|██████████| 4500/4500 [00:57<00:00, 77.78batch/s]

Model validation...
100%|██████████| 1125/1125 [00:10<00:00, 102.75batch/s]
top1_score    0.747700
top3_score    0.757231
top5_score    0.760241
dtype: float64
valid_loss:  2.8842362060546876
Learning rate after epoch 2: [0.00011387631445404469]
Epoch 3/20: 100%|██████████| 4500/4500 [00:57<00:00, 77.81batch/s]

Model validation...
100%|██████████| 1125/1125 [00:10<00:00, 102.91batch/s]
[I 2025-06-25 01:21:07,367] Trial 25 pruned.
/home/c611/.local/lib/python3.11/site-packages/optuna/trial/_trial.py:682: RuntimeWarning: Inconsistent parameter values for distribution with name "sched_factor"! This might be a configuration mistake. Optuna allows to call the same distribution with the same name more than once in a trial. When the parameter values are inconsistent optuna only uses the values of the first call and ignores all following. Using these values: {'step': None, 'low': 0.0, 'high': 0.4, 'log': False}
  warnings.warn(
top1_score    0.761094
top3_score    0.775586
top5_score    0.777578
dtype: float64
valid_loss:  2.8708838276333277
Learning rate after epoch 3: [0.00011387631445404469]
{'lr': 3.589278992189573e-05, 'weight_decay': 0.0021049576605242848, 'num_epochs': 20, 'batch_size': 32, 'only_train_classifier': False, 'sched_patience': 6, 'sched_factor': 0.14951981079020601, 'sched_min_lr': 1e-05, 'train_with_CombinedClassifier': True, 'probabilities': [0.3193725987886371, 0.2328567851099496, 0.1964878228686645], 'config_CombinedClassifier': {'linear_out_1': 128, 'linear_out_2': 256, 'dropout': 0.14951981079020601}}


-------------- Start New Training: ---------------


Epoch 1/20: 100%|██████████| 4500/4500 [00:57<00:00, 78.54batch/s]

Model validation...
100%|██████████| 1125/1125 [00:10<00:00, 103.16batch/s]
top1_score    0.753596
top3_score    0.773836
top5_score    0.777016
dtype: float64
valid_loss:  2.879916887919108
Learning rate after epoch 1: [3.589278992189573e-05]
Epoch 2/20: 100%|██████████| 4500/4500 [00:57<00:00, 78.04batch/s]

Model validation...
100%|██████████| 1125/1125 [00:10<00:00, 102.74batch/s]
top1_score    0.778296
top3_score    0.800091
top5_score    0.802215
dtype: float64
valid_loss:  2.852505866792467
Learning rate after epoch 2: [3.589278992189573e-05]
Epoch 3/20: 100%|██████████| 4500/4500 [00:57<00:00, 78.04batch/s]

Model validation...
100%|██████████| 1125/1125 [00:10<00:00, 103.54batch/s]
top1_score    0.780468
top3_score    0.800163
top5_score    0.802298
dtype: float64
valid_loss:  2.8496391529507106
Learning rate after epoch 3: [3.589278992189573e-05]
Epoch 4/20: 100%|██████████| 4500/4500 [00:57<00:00, 78.39batch/s]

Model validation...
100%|██████████| 1125/1125 [00:10<00:00, 103.61batch/s]
top1_score    0.780390
top3_score    0.799322
top5_score    0.801683
dtype: float64
valid_loss:  2.849469532224867
Learning rate after epoch 4: [3.589278992189573e-05]
Epoch 5/20: 100%|██████████| 4500/4500 [00:57<00:00, 78.29batch/s]

Model validation...
100%|██████████| 1125/1125 [00:11<00:00, 102.11batch/s]
top1_score    0.790326
top3_score    0.808703
top5_score    0.811816
dtype: float64
valid_loss:  2.8405556729634602
Learning rate after epoch 5: [3.589278992189573e-05]
Epoch 6/20: 100%|██████████| 4500/4500 [00:57<00:00, 78.46batch/s]

Model validation...
100%|██████████| 1125/1125 [00:10<00:00, 102.45batch/s]
[I 2025-06-25 01:27:58,199] Trial 26 pruned.
/home/c611/.local/lib/python3.11/site-packages/optuna/trial/_trial.py:682: RuntimeWarning: Inconsistent parameter values for distribution with name "sched_factor"! This might be a configuration mistake. Optuna allows to call the same distribution with the same name more than once in a trial. When the parameter values are inconsistent optuna only uses the values of the first call and ignores all following. Using these values: {'step': None, 'low': 0.0, 'high': 0.4, 'log': False}
  warnings.warn(
top1_score    0.793708
top3_score    0.812115
top5_score    0.814460
dtype: float64
valid_loss:  2.8366124793158636
Learning rate after epoch 6: [3.589278992189573e-05]
{'lr': 1.8474754395063474e-05, 'weight_decay': 0.0009596406946973163, 'num_epochs': 20, 'batch_size': 16, 'only_train_classifier': False, 'sched_patience': 10, 'sched_factor': 0.03327389785921144, 'sched_min_lr': 1e-05, 'train_with_CombinedClassifier': True, 'probabilities': [0.4820750561103545, 0.43434854780839216, 0.32379663535222786], 'config_CombinedClassifier': {'linear_out_1': 128, 'linear_out_2': 256, 'dropout': 0.03327389785921144}}


-------------- Start New Training: ---------------


Epoch 1/20: 100%|██████████| 9000/9000 [01:33<00:00, 96.00batch/s]

Model validation...
100%|██████████| 2250/2250 [00:14<00:00, 151.19batch/s]
top1_score    0.752207
top3_score    0.765263
top5_score    0.766837
dtype: float64
valid_loss:  2.8824601971308392
Learning rate after epoch 1: [1.8474754395063474e-05]
Epoch 2/20: 100%|██████████| 9000/9000 [01:33<00:00, 96.11batch/s]

Model validation...
100%|██████████| 2250/2250 [00:14<00:00, 151.42batch/s]
top1_score    0.756825
top3_score    0.767318
top5_score    0.768478
dtype: float64
valid_loss:  2.8767878279156154
Learning rate after epoch 2: [1.8474754395063474e-05]
Epoch 3/20: 100%|██████████| 9000/9000 [01:33<00:00, 96.66batch/s]

Model validation...
100%|██████████| 2250/2250 [00:14<00:00, 151.02batch/s]
[I 2025-06-25 01:33:23,655] Trial 27 pruned.
/home/c611/.local/lib/python3.11/site-packages/optuna/trial/_trial.py:682: RuntimeWarning: Inconsistent parameter values for distribution with name "sched_factor"! This might be a configuration mistake. Optuna allows to call the same distribution with the same name more than once in a trial. When the parameter values are inconsistent optuna only uses the values of the first call and ignores all following. Using these values: {'step': None, 'low': 0.0, 'high': 0.4, 'log': False}
  warnings.warn(
top1_score    0.760636
top3_score    0.770573
top5_score    0.772318
dtype: float64
valid_loss:  2.872171817673577
Learning rate after epoch 3: [1.8474754395063474e-05]
{'lr': 6.654298699457408e-05, 'weight_decay': 0.0005912847459294557, 'num_epochs': 20, 'batch_size': 32, 'only_train_classifier': False, 'sched_patience': 7, 'sched_factor': 0.07393554869180227, 'sched_min_lr': 1e-05, 'train_with_CombinedClassifier': True, 'probabilities': [0.19634352746147013, 0.8826681468928563, 0.1141286732808218], 'config_CombinedClassifier': {'linear_out_1': 128, 'linear_out_2': 256, 'dropout': 0.07393554869180227}}


-------------- Start New Training: ---------------


Epoch 1/20: 100%|██████████| 4500/4500 [01:03<00:00, 71.24batch/s]

Model validation...
100%|██████████| 1125/1125 [00:10<00:00, 103.76batch/s]
top1_score    0.738250
top3_score    0.750320
top5_score    0.751931
dtype: float64
valid_loss:  2.8973673559824626
Learning rate after epoch 1: [6.654298699457408e-05]
Epoch 2/20: 100%|██████████| 4500/4500 [01:05<00:00, 69.11batch/s]

Model validation...
100%|██████████| 1125/1125 [00:10<00:00, 104.72batch/s]
top1_score    0.770163
top3_score    0.792670
top5_score    0.794859
dtype: float64
valid_loss:  2.863979852888319
Learning rate after epoch 2: [6.654298699457408e-05]
Epoch 3/20: 100%|██████████| 4500/4500 [01:04<00:00, 70.14batch/s]

Model validation...
100%|██████████| 1125/1125 [00:10<00:00, 104.60batch/s]
top1_score    0.777268
top3_score    0.793321
top5_score    0.795256
dtype: float64
valid_loss:  2.8570032068888347
Learning rate after epoch 3: [6.654298699457408e-05]
Epoch 4/20: 100%|██████████| 4500/4500 [01:03<00:00, 70.36batch/s]

Model validation...
100%|██████████| 1125/1125 [00:10<00:00, 104.71batch/s]
top1_score    0.777757
top3_score    0.792140
top5_score    0.794042
dtype: float64
valid_loss:  2.857008128696018
Learning rate after epoch 4: [6.654298699457408e-05]
Epoch 5/20: 100%|██████████| 4500/4500 [01:05<00:00, 68.44batch/s]

Model validation...
100%|██████████| 1125/1125 [00:10<00:00, 104.49batch/s]
top1_score    0.778161
top3_score    0.792919
top5_score    0.794547
dtype: float64
valid_loss:  2.8554420238071017
Learning rate after epoch 5: [6.654298699457408e-05]
Epoch 6/20: 100%|██████████| 4500/4500 [01:03<00:00, 71.32batch/s]

Model validation...
100%|██████████| 1125/1125 [00:10<00:00, 105.56batch/s]
[I 2025-06-25 01:40:53,810] Trial 28 pruned.
/home/c611/.local/lib/python3.11/site-packages/optuna/trial/_trial.py:682: RuntimeWarning: Inconsistent parameter values for distribution with name "sched_factor"! This might be a configuration mistake. Optuna allows to call the same distribution with the same name more than once in a trial. When the parameter values are inconsistent optuna only uses the values of the first call and ignores all following. Using these values: {'step': None, 'low': 0.0, 'high': 0.4, 'log': False}
  warnings.warn(
top1_score    0.787948
top3_score    0.798846
top5_score    0.800575
dtype: float64
valid_loss:  2.8457107490963405
Learning rate after epoch 6: [6.654298699457408e-05]
{'lr': 1.4842864358022817e-05, 'weight_decay': 0.0002132508432812982, 'num_epochs': 20, 'batch_size': 32, 'only_train_classifier': False, 'sched_patience': 8, 'sched_factor': 0.10952035157924785, 'sched_min_lr': 1e-05, 'train_with_CombinedClassifier': True, 'probabilities': [0.2481476021411757, 0.7461968051548367, 0.20114270157762948], 'config_CombinedClassifier': {'linear_out_1': 256, 'linear_out_2': 256, 'dropout': 0.10952035157924785}}


-------------- Start New Training: ---------------


Epoch 1/20: 100%|██████████| 4500/4500 [01:01<00:00, 73.45batch/s]

Model validation...
100%|██████████| 1125/1125 [00:10<00:00, 104.47batch/s]
top1_score    0.730138
top3_score    0.754650
top5_score    0.758158
dtype: float64
valid_loss:  2.90688429429796
Learning rate after epoch 1: [1.4842864358022817e-05]
Epoch 2/20: 100%|██████████| 4500/4500 [01:00<00:00, 74.20batch/s]

Model validation...
100%|██████████| 1125/1125 [00:10<00:00, 102.83batch/s]
top1_score    0.763009
top3_score    0.785344
top5_score    0.788812
dtype: float64
valid_loss:  2.8716760262383354
Learning rate after epoch 2: [1.4842864358022817e-05]
Epoch 3/20: 100%|██████████| 4500/4500 [01:01<00:00, 73.57batch/s]

Model validation...
100%|██████████| 1125/1125 [00:10<00:00, 103.79batch/s]
top1_score    0.771628
top3_score    0.788217
top5_score    0.790528
dtype: float64
valid_loss:  2.8615734060075546
Learning rate after epoch 3: [1.4842864358022817e-05]
Epoch 4/20: 100%|██████████| 4500/4500 [01:00<00:00, 74.27batch/s]

Model validation...
100%|██████████| 1125/1125 [00:10<00:00, 103.72batch/s]
top1_score    0.783166
top3_score    0.801028
top5_score    0.803930
dtype: float64
valid_loss:  2.8511262980567085
Learning rate after epoch 4: [1.4842864358022817e-05]
Epoch 5/20: 100%|██████████| 4500/4500 [01:00<00:00, 74.51batch/s]

Model validation...
100%|██████████| 1125/1125 [00:10<00:00, 103.69batch/s]
top1_score    0.788691
top3_score    0.804499
top5_score    0.806733
dtype: float64
valid_loss:  2.8442252816094293
Learning rate after epoch 5: [1.4842864358022817e-05]
Epoch 6/20: 100%|██████████| 4500/4500 [01:00<00:00, 74.07batch/s]

Model validation...
100%|██████████| 1125/1125 [00:10<00:00, 103.97batch/s]
top1_score    0.797776
top3_score    0.819895
top5_score    0.823241
dtype: float64
valid_loss:  2.838037236743503
Learning rate after epoch 6: [1.4842864358022817e-05]
Epoch 7/20: 100%|██████████| 4500/4500 [01:00<00:00, 74.62batch/s]

Model validation...
100%|██████████| 1125/1125 [00:10<00:00, 104.75batch/s]
top1_score    0.807042
top3_score    0.826778
top5_score    0.830593
dtype: float64
valid_loss:  2.825617542690701
Learning rate after epoch 7: [1.4842864358022817e-05]
Epoch 8/20: 100%|██████████| 4500/4500 [01:00<00:00, 73.85batch/s]

Model validation...
100%|██████████| 1125/1125 [00:10<00:00, 104.21batch/s]
top1_score    0.816421
top3_score    0.836310
top5_score    0.838800
dtype: float64
valid_loss:  2.8163213051689997
Learning rate after epoch 8: [1.4842864358022817e-05]
Epoch 9/20: 100%|██████████| 4500/4500 [01:01<00:00, 72.74batch/s]

Model validation...
100%|██████████| 1125/1125 [00:10<00:00, 104.19batch/s]
top1_score    0.833233
top3_score    0.854511
top5_score    0.857230
dtype: float64
valid_loss:  2.8007136874728733
Learning rate after epoch 9: [1.4842864358022817e-05]
Epoch 10/20: 100%|██████████| 4500/4500 [00:59<00:00, 75.53batch/s]

Model validation...
100%|██████████| 1125/1125 [00:10<00:00, 103.91batch/s]
top1_score    0.832517
top3_score    0.850364
top5_score    0.852896
dtype: float64
valid_loss:  2.8000508984459773
Learning rate after epoch 10: [1.4842864358022817e-05]
Epoch 11/20: 100%|██████████| 4500/4500 [01:01<00:00, 73.66batch/s]

Model validation...
100%|██████████| 1125/1125 [00:10<00:00, 103.00batch/s]
top1_score    0.837772
top3_score    0.855630
top5_score    0.857813
dtype: float64
valid_loss:  2.7958432581159802
Learning rate after epoch 11: [1.4842864358022817e-05]
Epoch 12/20: 100%|██████████| 4500/4500 [00:59<00:00, 75.14batch/s]

Model validation...
100%|██████████| 1125/1125 [00:10<00:00, 104.30batch/s]
top1_score    0.834828
top3_score    0.853251
top5_score    0.855834
dtype: float64
valid_loss:  2.7978609449598526
Learning rate after epoch 12: [1.4842864358022817e-05]
Epoch 13/20: 100%|██████████| 4500/4500 [01:00<00:00, 74.08batch/s]

Model validation...
100%|██████████| 1125/1125 [00:10<00:00, 103.61batch/s]
top1_score    0.836026
top3_score    0.852989
top5_score    0.855131
dtype: float64
valid_loss:  2.7963282873365616
Learning rate after epoch 13: [1.4842864358022817e-05]
Epoch 14/20: 100%|██████████| 4500/4500 [01:00<00:00, 73.88batch/s]

Model validation...
100%|██████████| 1125/1125 [00:10<00:00, 104.50batch/s]
top1_score    0.837089
top3_score    0.855371
top5_score    0.857653
dtype: float64
valid_loss:  2.7952369598812528
Learning rate after epoch 14: [1.4842864358022817e-05]
Epoch 15/20: 100%|██████████| 4500/4500 [01:00<00:00, 74.58batch/s]

Model validation...
100%|██████████| 1125/1125 [00:10<00:00, 103.82batch/s]
top1_score    0.834119
top3_score    0.852103
top5_score    0.854196
dtype: float64
valid_loss:  2.797630918926663
Learning rate after epoch 15: [1.4842864358022817e-05]
Epoch 16/20: 100%|██████████| 4500/4500 [01:00<00:00, 74.47batch/s]

Model validation...
100%|██████████| 1125/1125 [00:10<00:00, 103.82batch/s]
top1_score    0.832626
top3_score    0.847145
top5_score    0.849031
dtype: float64
valid_loss:  2.7996539902157256
Learning rate after epoch 16: [1.4842864358022817e-05]
Epoch 17/20: 100%|██████████| 4500/4500 [01:01<00:00, 73.65batch/s]

Model validation...
100%|██████████| 1125/1125 [00:10<00:00, 103.06batch/s]
top1_score    0.841247
top3_score    0.856001
top5_score    0.858091
dtype: float64
valid_loss:  2.7913830801645916
Learning rate after epoch 17: [1.4842864358022817e-05]
Epoch 18/20: 100%|██████████| 4500/4500 [01:01<00:00, 73.13batch/s]

Model validation...
100%|██████████| 1125/1125 [00:10<00:00, 103.62batch/s]
top1_score    0.830629
top3_score    0.848691
top5_score    0.850375
dtype: float64
valid_loss:  2.8017242715623643
Learning rate after epoch 18: [1.4842864358022817e-05]
Epoch 19/20: 100%|██████████| 4500/4500 [01:01<00:00, 73.69batch/s]

Model validation...
100%|██████████| 1125/1125 [00:10<00:00, 104.73batch/s]
top1_score    0.837754
top3_score    0.852747
top5_score    0.855226
dtype: float64
valid_loss:  2.794362386279636
Learning rate after epoch 19: [1.4842864358022817e-05]
Epoch 20/20: 100%|██████████| 4500/4500 [00:59<00:00, 75.45batch/s]

Model validation...
100%|██████████| 1125/1125 [00:10<00:00, 103.80batch/s]
[I 2025-06-25 02:04:45,399] Trial 29 finished with value: 0.8415035605430603 and parameters: {'linear_out_1': 256, 'linear_out_2': 256, 'sched_factor': 0.10952035157924785, 'lr': 1.4842864358022817e-05, 'weight_decay': 0.0002132508432812982, 'batch_size': 32, 'sched_patience': 8, 'sched_min_lr': 1e-05, 'a1': 0.2481476021411757, 'a2': 0.7461968051548367, 'a3': 0.20114270157762948}. Best is trial 7 with value: 0.8432953357696533.
/home/c611/.local/lib/python3.11/site-packages/optuna/trial/_trial.py:682: RuntimeWarning: Inconsistent parameter values for distribution with name "sched_factor"! This might be a configuration mistake. Optuna allows to call the same distribution with the same name more than once in a trial. When the parameter values are inconsistent optuna only uses the values of the first call and ignores all following. Using these values: {'step': None, 'low': 0.0, 'high': 0.4, 'log': False}
  warnings.warn(
top1_score    0.841504
top3_score    0.855631
top5_score    0.858056
dtype: float64
valid_loss:  2.7903049897087944
Learning rate after epoch 20: [1.4842864358022817e-05]
Training finished!
{'lr': 0.005874648217697634, 'weight_decay': 0.0001887404251024891, 'num_epochs': 20, 'batch_size': 32, 'only_train_classifier': False, 'sched_patience': 9, 'sched_factor': 0.21355619235522313, 'sched_min_lr': 1e-05, 'train_with_CombinedClassifier': True, 'probabilities': [0.24360866200181608, 0.7357628671526363, 0.206576686009256], 'config_CombinedClassifier': {'linear_out_1': 256, 'linear_out_2': 128, 'dropout': 0.21355619235522313}}


-------------- Start New Training: ---------------


Epoch 1/20: 100%|██████████| 4500/4500 [01:00<00:00, 74.70batch/s]

Model validation...
100%|██████████| 1125/1125 [00:10<00:00, 103.43batch/s]
top1_score    0.027482
top3_score    0.074913
top5_score    0.138448
dtype: float64
valid_loss:  3.5835189819335938
Learning rate after epoch 1: [0.005874648217697634]
Epoch 2/20: 100%|██████████| 4500/4500 [01:01<00:00, 73.05batch/s]

Model validation...
100%|██████████| 1125/1125 [00:10<00:00, 104.84batch/s]
top1_score    0.027624
top3_score    0.076047
top5_score    0.136673
dtype: float64
valid_loss:  3.5835189819335938
Learning rate after epoch 2: [0.005874648217697634]
Epoch 3/20: 100%|██████████| 4500/4500 [00:59<00:00, 75.32batch/s]

Model validation...
100%|██████████| 1125/1125 [00:10<00:00, 104.12batch/s]
[I 2025-06-25 02:08:19,701] Trial 30 pruned.
/home/c611/.local/lib/python3.11/site-packages/optuna/trial/_trial.py:682: RuntimeWarning: Inconsistent parameter values for distribution with name "sched_factor"! This might be a configuration mistake. Optuna allows to call the same distribution with the same name more than once in a trial. When the parameter values are inconsistent optuna only uses the values of the first call and ignores all following. Using these values: {'step': None, 'low': 0.0, 'high': 0.4, 'log': False}
  warnings.warn(
top1_score    0.026726
top3_score    0.082636
top5_score    0.138988
dtype: float64
valid_loss:  3.5835189819335938
Learning rate after epoch 3: [0.005874648217697634]
{'lr': 1.7538342545314068e-05, 'weight_decay': 0.000101712535172645, 'num_epochs': 20, 'batch_size': 32, 'only_train_classifier': False, 'sched_patience': 8, 'sched_factor': 0.1255266403801695, 'sched_min_lr': 1e-05, 'train_with_CombinedClassifier': True, 'probabilities': [0.1744449624016834, 0.9131680868600749, 0.29474599278005603], 'config_CombinedClassifier': {'linear_out_1': 256, 'linear_out_2': 256, 'dropout': 0.1255266403801695}}


-------------- Start New Training: ---------------


Epoch 1/20: 100%|██████████| 4500/4500 [01:02<00:00, 71.93batch/s]

Model validation...
100%|██████████| 1125/1125 [00:10<00:00, 102.61batch/s]
top1_score    0.755522
top3_score    0.778678
top5_score    0.781526
dtype: float64
valid_loss:  2.8793097320132786
Learning rate after epoch 1: [1.7538342545314068e-05]
Epoch 2/20: 100%|██████████| 4500/4500 [01:03<00:00, 70.81batch/s]

Model validation...
100%|██████████| 1125/1125 [00:10<00:00, 103.20batch/s]
top1_score    0.764787
top3_score    0.783146
top5_score    0.785841
dtype: float64
valid_loss:  2.868308465745714
Learning rate after epoch 2: [1.7538342545314068e-05]
Epoch 3/20: 100%|██████████| 4500/4500 [01:01<00:00, 73.27batch/s]

Model validation...
100%|██████████| 1125/1125 [00:10<00:00, 104.15batch/s]
top1_score    0.781494
top3_score    0.801365
top5_score    0.803395
dtype: float64
valid_loss:  2.850390040927463
Learning rate after epoch 3: [1.7538342545314068e-05]
Epoch 4/20: 100%|██████████| 4500/4500 [01:01<00:00, 72.64batch/s]

Model validation...
100%|██████████| 1125/1125 [00:10<00:00, 104.39batch/s]
top1_score    0.796405
top3_score    0.819835
top5_score    0.822258
dtype: float64
valid_loss:  2.836746480729845
Learning rate after epoch 4: [1.7538342545314068e-05]
Epoch 5/20: 100%|██████████| 4500/4500 [01:01<00:00, 72.74batch/s]

Model validation...
100%|██████████| 1125/1125 [00:10<00:00, 104.08batch/s]
top1_score    0.800525
top3_score    0.818166
top5_score    0.820333
dtype: float64
valid_loss:  2.8311713042789037
Learning rate after epoch 5: [1.7538342545314068e-05]
Epoch 6/20: 100%|██████████| 4500/4500 [01:02<00:00, 71.91batch/s]

Model validation...
100%|██████████| 1125/1125 [00:10<00:00, 103.03batch/s]
top1_score    0.798557
top3_score    0.818080
top5_score    0.820140
dtype: float64
valid_loss:  2.8325517213609483
Learning rate after epoch 6: [1.7538342545314068e-05]
Epoch 7/20: 100%|██████████| 4500/4500 [01:01<00:00, 72.73batch/s]

Model validation...
100%|██████████| 1125/1125 [00:10<00:00, 104.42batch/s]
top1_score    0.814950
top3_score    0.835731
top5_score    0.838700
dtype: float64
valid_loss:  2.81778164185418
Learning rate after epoch 7: [1.7538342545314068e-05]
Epoch 8/20: 100%|██████████| 4500/4500 [01:02<00:00, 71.85batch/s]

Model validation...
100%|██████████| 1125/1125 [00:10<00:00, 103.93batch/s]
top1_score    0.820137
top3_score    0.840198
top5_score    0.842066
dtype: float64
valid_loss:  2.8122641688452825
Learning rate after epoch 8: [1.7538342545314068e-05]
Epoch 9/20: 100%|██████████| 4500/4500 [01:02<00:00, 72.05batch/s]

Model validation...
100%|██████████| 1125/1125 [00:10<00:00, 104.03batch/s]
top1_score    0.826554
top3_score    0.846701
top5_score    0.848858
dtype: float64
valid_loss:  2.8062588873969183
Learning rate after epoch 9: [1.7538342545314068e-05]
Epoch 10/20: 100%|██████████| 4500/4500 [01:02<00:00, 71.94batch/s]

Model validation...
100%|██████████| 1125/1125 [00:10<00:00, 103.65batch/s]
top1_score    0.829050
top3_score    0.849380
top5_score    0.851307
dtype: float64
valid_loss:  2.8036960203382706
Learning rate after epoch 10: [1.7538342545314068e-05]
Epoch 11/20: 100%|██████████| 4500/4500 [01:01<00:00, 73.05batch/s]

Model validation...
100%|██████████| 1125/1125 [00:10<00:00, 104.41batch/s]
top1_score    0.830057
top3_score    0.847465
top5_score    0.849182
dtype: float64
valid_loss:  2.8028641397688125
Learning rate after epoch 11: [1.7538342545314068e-05]
Epoch 12/20: 100%|██████████| 4500/4500 [01:03<00:00, 71.28batch/s]

Model validation...
100%|██████████| 1125/1125 [00:10<00:00, 104.18batch/s]
top1_score    0.829639
top3_score    0.847480
top5_score    0.849710
dtype: float64
valid_loss:  2.803081161287096
Learning rate after epoch 12: [1.7538342545314068e-05]
Epoch 13/20: 100%|██████████| 4500/4500 [01:02<00:00, 72.18batch/s]

Model validation...
100%|██████████| 1125/1125 [00:10<00:00, 103.93batch/s]
top1_score    0.828514
top3_score    0.845083
top5_score    0.847225
dtype: float64
valid_loss:  2.803620251337687
Learning rate after epoch 13: [1.7538342545314068e-05]
Epoch 14/20: 100%|██████████| 4500/4500 [01:00<00:00, 74.94batch/s]

Model validation...
100%|██████████| 1125/1125 [00:10<00:00, 104.60batch/s]
top1_score    0.831826
top3_score    0.850850
top5_score    0.852219
dtype: float64
valid_loss:  2.7998160220252144
Learning rate after epoch 14: [1.7538342545314068e-05]
Epoch 15/20: 100%|██████████| 4500/4500 [01:02<00:00, 72.52batch/s]

Model validation...
100%|██████████| 1125/1125 [00:10<00:00, 102.39batch/s]
top1_score    0.833724
top3_score    0.847959
top5_score    0.849890
dtype: float64
valid_loss:  2.7984598882463243
Learning rate after epoch 15: [1.7538342545314068e-05]
Epoch 16/20: 100%|██████████| 4500/4500 [01:02<00:00, 72.05batch/s]

Model validation...
100%|██████████| 1125/1125 [00:10<00:00, 103.61batch/s]
top1_score    0.832754
top3_score    0.848420
top5_score    0.850239
dtype: float64
valid_loss:  2.7992743617163764
Learning rate after epoch 16: [1.7538342545314068e-05]
Epoch 17/20: 100%|██████████| 4500/4500 [01:02<00:00, 72.03batch/s]

Model validation...
100%|██████████| 1125/1125 [00:10<00:00, 104.09batch/s]
top1_score    0.832525
top3_score    0.849945
top5_score    0.851647
dtype: float64
valid_loss:  2.8000234938727484
Learning rate after epoch 17: [1.7538342545314068e-05]
Epoch 18/20: 100%|██████████| 4500/4500 [01:01<00:00, 73.48batch/s]

Model validation...
100%|██████████| 1125/1125 [00:10<00:00, 103.47batch/s]
top1_score    0.836251
top3_score    0.853191
top5_score    0.855011
dtype: float64
valid_loss:  2.79631714714898
Learning rate after epoch 18: [1.7538342545314068e-05]
Epoch 19/20: 100%|██████████| 4500/4500 [01:02<00:00, 72.20batch/s]

Model validation...
100%|██████████| 1125/1125 [00:10<00:00, 105.06batch/s]
top1_score    0.834066
top3_score    0.850056
top5_score    0.851823
dtype: float64
valid_loss:  2.798330634435018
Learning rate after epoch 19: [1.7538342545314068e-05]
Epoch 20/20: 100%|██████████| 4500/4500 [01:02<00:00, 71.58batch/s]

Model validation...
100%|██████████| 1125/1125 [00:10<00:00, 103.71batch/s]
[I 2025-06-25 02:32:41,141] Trial 31 finished with value: 0.836250901222229 and parameters: {'linear_out_1': 256, 'linear_out_2': 256, 'sched_factor': 0.1255266403801695, 'lr': 1.7538342545314068e-05, 'weight_decay': 0.000101712535172645, 'batch_size': 32, 'sched_patience': 8, 'sched_min_lr': 1e-05, 'a1': 0.1744449624016834, 'a2': 0.9131680868600749, 'a3': 0.29474599278005603}. Best is trial 7 with value: 0.8432953357696533.
/home/c611/.local/lib/python3.11/site-packages/optuna/trial/_trial.py:682: RuntimeWarning: Inconsistent parameter values for distribution with name "sched_factor"! This might be a configuration mistake. Optuna allows to call the same distribution with the same name more than once in a trial. When the parameter values are inconsistent optuna only uses the values of the first call and ignores all following. Using these values: {'step': None, 'low': 0.0, 'high': 0.4, 'log': False}
  warnings.warn(
top1_score    0.835508
top3_score    0.851687
top5_score    0.853305
dtype: float64
valid_loss:  2.796996315214369
Learning rate after epoch 20: [1.7538342545314068e-05]
Training finished!
{'lr': 1.478517413218617e-05, 'weight_decay': 0.00010089087819977251, 'num_epochs': 20, 'batch_size': 32, 'only_train_classifier': False, 'sched_patience': 8, 'sched_factor': 0.12149987111511931, 'sched_min_lr': 1e-05, 'train_with_CombinedClassifier': True, 'probabilities': [0.34573351059822954, 0.9983109336408298, 0.3201704620123584], 'config_CombinedClassifier': {'linear_out_1': 256, 'linear_out_2': 256, 'dropout': 0.12149987111511931}}


-------------- Start New Training: ---------------


Epoch 1/20: 100%|██████████| 4500/4500 [00:59<00:00, 75.65batch/s]

Model validation...
100%|██████████| 1125/1125 [00:10<00:00, 103.93batch/s]
top1_score    0.719144
top3_score    0.735049
top5_score    0.739230
dtype: float64
valid_loss:  2.9164000487857393
Learning rate after epoch 1: [1.478517413218617e-05]
Epoch 2/20: 100%|██████████| 4500/4500 [01:00<00:00, 74.91batch/s]

Model validation...
100%|██████████| 1125/1125 [00:10<00:00, 103.64batch/s]
top1_score    0.758170
top3_score    0.770761
top5_score    0.772316
dtype: float64
valid_loss:  2.8756572166019017
Learning rate after epoch 2: [1.478517413218617e-05]
Epoch 3/20: 100%|██████████| 4500/4500 [01:00<00:00, 74.29batch/s]

Model validation...
100%|██████████| 1125/1125 [00:10<00:00, 103.68batch/s]
[I 2025-06-25 02:36:14,115] Trial 32 pruned.
/home/c611/.local/lib/python3.11/site-packages/optuna/trial/_trial.py:682: RuntimeWarning: Inconsistent parameter values for distribution with name "sched_factor"! This might be a configuration mistake. Optuna allows to call the same distribution with the same name more than once in a trial. When the parameter values are inconsistent optuna only uses the values of the first call and ignores all following. Using these values: {'step': None, 'low': 0.0, 'high': 0.4, 'log': False}
  warnings.warn(
top1_score    0.762269
top3_score    0.772337
top5_score    0.773581
dtype: float64
valid_loss:  2.8711825885772706
Learning rate after epoch 3: [1.478517413218617e-05]
{'lr': 1.0374467123587445e-05, 'weight_decay': 0.00013544410294083335, 'num_epochs': 20, 'batch_size': 16, 'only_train_classifier': False, 'sched_patience': 6, 'sched_factor': 0.1678482238384672, 'sched_min_lr': 1e-05, 'train_with_CombinedClassifier': True, 'probabilities': [0.23937914850771813, 0.9288150350197603, 0.5849927610263581], 'config_CombinedClassifier': {'linear_out_1': 256, 'linear_out_2': 256, 'dropout': 0.1678482238384672}}


-------------- Start New Training: ---------------


Epoch 1/20: 100%|██████████| 9000/9000 [01:41<00:00, 88.91batch/s]

Model validation...
100%|██████████| 2250/2250 [00:14<00:00, 150.61batch/s]
top1_score    0.629882
top3_score    0.643721
top5_score    0.646846
dtype: float64
valid_loss:  3.00457322745853
Learning rate after epoch 1: [1.0374467123587445e-05]
Epoch 2/20: 100%|██████████| 9000/9000 [01:41<00:00, 88.89batch/s]

Model validation...
100%|██████████| 2250/2250 [00:15<00:00, 149.77batch/s]
top1_score    0.762858
top3_score    0.782207
top5_score    0.784171
dtype: float64
valid_loss:  2.873642799695333
Learning rate after epoch 2: [1.0374467123587445e-05]
Epoch 3/20: 100%|██████████| 9000/9000 [01:40<00:00, 89.71batch/s]

Model validation...
100%|██████████| 2250/2250 [00:14<00:00, 151.58batch/s]
top1_score    0.775830
top3_score    0.791177
top5_score    0.794441
dtype: float64
valid_loss:  2.858429580582513
Learning rate after epoch 3: [1.0374467123587445e-05]
Epoch 4/20: 100%|██████████| 9000/9000 [01:39<00:00, 90.30batch/s]

Model validation...
100%|██████████| 2250/2250 [00:15<00:00, 149.71batch/s]
top1_score    0.794740
top3_score    0.812522
top5_score    0.814247
dtype: float64
valid_loss:  2.838757106569078
Learning rate after epoch 4: [1.0374467123587445e-05]
Epoch 5/20: 100%|██████████| 9000/9000 [01:42<00:00, 88.21batch/s]

Model validation...
100%|██████████| 2250/2250 [00:14<00:00, 150.57batch/s]
top1_score    0.797719
top3_score    0.812545
top5_score    0.815166
dtype: float64
valid_loss:  2.835279125637478
Learning rate after epoch 5: [1.0374467123587445e-05]
Epoch 6/20: 100%|██████████| 9000/9000 [01:42<00:00, 88.23batch/s]

Model validation...
100%|██████████| 2250/2250 [00:14<00:00, 152.33batch/s]
top1_score    0.799962
top3_score    0.814343
top5_score    0.816538
dtype: float64
valid_loss:  2.832177277988858
Learning rate after epoch 6: [1.0374467123587445e-05]
Epoch 7/20: 100%|██████████| 9000/9000 [01:40<00:00, 89.90batch/s]

Model validation...
100%|██████████| 2250/2250 [00:14<00:00, 150.84batch/s]
top1_score    0.801793
top3_score    0.814541
top5_score    0.816271
dtype: float64
valid_loss:  2.830134938240051
Learning rate after epoch 7: [1.0374467123587445e-05]
Epoch 8/20: 100%|██████████| 9000/9000 [01:40<00:00, 89.98batch/s]

Model validation...
100%|██████████| 2250/2250 [00:14<00:00, 150.50batch/s]
top1_score    0.801004
top3_score    0.814232
top5_score    0.815747
dtype: float64
valid_loss:  2.830834282239278
Learning rate after epoch 8: [1.0374467123587445e-05]
Epoch 9/20: 100%|██████████| 9000/9000 [01:39<00:00, 90.54batch/s]

Model validation...
100%|██████████| 2250/2250 [00:14<00:00, 151.23batch/s]
[I 2025-06-25 02:53:34,945] Trial 33 pruned.
/home/c611/.local/lib/python3.11/site-packages/optuna/trial/_trial.py:682: RuntimeWarning: Inconsistent parameter values for distribution with name "sched_factor"! This might be a configuration mistake. Optuna allows to call the same distribution with the same name more than once in a trial. When the parameter values are inconsistent optuna only uses the values of the first call and ignores all following. Using these values: {'step': None, 'low': 0.0, 'high': 0.4, 'log': False}
  warnings.warn(
top1_score    0.803143
top3_score    0.814635
top5_score    0.816467
dtype: float64
valid_loss:  2.828313215573629
Learning rate after epoch 9: [1.0374467123587445e-05]
{'lr': 1.777210432360355e-05, 'weight_decay': 0.00022227278915999804, 'num_epochs': 20, 'batch_size': 32, 'only_train_classifier': False, 'sched_patience': 7, 'sched_factor': 0.1229278859465823, 'sched_min_lr': 1e-05, 'train_with_CombinedClassifier': True, 'probabilities': [0.1709795279483417, 0.7814785454388185, 0.1979647921406802], 'config_CombinedClassifier': {'linear_out_1': 256, 'linear_out_2': 256, 'dropout': 0.1229278859465823}}


-------------- Start New Training: ---------------


Epoch 1/20: 100%|██████████| 4500/4500 [01:03<00:00, 70.40batch/s]

Model validation...
100%|██████████| 1125/1125 [00:10<00:00, 104.43batch/s]
top1_score    0.755706
top3_score    0.779267
top5_score    0.781232
dtype: float64
valid_loss:  2.8807111462487116
Learning rate after epoch 1: [1.777210432360355e-05]
Epoch 2/20: 100%|██████████| 4500/4500 [01:03<00:00, 70.59batch/s]

Model validation...
100%|██████████| 1125/1125 [00:10<00:00, 104.31batch/s]
top1_score    0.757562
top3_score    0.779958
top5_score    0.781880
dtype: float64
valid_loss:  2.8763463967641196
Learning rate after epoch 2: [1.777210432360355e-05]
Epoch 3/20: 100%|██████████| 4500/4500 [01:02<00:00, 71.52batch/s]

Model validation...
100%|██████████| 1125/1125 [00:10<00:00, 104.01batch/s]
top1_score    0.784269
top3_score    0.804811
top5_score    0.807022
dtype: float64
valid_loss:  2.849528275383843
Learning rate after epoch 3: [1.777210432360355e-05]
Epoch 4/20: 100%|██████████| 4500/4500 [01:01<00:00, 72.74batch/s]

Model validation...
100%|██████████| 1125/1125 [00:10<00:00, 104.37batch/s]
top1_score    0.787028
top3_score    0.804945
top5_score    0.807152
dtype: float64
valid_loss:  2.84573762872484
Learning rate after epoch 4: [1.777210432360355e-05]
Epoch 5/20: 100%|██████████| 4500/4500 [01:03<00:00, 70.65batch/s]

Model validation...
100%|██████████| 1125/1125 [00:10<00:00, 104.84batch/s]
top1_score    0.785032
top3_score    0.802606
top5_score    0.804493
dtype: float64
valid_loss:  2.847315974977281
Learning rate after epoch 5: [1.777210432360355e-05]
Epoch 6/20: 100%|██████████| 4500/4500 [01:03<00:00, 70.75batch/s]

Model validation...
100%|██████████| 1125/1125 [00:10<00:00, 104.63batch/s]
[I 2025-06-25 03:00:59,714] Trial 34 pruned.
/home/c611/.local/lib/python3.11/site-packages/optuna/trial/_trial.py:682: RuntimeWarning: Inconsistent parameter values for distribution with name "sched_factor"! This might be a configuration mistake. Optuna allows to call the same distribution with the same name more than once in a trial. When the parameter values are inconsistent optuna only uses the values of the first call and ignores all following. Using these values: {'step': None, 'low': 0.0, 'high': 0.4, 'log': False}
  warnings.warn(
top1_score    0.790482
top3_score    0.806913
top5_score    0.809083
dtype: float64
valid_loss:  2.842156709247165
Learning rate after epoch 6: [1.777210432360355e-05]
{'lr': 2.6357865330378125e-05, 'weight_decay': 0.0003623783164806083, 'num_epochs': 20, 'batch_size': 32, 'only_train_classifier': False, 'sched_patience': 5, 'sched_factor': 0.18602253515913544, 'sched_min_lr': 1e-05, 'train_with_CombinedClassifier': True, 'probabilities': [0.4459754767221127, 0.841363558924677, 0.4613749676870826], 'config_CombinedClassifier': {'linear_out_1': 256, 'linear_out_2': 256, 'dropout': 0.18602253515913544}}


-------------- Start New Training: ---------------


Epoch 1/20: 100%|██████████| 4500/4500 [00:57<00:00, 78.04batch/s]

Model validation...
100%|██████████| 1125/1125 [00:10<00:00, 103.39batch/s]
top1_score    0.720443
top3_score    0.751694
top5_score    0.756531
dtype: float64
valid_loss:  2.9160840176476372
Learning rate after epoch 1: [2.6357865330378125e-05]
Epoch 2/20: 100%|██████████| 4500/4500 [00:58<00:00, 76.49batch/s]

Model validation...
100%|██████████| 1125/1125 [00:10<00:00, 103.55batch/s]
top1_score    0.767216
top3_score    0.789050
top5_score    0.791562
dtype: float64
valid_loss:  2.8652086859809027
Learning rate after epoch 2: [2.6357865330378125e-05]
Epoch 3/20: 100%|██████████| 4500/4500 [00:57<00:00, 78.38batch/s]

Model validation...
100%|██████████| 1125/1125 [00:10<00:00, 103.50batch/s]
top1_score    0.791983
top3_score    0.812508
top5_score    0.815713
dtype: float64
valid_loss:  2.8400911286671957
Learning rate after epoch 3: [2.6357865330378125e-05]
Epoch 4/20: 100%|██████████| 4500/4500 [00:57<00:00, 78.65batch/s]

Model validation...
100%|██████████| 1125/1125 [00:10<00:00, 103.69batch/s]
top1_score    0.794672
top3_score    0.814934
top5_score    0.816936
dtype: float64
valid_loss:  2.836623294406467
Learning rate after epoch 4: [2.6357865330378125e-05]
Epoch 5/20: 100%|██████████| 4500/4500 [00:57<00:00, 77.71batch/s]

Model validation...
100%|██████████| 1125/1125 [00:10<00:00, 102.97batch/s]
top1_score    0.801554
top3_score    0.818720
top5_score    0.820969
dtype: float64
valid_loss:  2.830000988430447
Learning rate after epoch 5: [2.6357865330378125e-05]
Epoch 6/20: 100%|██████████| 4500/4500 [00:57<00:00, 77.81batch/s]

Model validation...
100%|██████████| 1125/1125 [00:10<00:00, 103.45batch/s]
top1_score    0.817511
top3_score    0.839511
top5_score    0.843503
dtype: float64
valid_loss:  2.8167808053758407
Learning rate after epoch 6: [2.6357865330378125e-05]
Epoch 7/20: 100%|██████████| 4500/4500 [00:58<00:00, 77.44batch/s]

Model validation...
100%|██████████| 1125/1125 [00:10<00:00, 103.20batch/s]
top1_score    0.827343
top3_score    0.846954
top5_score    0.850220
dtype: float64
valid_loss:  2.805521133210924
Learning rate after epoch 7: [2.6357865330378125e-05]
Epoch 8/20: 100%|██████████| 4500/4500 [00:57<00:00, 77.68batch/s]

Model validation...
100%|██████████| 1125/1125 [00:10<00:00, 103.84batch/s]
top1_score    0.837313
top3_score    0.857263
top5_score    0.860363
dtype: float64
valid_loss:  2.795294626024034
Learning rate after epoch 8: [2.6357865330378125e-05]
Epoch 9/20: 100%|██████████| 4500/4500 [00:57<00:00, 78.31batch/s]

Model validation...
100%|██████████| 1125/1125 [00:10<00:00, 103.27batch/s]
top1_score    0.837326
top3_score    0.855892
top5_score    0.859043
dtype: float64
valid_loss:  2.794917722913954
Learning rate after epoch 9: [2.6357865330378125e-05]
Epoch 10/20: 100%|██████████| 4500/4500 [00:57<00:00, 78.31batch/s]

Model validation...
100%|██████████| 1125/1125 [00:10<00:00, 102.62batch/s]
top1_score    0.836705
top3_score    0.856124
top5_score    0.858305
dtype: float64
valid_loss:  2.795044685575697
Learning rate after epoch 10: [2.6357865330378125e-05]
Epoch 11/20: 100%|██████████| 4500/4500 [00:57<00:00, 78.47batch/s]

Model validation...
100%|██████████| 1125/1125 [00:11<00:00, 101.37batch/s]
top1_score    0.833452
top3_score    0.851896
top5_score    0.854603
dtype: float64
valid_loss:  2.7982407150268553
Learning rate after epoch 11: [2.6357865330378125e-05]
Epoch 12/20: 100%|██████████| 4500/4500 [00:57<00:00, 77.85batch/s]

Model validation...
100%|██████████| 1125/1125 [00:10<00:00, 103.17batch/s]
top1_score    0.835738
top3_score    0.853609
top5_score    0.856232
dtype: float64
valid_loss:  2.7959489148457846
Learning rate after epoch 12: [2.6357865330378125e-05]
Epoch 13/20: 100%|██████████| 4500/4500 [00:57<00:00, 78.15batch/s]

Model validation...
100%|██████████| 1125/1125 [00:11<00:00, 102.06batch/s]
top1_score    0.834442
top3_score    0.850984
top5_score    0.854610
dtype: float64
valid_loss:  2.7973654062483044
Learning rate after epoch 13: [2.6357865330378125e-05]
Epoch 14/20: 100%|██████████| 4500/4500 [00:57<00:00, 78.22batch/s]

Model validation...
100%|██████████| 1125/1125 [00:10<00:00, 103.50batch/s]
top1_score    0.840501
top3_score    0.857900
top5_score    0.859803
dtype: float64
valid_loss:  2.7907864979637993
Learning rate after epoch 14: [2.6357865330378125e-05]
Epoch 15/20: 100%|██████████| 4500/4500 [00:57<00:00, 77.95batch/s]

Model validation...
100%|██████████| 1125/1125 [00:10<00:00, 103.71batch/s]
top1_score    0.839509
top3_score    0.856103
top5_score    0.858410
dtype: float64
valid_loss:  2.791816704008314
Learning rate after epoch 15: [2.6357865330378125e-05]
Epoch 16/20: 100%|██████████| 4500/4500 [00:57<00:00, 77.78batch/s]

Model validation...
100%|██████████| 1125/1125 [00:10<00:00, 103.54batch/s]
top1_score    0.841228
top3_score    0.859631
top5_score    0.861307
dtype: float64
valid_loss:  2.789582442389594
Learning rate after epoch 16: [2.6357865330378125e-05]
Epoch 17/20: 100%|██████████| 4500/4500 [00:57<00:00, 78.15batch/s]

Model validation...
100%|██████████| 1125/1125 [00:10<00:00, 103.49batch/s]
top1_score    0.838053
top3_score    0.854378
top5_score    0.856144
dtype: float64
valid_loss:  2.793640929116143
Learning rate after epoch 17: [2.6357865330378125e-05]
Epoch 18/20: 100%|██████████| 4500/4500 [00:57<00:00, 78.57batch/s]

Model validation...
100%|██████████| 1125/1125 [00:10<00:00, 104.49batch/s]
top1_score    0.839529
top3_score    0.856786
top5_score    0.858984
dtype: float64
valid_loss:  2.7921092823876275
Learning rate after epoch 18: [2.6357865330378125e-05]
Epoch 19/20: 100%|██████████| 4500/4500 [00:57<00:00, 78.08batch/s]

Model validation...
100%|██████████| 1125/1125 [00:10<00:00, 103.37batch/s]
top1_score    0.835316
top3_score    0.850548
top5_score    0.852440
dtype: float64
valid_loss:  2.7962544807857936
Learning rate after epoch 19: [2.6357865330378125e-05]
Epoch 20/20: 100%|██████████| 4500/4500 [00:57<00:00, 78.21batch/s]

Model validation...
100%|██████████| 1125/1125 [00:10<00:00, 103.32batch/s]
[I 2025-06-25 03:23:52,071] Trial 35 finished with value: 0.8412275314331055 and parameters: {'linear_out_1': 256, 'linear_out_2': 256, 'sched_factor': 0.18602253515913544, 'lr': 2.6357865330378125e-05, 'weight_decay': 0.0003623783164806083, 'batch_size': 32, 'sched_patience': 5, 'sched_min_lr': 1e-05, 'a1': 0.4459754767221127, 'a2': 0.841363558924677, 'a3': 0.4613749676870826}. Best is trial 7 with value: 0.8432953357696533.
/home/c611/.local/lib/python3.11/site-packages/optuna/trial/_trial.py:682: RuntimeWarning: Inconsistent parameter values for distribution with name "sched_factor"! This might be a configuration mistake. Optuna allows to call the same distribution with the same name more than once in a trial. When the parameter values are inconsistent optuna only uses the values of the first call and ignores all following. Using these values: {'step': None, 'low': 0.0, 'high': 0.4, 'log': False}
  warnings.warn(
top1_score    0.840148
top3_score    0.855568
top5_score    0.857567
dtype: float64
valid_loss:  2.7909877808888752
Learning rate after epoch 20: [2.6357865330378125e-05]
Training finished!
{'lr': 2.9065149556634354e-05, 'weight_decay': 0.000379833204467572, 'num_epochs': 20, 'batch_size': 16, 'only_train_classifier': False, 'sched_patience': 5, 'sched_factor': 0.21880935837518412, 'sched_min_lr': 1e-05, 'train_with_CombinedClassifier': True, 'probabilities': [0.5686853517163848, 0.8054287814195823, 0.5290815380200952], 'config_CombinedClassifier': {'linear_out_1': 256, 'linear_out_2': 128, 'dropout': 0.21880935837518412}}


-------------- Start New Training: ---------------


Epoch 1/20: 100%|██████████| 9000/9000 [01:36<00:00, 93.03batch/s]

Model validation...
100%|██████████| 2250/2250 [00:14<00:00, 152.86batch/s]
top1_score    0.751936
top3_score    0.774939
top5_score    0.777688
dtype: float64
valid_loss:  2.881636157989502
Learning rate after epoch 1: [2.9065149556634354e-05]
Epoch 2/20: 100%|██████████| 9000/9000 [01:36<00:00, 93.17batch/s]

Model validation...
100%|██████████| 2250/2250 [00:15<00:00, 149.61batch/s]
top1_score    0.760487
top3_score    0.779193
top5_score    0.781914
dtype: float64
valid_loss:  2.8716322010887994
Learning rate after epoch 2: [2.9065149556634354e-05]
Epoch 3/20: 100%|██████████| 9000/9000 [01:36<00:00, 93.26batch/s]

Model validation...
100%|██████████| 2250/2250 [00:15<00:00, 147.95batch/s]
[I 2025-06-25 03:29:27,197] Trial 36 pruned.
/home/c611/.local/lib/python3.11/site-packages/optuna/trial/_trial.py:682: RuntimeWarning: Inconsistent parameter values for distribution with name "sched_factor"! This might be a configuration mistake. Optuna allows to call the same distribution with the same name more than once in a trial. When the parameter values are inconsistent optuna only uses the values of the first call and ignores all following. Using these values: {'step': None, 'low': 0.0, 'high': 0.4, 'log': False}
  warnings.warn(
top1_score    0.760486
top3_score    0.778232
top5_score    0.781169
dtype: float64
valid_loss:  2.8710840515560574
Learning rate after epoch 3: [2.9065149556634354e-05]
{'lr': 2.5029072629102545e-05, 'weight_decay': 0.0002484863771894737, 'num_epochs': 20, 'batch_size': 32, 'only_train_classifier': False, 'sched_patience': 4, 'sched_factor': 0.18321168895163065, 'sched_min_lr': 1e-05, 'train_with_CombinedClassifier': True, 'probabilities': [0.4360936709321135, 0.7424832390482186, 0.47487891425747064], 'config_CombinedClassifier': {'linear_out_1': 256, 'linear_out_2': 256, 'dropout': 0.18321168895163065}}


-------------- Start New Training: ---------------


Epoch 1/20: 100%|██████████| 4500/4500 [00:57<00:00, 78.45batch/s]

Model validation...
100%|██████████| 1125/1125 [00:10<00:00, 103.49batch/s]
top1_score    0.751061
top3_score    0.769233
top5_score    0.772238
dtype: float64
valid_loss:  2.886804487652249
Learning rate after epoch 1: [2.5029072629102545e-05]
Epoch 2/20: 100%|██████████| 4500/4500 [00:57<00:00, 78.26batch/s]

Model validation...
100%|██████████| 1125/1125 [00:10<00:00, 104.12batch/s]
top1_score    0.777226
top3_score    0.792712
top5_score    0.795253
dtype: float64
valid_loss:  2.858661327573988
Learning rate after epoch 2: [2.5029072629102545e-05]
Epoch 3/20: 100%|██████████| 4500/4500 [00:57<00:00, 78.53batch/s]

Model validation...
100%|██████████| 1125/1125 [00:10<00:00, 103.42batch/s]
top1_score    0.783431
top3_score    0.797695
top5_score    0.799770
dtype: float64
valid_loss:  2.850590752919515
Learning rate after epoch 3: [2.5029072629102545e-05]
Epoch 4/20: 100%|██████████| 4500/4500 [00:57<00:00, 78.58batch/s]

Model validation...
100%|██████████| 1125/1125 [00:10<00:00, 102.30batch/s]
top1_score    0.785624
top3_score    0.798735
top5_score    0.800702
dtype: float64
valid_loss:  2.848050252702501
Learning rate after epoch 4: [2.5029072629102545e-05]
Epoch 5/20: 100%|██████████| 4500/4500 [00:57<00:00, 77.62batch/s]

Model validation...
100%|██████████| 1125/1125 [00:10<00:00, 102.85batch/s]
top1_score    0.788143
top3_score    0.800379
top5_score    0.801946
dtype: float64
valid_loss:  2.8458308673434787
Learning rate after epoch 5: [2.5029072629102545e-05]
Epoch 6/20: 100%|██████████| 4500/4500 [00:57<00:00, 77.61batch/s]

Model validation...
100%|██████████| 1125/1125 [00:10<00:00, 102.63batch/s]
[I 2025-06-25 03:36:18,429] Trial 37 pruned.
/home/c611/.local/lib/python3.11/site-packages/optuna/trial/_trial.py:682: RuntimeWarning: Inconsistent parameter values for distribution with name "sched_factor"! This might be a configuration mistake. Optuna allows to call the same distribution with the same name more than once in a trial. When the parameter values are inconsistent optuna only uses the values of the first call and ignores all following. Using these values: {'step': None, 'low': 0.0, 'high': 0.4, 'log': False}
  warnings.warn(
top1_score    0.789617
top3_score    0.801175
top5_score    0.802575
dtype: float64
valid_loss:  2.843925523122152
Learning rate after epoch 6: [2.5029072629102545e-05]
{'lr': 0.002483729802829794, 'weight_decay': 0.00035500032627910097, 'num_epochs': 20, 'batch_size': 32, 'only_train_classifier': False, 'sched_patience': 5, 'sched_factor': 0.24757941942293132, 'sched_min_lr': 1e-06, 'train_with_CombinedClassifier': True, 'probabilities': [0.4695923600599855, 0.8446702839117671, 0.3809617772185559], 'config_CombinedClassifier': {'linear_out_1': 256, 'linear_out_2': 128, 'dropout': 0.24757941942293132}}


-------------- Start New Training: ---------------


Epoch 1/20: 100%|██████████| 4500/4500 [00:57<00:00, 78.45batch/s]

Model validation...
100%|██████████| 1125/1125 [00:10<00:00, 103.89batch/s]
top1_score    0.030137
top3_score    0.078226
top5_score    0.137537
dtype: float64
valid_loss:  3.5835189819335938
Learning rate after epoch 1: [0.002483729802829794]
Epoch 2/20: 100%|██████████| 4500/4500 [00:57<00:00, 77.73batch/s]

Model validation...
100%|██████████| 1125/1125 [00:10<00:00, 104.02batch/s]
top1_score    0.029578
top3_score    0.078633
top5_score    0.139302
dtype: float64
valid_loss:  3.5835189819335938
Learning rate after epoch 2: [0.002483729802829794]
Epoch 3/20: 100%|██████████| 4500/4500 [00:57<00:00, 78.49batch/s]

Model validation...
100%|██████████| 1125/1125 [00:10<00:00, 102.55batch/s]
[I 2025-06-25 03:39:43,932] Trial 38 pruned.
/home/c611/.local/lib/python3.11/site-packages/optuna/trial/_trial.py:682: RuntimeWarning: Inconsistent parameter values for distribution with name "sched_factor"! This might be a configuration mistake. Optuna allows to call the same distribution with the same name more than once in a trial. When the parameter values are inconsistent optuna only uses the values of the first call and ignores all following. Using these values: {'step': None, 'low': 0.0, 'high': 0.4, 'log': False}
  warnings.warn(
top1_score    0.029889
top3_score    0.077821
top5_score    0.137454
dtype: float64
valid_loss:  3.5835189819335938
Learning rate after epoch 3: [0.002483729802829794]
{'lr': 5.130600625148669e-05, 'weight_decay': 0.00018851782918876653, 'num_epochs': 20, 'batch_size': 16, 'only_train_classifier': False, 'sched_patience': 5, 'sched_factor': 0.24930505247930962, 'sched_min_lr': 1e-05, 'train_with_CombinedClassifier': True, 'probabilities': [0.3591844723797527, 0.6995196810915295, 0.7009256124298469], 'config_CombinedClassifier': {'linear_out_1': 256, 'linear_out_2': 256, 'dropout': 0.24930505247930962}}


-------------- Start New Training: ---------------


Epoch 1/20: 100%|██████████| 9000/9000 [01:35<00:00, 94.45batch/s]

Model validation...
100%|██████████| 2250/2250 [00:14<00:00, 150.94batch/s]
top1_score    0.723199
top3_score    0.748217
top5_score    0.750386
dtype: float64
valid_loss:  2.9091475955115422
Learning rate after epoch 1: [5.130600625148669e-05]
Epoch 2/20: 100%|██████████| 9000/9000 [01:35<00:00, 94.42batch/s]

Model validation...
100%|██████████| 2250/2250 [00:14<00:00, 151.22batch/s]
top1_score    0.735403
top3_score    0.752592
top5_score    0.755426
dtype: float64
valid_loss:  2.89434133582645
Learning rate after epoch 2: [5.130600625148669e-05]
Epoch 3/20: 100%|██████████| 9000/9000 [01:34<00:00, 95.28batch/s]

Model validation...
100%|██████████| 2250/2250 [00:14<00:00, 150.37batch/s]
[I 2025-06-25 03:45:14,053] Trial 39 pruned.
/home/c611/.local/lib/python3.11/site-packages/optuna/trial/_trial.py:682: RuntimeWarning: Inconsistent parameter values for distribution with name "sched_factor"! This might be a configuration mistake. Optuna allows to call the same distribution with the same name more than once in a trial. When the parameter values are inconsistent optuna only uses the values of the first call and ignores all following. Using these values: {'step': None, 'low': 0.0, 'high': 0.4, 'log': False}
  warnings.warn(
top1_score    0.752542
top3_score    0.771487
top5_score    0.773935
dtype: float64
valid_loss:  2.876268856048584
Learning rate after epoch 3: [5.130600625148669e-05]
{'lr': 2.6434164713235572e-05, 'weight_decay': 0.0041614200514018575, 'num_epochs': 20, 'batch_size': 32, 'only_train_classifier': False, 'sched_patience': 6, 'sched_factor': 0.20048954151975562, 'sched_min_lr': 1e-06, 'train_with_CombinedClassifier': True, 'probabilities': [0.5576343188275663, 0.7704263840246816, 0.41628451981810977], 'config_CombinedClassifier': {'linear_out_1': 256, 'linear_out_2': 256, 'dropout': 0.20048954151975562}}


-------------- Start New Training: ---------------


Epoch 1/20: 100%|██████████| 4500/4500 [00:57<00:00, 77.59batch/s]

Model validation...
100%|██████████| 1125/1125 [00:10<00:00, 103.72batch/s]
top1_score    0.754609
top3_score    0.772379
top5_score    0.774840
dtype: float64
valid_loss:  2.8826180364820693
Learning rate after epoch 1: [2.6434164713235572e-05]
Epoch 2/20: 100%|██████████| 4500/4500 [00:57<00:00, 78.36batch/s]

Model validation...
100%|██████████| 1125/1125 [00:10<00:00, 103.49batch/s]
top1_score    0.770549
top3_score    0.791109
top5_score    0.794152
dtype: float64
valid_loss:  2.8656926837497285
Learning rate after epoch 2: [2.6434164713235572e-05]
Epoch 3/20: 100%|██████████| 4500/4500 [00:57<00:00, 78.33batch/s]

Model validation...
100%|██████████| 1125/1125 [00:10<00:00, 103.53batch/s]
top1_score    0.794572
top3_score    0.809788
top5_score    0.811727
dtype: float64
valid_loss:  2.839889711168077
Learning rate after epoch 3: [2.6434164713235572e-05]
Epoch 4/20: 100%|██████████| 4500/4500 [00:57<00:00, 78.08batch/s]

Model validation...
100%|██████████| 1125/1125 [00:10<00:00, 103.60batch/s]
top1_score    0.800791
top3_score    0.814608
top5_score    0.816577
dtype: float64
valid_loss:  2.833233685599433
Learning rate after epoch 4: [2.6434164713235572e-05]
Epoch 5/20: 100%|██████████| 4500/4500 [00:57<00:00, 77.68batch/s]

Model validation...
100%|██████████| 1125/1125 [00:10<00:00, 103.73batch/s]
top1_score    0.800336
top3_score    0.813295
top5_score    0.814997
dtype: float64
valid_loss:  2.833311466217041
Learning rate after epoch 5: [2.6434164713235572e-05]
Epoch 6/20: 100%|██████████| 4500/4500 [00:57<00:00, 78.45batch/s]

Model validation...
100%|██████████| 1125/1125 [00:10<00:00, 103.69batch/s]
top1_score    0.806491
top3_score    0.817406
top5_score    0.818869
dtype: float64
valid_loss:  2.8267647813161214
Learning rate after epoch 6: [2.6434164713235572e-05]
Epoch 7/20: 100%|██████████| 4500/4500 [00:57<00:00, 78.66batch/s]

Model validation...
100%|██████████| 1125/1125 [00:10<00:00, 103.60batch/s]
top1_score    0.797666
top3_score    0.811038
top5_score    0.812897
dtype: float64
valid_loss:  2.835705697801378
Learning rate after epoch 7: [2.6434164713235572e-05]
Epoch 8/20: 100%|██████████| 4500/4500 [00:57<00:00, 78.51batch/s]

Model validation...
100%|██████████| 1125/1125 [00:10<00:00, 102.71batch/s]
top1_score    0.809816
top3_score    0.822843
top5_score    0.824435
dtype: float64
valid_loss:  2.8236241573757597
Learning rate after epoch 8: [2.6434164713235572e-05]
Epoch 9/20: 100%|██████████| 4500/4500 [00:57<00:00, 78.10batch/s]

Model validation...
100%|██████████| 1125/1125 [00:10<00:00, 102.29batch/s]
[I 2025-06-25 03:55:30,396] Trial 40 pruned.
/home/c611/.local/lib/python3.11/site-packages/optuna/trial/_trial.py:682: RuntimeWarning: Inconsistent parameter values for distribution with name "sched_factor"! This might be a configuration mistake. Optuna allows to call the same distribution with the same name more than once in a trial. When the parameter values are inconsistent optuna only uses the values of the first call and ignores all following. Using these values: {'step': None, 'low': 0.0, 'high': 0.4, 'log': False}
  warnings.warn(
top1_score    0.810831
top3_score    0.821061
top5_score    0.822188
dtype: float64
valid_loss:  2.822448007159763
Learning rate after epoch 9: [2.6434164713235572e-05]
{'lr': 1.626110411329874e-05, 'weight_decay': 0.00013624047118037028, 'num_epochs': 20, 'batch_size': 32, 'only_train_classifier': False, 'sched_patience': 4, 'sched_factor': 0.14152344437119535, 'sched_min_lr': 1e-05, 'train_with_CombinedClassifier': True, 'probabilities': [0.26431655331159387, 0.9332353582988314, 0.17577408265325567], 'config_CombinedClassifier': {'linear_out_1': 256, 'linear_out_2': 256, 'dropout': 0.14152344437119535}}


-------------- Start New Training: ---------------


Epoch 1/20: 100%|██████████| 4500/4500 [01:01<00:00, 72.70batch/s]

Model validation...
100%|██████████| 1125/1125 [00:10<00:00, 104.89batch/s]
top1_score    0.670650
top3_score    0.694107
top5_score    0.699314
dtype: float64
valid_loss:  2.960481598112318
Learning rate after epoch 1: [1.626110411329874e-05]
Epoch 2/20: 100%|██████████| 4500/4500 [01:03<00:00, 70.35batch/s]

Model validation...
100%|██████████| 1125/1125 [00:10<00:00, 105.08batch/s]
top1_score    0.749344
top3_score    0.771841
top5_score    0.774232
dtype: float64
valid_loss:  2.8826467435624865
Learning rate after epoch 2: [1.626110411329874e-05]
Epoch 3/20: 100%|██████████| 4500/4500 [01:03<00:00, 70.55batch/s]

Model validation...
100%|██████████| 1125/1125 [00:10<00:00, 103.81batch/s]
top1_score    0.778596
top3_score    0.799329
top5_score    0.801689
dtype: float64
valid_loss:  2.853335023880005
Learning rate after epoch 3: [1.626110411329874e-05]
Epoch 4/20: 100%|██████████| 4500/4500 [01:03<00:00, 70.88batch/s]

Model validation...
100%|██████████| 1125/1125 [00:10<00:00, 104.47batch/s]
top1_score    0.785643
top3_score    0.805722
top5_score    0.807725
dtype: float64
valid_loss:  2.8451375041537816
Learning rate after epoch 4: [1.626110411329874e-05]
Epoch 5/20: 100%|██████████| 4500/4500 [01:03<00:00, 70.64batch/s]

Model validation...
100%|██████████| 1125/1125 [00:10<00:00, 104.89batch/s]
top1_score    0.789005
top3_score    0.806913
top5_score    0.809103
dtype: float64
valid_loss:  2.8415894046359593
Learning rate after epoch 5: [1.626110411329874e-05]
Epoch 6/20: 100%|██████████| 4500/4500 [01:02<00:00, 71.64batch/s]

Model validation...
100%|██████████| 1125/1125 [00:10<00:00, 104.89batch/s]
[I 2025-06-25 04:02:54,954] Trial 41 pruned.
/home/c611/.local/lib/python3.11/site-packages/optuna/trial/_trial.py:682: RuntimeWarning: Inconsistent parameter values for distribution with name "sched_factor"! This might be a configuration mistake. Optuna allows to call the same distribution with the same name more than once in a trial. When the parameter values are inconsistent optuna only uses the values of the first call and ignores all following. Using these values: {'step': None, 'low': 0.0, 'high': 0.4, 'log': False}
  warnings.warn(
top1_score    0.789574
top3_score    0.807397
top5_score    0.809421
dtype: float64
valid_loss:  2.840413566377428
Learning rate after epoch 6: [1.626110411329874e-05]
{'lr': 2.0840853834059672e-05, 'weight_decay': 0.0002955148128632401, 'num_epochs': 20, 'batch_size': 32, 'only_train_classifier': False, 'sched_patience': 9, 'sched_factor': 0.1008531281271576, 'sched_min_lr': 1e-05, 'train_with_CombinedClassifier': True, 'probabilities': [0.6132027130704099, 0.9046167456360251, 0.28722377292935103], 'config_CombinedClassifier': {'linear_out_1': 256, 'linear_out_2': 256, 'dropout': 0.1008531281271576}}


-------------- Start New Training: ---------------


Epoch 1/20: 100%|██████████| 4500/4500 [00:57<00:00, 77.91batch/s]

Model validation...
100%|██████████| 1125/1125 [00:10<00:00, 102.56batch/s]
top1_score    0.754218
top3_score    0.776541
top5_score    0.780363
dtype: float64
valid_loss:  2.883296759499444
Learning rate after epoch 1: [2.0840853834059672e-05]
Epoch 2/20: 100%|██████████| 4500/4500 [00:57<00:00, 77.97batch/s]

Model validation...
100%|██████████| 1125/1125 [00:10<00:00, 103.71batch/s]
top1_score    0.769491
top3_score    0.784996
top5_score    0.786458
dtype: float64
valid_loss:  2.8650457973480226
Learning rate after epoch 2: [2.0840853834059672e-05]
Epoch 3/20: 100%|██████████| 4500/4500 [00:57<00:00, 78.76batch/s]

Model validation...
100%|██████████| 1125/1125 [00:11<00:00, 101.04batch/s]
top1_score    0.787999
top3_score    0.804042
top5_score    0.805908
dtype: float64
valid_loss:  2.8467455217573376
Learning rate after epoch 3: [2.0840853834059672e-05]
Epoch 4/20: 100%|██████████| 4500/4500 [00:56<00:00, 78.99batch/s]

Model validation...
100%|██████████| 1125/1125 [00:10<00:00, 104.17batch/s]
top1_score    0.790287
top3_score    0.804488
top5_score    0.806945
dtype: float64
valid_loss:  2.84380894724528
Learning rate after epoch 4: [2.0840853834059672e-05]
Epoch 5/20: 100%|██████████| 4500/4500 [00:57<00:00, 78.49batch/s]

Model validation...
100%|██████████| 1125/1125 [00:10<00:00, 104.05batch/s]
top1_score    0.793039
top3_score    0.807339
top5_score    0.808709
dtype: float64
valid_loss:  2.840092056910197
Learning rate after epoch 5: [2.0840853834059672e-05]
Epoch 6/20: 100%|██████████| 4500/4500 [00:58<00:00, 77.24batch/s]

Model validation...
100%|██████████| 1125/1125 [00:10<00:00, 103.59batch/s]
top1_score    0.810461
top3_score    0.827604
top5_score    0.830093
dtype: float64
valid_loss:  2.825313100814819
Learning rate after epoch 6: [2.0840853834059672e-05]
Epoch 7/20: 100%|██████████| 4500/4500 [00:56<00:00, 79.18batch/s]

Model validation...
100%|██████████| 1125/1125 [00:10<00:00, 103.61batch/s]
top1_score    0.815955
top3_score    0.831146
top5_score    0.832800
dtype: float64
valid_loss:  2.8188692027197946
Learning rate after epoch 7: [2.0840853834059672e-05]
Epoch 8/20: 100%|██████████| 4500/4500 [00:56<00:00, 79.16batch/s]

Model validation...
100%|██████████| 1125/1125 [00:10<00:00, 103.17batch/s]
top1_score    0.817732
top3_score    0.832164
top5_score    0.834157
dtype: float64
valid_loss:  2.8170450723436145
Learning rate after epoch 8: [2.0840853834059672e-05]
Epoch 9/20: 100%|██████████| 4500/4500 [00:57<00:00, 78.76batch/s]

Model validation...
100%|██████████| 1125/1125 [00:10<00:00, 103.77batch/s]
top1_score    0.816865
top3_score    0.831168
top5_score    0.833312
dtype: float64
valid_loss:  2.8171370974646672
Learning rate after epoch 9: [2.0840853834059672e-05]
Epoch 10/20: 100%|██████████| 4500/4500 [00:57<00:00, 77.87batch/s]

Model validation...
100%|██████████| 1125/1125 [00:10<00:00, 103.61batch/s]
top1_score    0.821151
top3_score    0.834656
top5_score    0.836417
dtype: float64
valid_loss:  2.8133720703125
Learning rate after epoch 10: [2.0840853834059672e-05]
Epoch 11/20: 100%|██████████| 4500/4500 [00:57<00:00, 77.98batch/s]

Model validation...
100%|██████████| 1125/1125 [00:10<00:00, 103.19batch/s]
top1_score    0.819240
top3_score    0.831273
top5_score    0.832974
dtype: float64
valid_loss:  2.814778682284885
Learning rate after epoch 11: [2.0840853834059672e-05]
Epoch 12/20: 100%|██████████| 4500/4500 [00:57<00:00, 77.99batch/s]

Model validation...
100%|██████████| 1125/1125 [00:10<00:00, 103.38batch/s]
top1_score    0.820245
top3_score    0.831387
top5_score    0.833272
dtype: float64
valid_loss:  2.813879614935981
Learning rate after epoch 12: [2.0840853834059672e-05]
Epoch 13/20: 100%|██████████| 4500/4500 [00:58<00:00, 77.22batch/s]

Model validation...
100%|██████████| 1125/1125 [00:10<00:00, 103.89batch/s]
top1_score    0.818471
top3_score    0.831125
top5_score    0.832853
dtype: float64
valid_loss:  2.8159437207116023
Learning rate after epoch 13: [2.0840853834059672e-05]
Epoch 14/20: 100%|██████████| 4500/4500 [00:57<00:00, 78.60batch/s]

Model validation...
100%|██████████| 1125/1125 [00:10<00:00, 103.67batch/s]
top1_score    0.822635
top3_score    0.834316
top5_score    0.836210
dtype: float64
valid_loss:  2.8112530973222523
Learning rate after epoch 14: [2.0840853834059672e-05]
Epoch 15/20: 100%|██████████| 4500/4500 [00:57<00:00, 78.28batch/s]

Model validation...
100%|██████████| 1125/1125 [00:10<00:00, 103.23batch/s]
top1_score    0.823908
top3_score    0.835963
top5_score    0.837406
dtype: float64
valid_loss:  2.8099718570709227
Learning rate after epoch 15: [2.0840853834059672e-05]
Epoch 16/20: 100%|██████████| 4500/4500 [00:56<00:00, 79.35batch/s]

Model validation...
100%|██████████| 1125/1125 [00:10<00:00, 103.76batch/s]
top1_score    0.820258
top3_score    0.831786
top5_score    0.833392
dtype: float64
valid_loss:  2.8127362304263643
Learning rate after epoch 16: [2.0840853834059672e-05]
Epoch 17/20: 100%|██████████| 4500/4500 [00:57<00:00, 78.79batch/s]

Model validation...
100%|██████████| 1125/1125 [00:10<00:00, 103.48batch/s]
top1_score    0.822780
top3_score    0.832639
top5_score    0.834123
dtype: float64
valid_loss:  2.810783659193251
Learning rate after epoch 17: [2.0840853834059672e-05]
Epoch 18/20: 100%|██████████| 4500/4500 [00:57<00:00, 78.32batch/s]

Model validation...
100%|██████████| 1125/1125 [00:10<00:00, 103.50batch/s]
top1_score    0.822079
top3_score    0.831544
top5_score    0.832590
dtype: float64
valid_loss:  2.8112459843953452
Learning rate after epoch 18: [2.0840853834059672e-05]
Epoch 19/20: 100%|██████████| 4500/4500 [00:57<00:00, 78.66batch/s]

Model validation...
100%|██████████| 1125/1125 [00:10<00:00, 104.44batch/s]
top1_score    0.822638
top3_score    0.833535
top5_score    0.834609
dtype: float64
valid_loss:  2.8103773345947265
Learning rate after epoch 19: [2.0840853834059672e-05]
Epoch 20/20: 100%|██████████| 4500/4500 [00:56<00:00, 79.15batch/s]

Model validation...
100%|██████████| 1125/1125 [00:10<00:00, 103.86batch/s]
[I 2025-06-25 04:25:40,716] Trial 42 finished with value: 0.8239076137542725 and parameters: {'linear_out_1': 256, 'linear_out_2': 256, 'sched_factor': 0.1008531281271576, 'lr': 2.0840853834059672e-05, 'weight_decay': 0.0002955148128632401, 'batch_size': 32, 'sched_patience': 9, 'sched_min_lr': 1e-05, 'a1': 0.6132027130704099, 'a2': 0.9046167456360251, 'a3': 0.28722377292935103}. Best is trial 7 with value: 0.8432953357696533.
/home/c611/.local/lib/python3.11/site-packages/optuna/trial/_trial.py:682: RuntimeWarning: Inconsistent parameter values for distribution with name "sched_factor"! This might be a configuration mistake. Optuna allows to call the same distribution with the same name more than once in a trial. When the parameter values are inconsistent optuna only uses the values of the first call and ignores all following. Using these values: {'step': None, 'low': 0.0, 'high': 0.4, 'log': False}
  warnings.warn(
top1_score    0.818052
top3_score    0.827295
top5_score    0.828668
dtype: float64
valid_loss:  2.814492477416992
Learning rate after epoch 20: [2.0840853834059672e-05]
Training finished!
{'lr': 1.436504053514967e-05, 'weight_decay': 0.00042474032760911497, 'num_epochs': 20, 'batch_size': 32, 'only_train_classifier': False, 'sched_patience': 4, 'sched_factor': 0.17352834622559538, 'sched_min_lr': 1e-05, 'train_with_CombinedClassifier': True, 'probabilities': [0.10573110587830598, 0.8399009736482208, 0.6165562109062541], 'config_CombinedClassifier': {'linear_out_1': 256, 'linear_out_2': 256, 'dropout': 0.17352834622559538}}


-------------- Start New Training: ---------------


Epoch 1/20: 100%|██████████| 4500/4500 [00:57<00:00, 77.75batch/s]

Model validation...
100%|██████████| 1125/1125 [00:10<00:00, 103.60batch/s]
top1_score    0.741339
top3_score    0.761675
top5_score    0.769520
dtype: float64
valid_loss:  2.8975176381005183
Learning rate after epoch 1: [1.436504053514967e-05]
Epoch 2/20: 100%|██████████| 4500/4500 [00:57<00:00, 78.08batch/s]

Model validation...
100%|██████████| 1125/1125 [00:10<00:00, 103.10batch/s]
top1_score    0.772918
top3_score    0.789540
top5_score    0.791623
dtype: float64
valid_loss:  2.8633141695658364
Learning rate after epoch 2: [1.436504053514967e-05]
Epoch 3/20: 100%|██████████| 4500/4500 [00:57<00:00, 77.76batch/s]

Model validation...
100%|██████████| 1125/1125 [00:10<00:00, 102.48batch/s]
top1_score    0.792168
top3_score    0.809830
top5_score    0.811455
dtype: float64
valid_loss:  2.842777510325114
Learning rate after epoch 3: [1.436504053514967e-05]
Epoch 4/20: 100%|██████████| 4500/4500 [00:58<00:00, 77.10batch/s]

Model validation...
100%|██████████| 1125/1125 [00:10<00:00, 103.09batch/s]
top1_score    0.793087
top3_score    0.810545
top5_score    0.812838
dtype: float64
valid_loss:  2.8408297822740343
Learning rate after epoch 4: [1.436504053514967e-05]
Epoch 5/20: 100%|██████████| 4500/4500 [00:58<00:00, 77.45batch/s]

Model validation...
100%|██████████| 1125/1125 [00:11<00:00, 102.17batch/s]
top1_score    0.811824
top3_score    0.831140
top5_score    0.834341
dtype: float64
valid_loss:  2.823973438474867
Learning rate after epoch 5: [1.436504053514967e-05]
Epoch 6/20: 100%|██████████| 4500/4500 [00:58<00:00, 77.28batch/s]

Model validation...
100%|██████████| 1125/1125 [00:10<00:00, 103.14batch/s]
top1_score    0.836214
top3_score    0.857905
top5_score    0.860206
dtype: float64
valid_loss:  2.80107673157586
Learning rate after epoch 6: [1.436504053514967e-05]
Epoch 7/20: 100%|██████████| 4500/4500 [00:58<00:00, 77.58batch/s]

Model validation...
100%|██████████| 1125/1125 [00:11<00:00, 101.95batch/s]
top1_score    0.839025
top3_score    0.858087
top5_score    0.860545
dtype: float64
valid_loss:  2.797367453257243
Learning rate after epoch 7: [1.436504053514967e-05]
Epoch 8/20: 100%|██████████| 4500/4500 [00:58<00:00, 76.78batch/s]

Model validation...
100%|██████████| 1125/1125 [00:10<00:00, 102.89batch/s]
top1_score    0.832734
top3_score    0.850114
top5_score    0.851943
dtype: float64
valid_loss:  2.8020096507602266
Learning rate after epoch 8: [1.436504053514967e-05]
Epoch 9/20: 100%|██████████| 4500/4500 [00:57<00:00, 77.81batch/s]

Model validation...
100%|██████████| 1125/1125 [00:10<00:00, 103.04batch/s]
top1_score    0.832946
top3_score    0.850071
top5_score    0.851840
dtype: float64
valid_loss:  2.8017454090118408
Learning rate after epoch 9: [1.436504053514967e-05]
Epoch 10/20: 100%|██████████| 4500/4500 [00:58<00:00, 77.50batch/s]

Model validation...
100%|██████████| 1125/1125 [00:10<00:00, 102.47batch/s]
top1_score    0.842877
top3_score    0.858609
top5_score    0.860118
dtype: float64
valid_loss:  2.791478004243639
Learning rate after epoch 10: [1.436504053514967e-05]
Epoch 11/20: 100%|██████████| 4500/4500 [00:58<00:00, 77.39batch/s]

Model validation...
100%|██████████| 1125/1125 [00:11<00:00, 101.66batch/s]
top1_score    0.842865
top3_score    0.857360
top5_score    0.859122
dtype: float64
valid_loss:  2.7919318981170655
Learning rate after epoch 11: [1.436504053514967e-05]
Epoch 12/20: 100%|██████████| 4500/4500 [00:58<00:00, 77.24batch/s]

Model validation...
100%|██████████| 1125/1125 [00:10<00:00, 102.96batch/s]
top1_score    0.846157
top3_score    0.861636
top5_score    0.863597
dtype: float64
valid_loss:  2.788283431371053
Learning rate after epoch 12: [1.436504053514967e-05]
Epoch 13/20: 100%|██████████| 4500/4500 [00:57<00:00, 78.57batch/s]

Model validation...
100%|██████████| 1125/1125 [00:10<00:00, 103.30batch/s]
top1_score    0.850759
top3_score    0.868784
top5_score    0.870912
dtype: float64
valid_loss:  2.7834497248331704
Learning rate after epoch 13: [1.436504053514967e-05]
Epoch 14/20: 100%|██████████| 4500/4500 [00:57<00:00, 77.70batch/s]

Model validation...
100%|██████████| 1125/1125 [00:10<00:00, 102.63batch/s]
top1_score    0.853324
top3_score    0.869387
top5_score    0.871160
dtype: float64
valid_loss:  2.7807260746426055
Learning rate after epoch 14: [1.436504053514967e-05]
Epoch 15/20: 100%|██████████| 4500/4500 [00:58<00:00, 77.13batch/s]

Model validation...
100%|██████████| 1125/1125 [00:10<00:00, 103.07batch/s]
top1_score    0.854030
top3_score    0.868640
top5_score    0.870488
dtype: float64
valid_loss:  2.7798798599243164
Learning rate after epoch 15: [1.436504053514967e-05]
Epoch 16/20: 100%|██████████| 4500/4500 [00:58<00:00, 76.79batch/s]

Model validation...
100%|██████████| 1125/1125 [00:11<00:00, 101.64batch/s]
top1_score    0.859033
top3_score    0.874433
top5_score    0.875955
dtype: float64
valid_loss:  2.774323226928711
Learning rate after epoch 16: [1.436504053514967e-05]
Epoch 17/20: 100%|██████████| 4500/4500 [00:58<00:00, 76.82batch/s]

Model validation...
100%|██████████| 1125/1125 [00:10<00:00, 103.47batch/s]
top1_score    0.856016
top3_score    0.870507
top5_score    0.871939
dtype: float64
valid_loss:  2.7772892642550997
Learning rate after epoch 17: [1.436504053514967e-05]
Epoch 18/20: 100%|██████████| 4500/4500 [00:57<00:00, 77.72batch/s]

Model validation...
100%|██████████| 1125/1125 [00:11<00:00, 101.56batch/s]
top1_score    0.858010
top3_score    0.872132
top5_score    0.873729
dtype: float64
valid_loss:  2.775930103937785
Learning rate after epoch 18: [1.436504053514967e-05]
Epoch 19/20: 100%|██████████| 4500/4500 [00:57<00:00, 77.90batch/s]

Model validation...
100%|██████████| 1125/1125 [00:10<00:00, 102.85batch/s]
top1_score    0.858412
top3_score    0.871981
top5_score    0.873661
dtype: float64
valid_loss:  2.7745582254197863
Learning rate after epoch 19: [1.436504053514967e-05]
Epoch 20/20: 100%|██████████| 4500/4500 [00:57<00:00, 78.21batch/s]

Model validation...
100%|██████████| 1125/1125 [00:11<00:00, 102.07batch/s]
[I 2025-06-25 04:48:41,600] Trial 43 finished with value: 0.8590327501296997 and parameters: {'linear_out_1': 256, 'linear_out_2': 256, 'sched_factor': 0.17352834622559538, 'lr': 1.436504053514967e-05, 'weight_decay': 0.00042474032760911497, 'batch_size': 32, 'sched_patience': 4, 'sched_min_lr': 1e-05, 'a1': 0.10573110587830598, 'a2': 0.8399009736482208, 'a3': 0.6165562109062541}. Best is trial 43 with value: 0.8590327501296997.
/home/c611/.local/lib/python3.11/site-packages/optuna/trial/_trial.py:682: RuntimeWarning: Inconsistent parameter values for distribution with name "sched_factor"! This might be a configuration mistake. Optuna allows to call the same distribution with the same name more than once in a trial. When the parameter values are inconsistent optuna only uses the values of the first call and ignores all following. Using these values: {'step': None, 'low': 0.0, 'high': 0.4, 'log': False}
  warnings.warn(
top1_score    0.855901
top3_score    0.870110
top5_score    0.871817
dtype: float64
valid_loss:  2.777287585788303
Learning rate after epoch 20: [1.436504053514967e-05]
Training finished!
{'lr': 1.2961300513390604e-05, 'weight_decay': 0.0004074535766866384, 'num_epochs': 20, 'batch_size': 32, 'only_train_classifier': False, 'sched_patience': 4, 'sched_factor': 0.17068410660243177, 'sched_min_lr': 1e-05, 'train_with_CombinedClassifier': True, 'probabilities': [0.10664224295061017, 0.8471397563955655, 0.6161801342096733], 'config_CombinedClassifier': {'linear_out_1': 256, 'linear_out_2': 256, 'dropout': 0.17068410660243177}}


-------------- Start New Training: ---------------


Epoch 1/20: 100%|██████████| 4500/4500 [00:59<00:00, 76.26batch/s]

Model validation...
100%|██████████| 1125/1125 [00:10<00:00, 103.53batch/s]
top1_score    0.688843
top3_score    0.721094
top5_score    0.724686
dtype: float64
valid_loss:  2.9521677034166123
Learning rate after epoch 1: [1.2961300513390604e-05]
Epoch 2/20: 100%|██████████| 4500/4500 [00:57<00:00, 77.74batch/s]

Model validation...
100%|██████████| 1125/1125 [00:10<00:00, 103.17batch/s]
top1_score    0.758906
top3_score    0.780570
top5_score    0.782864
dtype: float64
valid_loss:  2.8742889658610027
Learning rate after epoch 2: [1.2961300513390604e-05]
Epoch 3/20: 100%|██████████| 4500/4500 [00:58<00:00, 77.50batch/s]

Model validation...
100%|██████████| 1125/1125 [00:11<00:00, 102.10batch/s]
[I 2025-06-25 04:52:09,662] Trial 44 pruned.
/home/c611/.local/lib/python3.11/site-packages/optuna/trial/_trial.py:682: RuntimeWarning: Inconsistent parameter values for distribution with name "sched_factor"! This might be a configuration mistake. Optuna allows to call the same distribution with the same name more than once in a trial. When the parameter values are inconsistent optuna only uses the values of the first call and ignores all following. Using these values: {'step': None, 'low': 0.0, 'high': 0.4, 'log': False}
  warnings.warn(
top1_score    0.762406
top3_score    0.781106
top5_score    0.782955
dtype: float64
valid_loss:  2.8693766598171657
Learning rate after epoch 3: [1.2961300513390604e-05]
{'lr': 3.3433816609489485e-05, 'weight_decay': 0.0006750660056753163, 'num_epochs': 20, 'batch_size': 64, 'only_train_classifier': False, 'sched_patience': 5, 'sched_factor': 0.3177790808558699, 'sched_min_lr': 1e-06, 'train_with_CombinedClassifier': True, 'probabilities': [0.5043230315786856, 0.6477601922531856, 0.1021084391994099], 'config_CombinedClassifier': {'linear_out_1': 256, 'linear_out_2': 128, 'dropout': 0.3177790808558699}}


-------------- Start New Training: ---------------


Epoch 1/20: 100%|██████████| 2250/2250 [00:43<00:00, 51.48batch/s]

Model validation...
100%|██████████| 563/563 [00:09<00:00, 60.24batch/s]
top1_score    0.750476
top3_score    0.765772
top5_score    0.772320
dtype: float64
valid_loss:  2.886712771325933
Learning rate after epoch 1: [3.3433816609489485e-05]
Epoch 2/20: 100%|██████████| 2250/2250 [00:43<00:00, 51.67batch/s]

Model validation...
100%|██████████| 563/563 [00:09<00:00, 59.88batch/s]
top1_score    0.771983
top3_score    0.787291
top5_score    0.789005
dtype: float64
valid_loss:  2.8635554787955955
Learning rate after epoch 2: [3.3433816609489485e-05]
Epoch 3/20: 100%|██████████| 2250/2250 [00:43<00:00, 52.15batch/s]

Model validation...
100%|██████████| 563/563 [00:09<00:00, 60.32batch/s]
top1_score    0.792620
top3_score    0.809365
top5_score    0.811797
dtype: float64
valid_loss:  2.8435269676770876
Learning rate after epoch 3: [3.3433816609489485e-05]
Epoch 4/20: 100%|██████████| 2250/2250 [00:43<00:00, 52.03batch/s]

Model validation...
100%|██████████| 563/563 [00:09<00:00, 59.48batch/s]
top1_score    0.794907
top3_score    0.809091
top5_score    0.810964
dtype: float64
valid_loss:  2.8402008222644324
Learning rate after epoch 4: [3.3433816609489485e-05]
Epoch 5/20: 100%|██████████| 2250/2250 [00:43<00:00, 51.70batch/s]

Model validation...
100%|██████████| 563/563 [00:09<00:00, 59.71batch/s]
top1_score    0.793006
top3_score    0.806367
top5_score    0.807908
dtype: float64
valid_loss:  2.841811406675918
Learning rate after epoch 5: [3.3433816609489485e-05]
Epoch 6/20: 100%|██████████| 2250/2250 [00:43<00:00, 51.63batch/s]

Model validation...
100%|██████████| 563/563 [00:09<00:00, 59.44batch/s]
top1_score    0.800982
top3_score    0.814844
top5_score    0.816111
dtype: float64
valid_loss:  2.832578052635938
Learning rate after epoch 6: [3.3433816609489485e-05]
Epoch 7/20: 100%|██████████| 2250/2250 [00:43<00:00, 51.74batch/s]

Model validation...
100%|██████████| 563/563 [00:09<00:00, 59.83batch/s]
top1_score    0.804616
top3_score    0.816994
top5_score    0.818715
dtype: float64
valid_loss:  2.828686186941225
Learning rate after epoch 7: [3.3433816609489485e-05]
Epoch 8/20: 100%|██████████| 2250/2250 [00:43<00:00, 52.29batch/s]

Model validation...
100%|██████████| 563/563 [00:09<00:00, 60.19batch/s]
top1_score    0.810796
top3_score    0.822499
top5_score    0.824229
dtype: float64
valid_loss:  2.8229013454850573
Learning rate after epoch 8: [3.3433816609489485e-05]
Epoch 9/20: 100%|██████████| 2250/2250 [00:43<00:00, 51.98batch/s]

Model validation...
100%|██████████| 563/563 [00:09<00:00, 60.16batch/s]
[I 2025-06-25 05:00:05,268] Trial 45 pruned.
/home/c611/.local/lib/python3.11/site-packages/optuna/trial/_trial.py:682: RuntimeWarning: Inconsistent parameter values for distribution with name "sched_factor"! This might be a configuration mistake. Optuna allows to call the same distribution with the same name more than once in a trial. When the parameter values are inconsistent optuna only uses the values of the first call and ignores all following. Using these values: {'step': None, 'low': 0.0, 'high': 0.4, 'log': False}
  warnings.warn(
top1_score    0.811358
top3_score    0.822296
top5_score    0.824119
dtype: float64
valid_loss:  2.82255402593799
Learning rate after epoch 9: [3.3433816609489485e-05]
{'lr': 1.0393321583597656e-05, 'weight_decay': 0.0008759915117442526, 'num_epochs': 20, 'batch_size': 32, 'only_train_classifier': False, 'sched_patience': 5, 'sched_factor': 0.20311778230228472, 'sched_min_lr': 1e-05, 'train_with_CombinedClassifier': True, 'probabilities': [0.44375975686133196, 0.5408391125033881, 0.687378477744252], 'config_CombinedClassifier': {'linear_out_1': 256, 'linear_out_2': 256, 'dropout': 0.20311778230228472}}


-------------- Start New Training: ---------------


Epoch 1/20: 100%|██████████| 4500/4500 [00:58<00:00, 77.28batch/s]

Model validation...
100%|██████████| 1125/1125 [00:10<00:00, 102.36batch/s]
top1_score    0.750110
top3_score    0.779507
top5_score    0.788767
dtype: float64
valid_loss:  2.89298541047838
Learning rate after epoch 1: [1.0393321583597656e-05]
Epoch 2/20: 100%|██████████| 4500/4500 [00:57<00:00, 78.05batch/s]

Model validation...
100%|██████████| 1125/1125 [00:10<00:00, 102.46batch/s]
top1_score    0.795232
top3_score    0.823269
top5_score    0.826596
dtype: float64
valid_loss:  2.8446819987826877
Learning rate after epoch 2: [1.0393321583597656e-05]
Epoch 3/20: 100%|██████████| 4500/4500 [00:57<00:00, 78.12batch/s]

Model validation...
100%|██████████| 1125/1125 [00:10<00:00, 102.34batch/s]
top1_score    0.810000
top3_score    0.837335
top5_score    0.840308
dtype: float64
valid_loss:  2.827547378540039
Learning rate after epoch 3: [1.0393321583597656e-05]
Epoch 4/20: 100%|██████████| 4500/4500 [00:57<00:00, 77.88batch/s]

Model validation...
100%|██████████| 1125/1125 [00:11<00:00, 101.96batch/s]
top1_score    0.810757
top3_score    0.835653
top5_score    0.838231
dtype: float64
valid_loss:  2.8250585106743706
Learning rate after epoch 4: [1.0393321583597656e-05]
Epoch 5/20: 100%|██████████| 4500/4500 [00:57<00:00, 78.21batch/s]

Model validation...
100%|██████████| 1125/1125 [00:10<00:00, 102.55batch/s]
top1_score    0.816150
top3_score    0.838539
top5_score    0.841075
dtype: float64
valid_loss:  2.8194686027103
Learning rate after epoch 5: [1.0393321583597656e-05]
Epoch 6/20: 100%|██████████| 4500/4500 [00:57<00:00, 77.84batch/s]

Model validation...
100%|██████████| 1125/1125 [00:11<00:00, 102.14batch/s]
top1_score    0.817261
top3_score    0.838196
top5_score    0.841041
dtype: float64
valid_loss:  2.8177494309743247
Learning rate after epoch 6: [1.0393321583597656e-05]
Epoch 7/20: 100%|██████████| 4500/4500 [00:57<00:00, 78.17batch/s]

Model validation...
100%|██████████| 1125/1125 [00:10<00:00, 102.28batch/s]
top1_score    0.819987
top3_score    0.842357
top5_score    0.845746
dtype: float64
valid_loss:  2.8143468570709227
Learning rate after epoch 7: [1.0393321583597656e-05]
Epoch 8/20: 100%|██████████| 4500/4500 [00:57<00:00, 78.41batch/s]

Model validation...
100%|██████████| 1125/1125 [00:11<00:00, 100.01batch/s]
top1_score    0.824639
top3_score    0.847190
top5_score    0.849785
dtype: float64
valid_loss:  2.8099484748840333
Learning rate after epoch 8: [1.0393321583597656e-05]
Epoch 9/20: 100%|██████████| 4500/4500 [00:57<00:00, 77.69batch/s]

Model validation...
100%|██████████| 1125/1125 [00:10<00:00, 102.64batch/s]
top1_score    0.834550
top3_score    0.856809
top5_score    0.859183
dtype: float64
valid_loss:  2.8002317335340714
Learning rate after epoch 9: [1.0393321583597656e-05]
Epoch 10/20: 100%|██████████| 4500/4500 [00:57<00:00, 77.74batch/s]

Model validation...
100%|██████████| 1125/1125 [00:10<00:00, 102.59batch/s]
top1_score    0.832715
top3_score    0.853574
top5_score    0.856608
dtype: float64
valid_loss:  2.8019308874342177
Learning rate after epoch 10: [1.0393321583597656e-05]
Epoch 11/20: 100%|██████████| 4500/4500 [00:57<00:00, 77.90batch/s]

Model validation...
100%|██████████| 1125/1125 [00:11<00:00, 101.36batch/s]
top1_score    0.837693
top3_score    0.858969
top5_score    0.861029
dtype: float64
valid_loss:  2.7972882107628716
Learning rate after epoch 11: [1.0393321583597656e-05]
Epoch 12/20: 100%|██████████| 4500/4500 [00:57<00:00, 78.56batch/s]

Model validation...
100%|██████████| 1125/1125 [00:11<00:00, 102.22batch/s]
top1_score    0.838551
top3_score    0.857876
top5_score    0.860355
dtype: float64
valid_loss:  2.7951261054144965
Learning rate after epoch 12: [1.0393321583597656e-05]
Epoch 13/20: 100%|██████████| 4500/4500 [00:57<00:00, 77.76batch/s]

Model validation...
100%|██████████| 1125/1125 [00:10<00:00, 102.52batch/s]
top1_score    0.837822
top3_score    0.855455
top5_score    0.857904
dtype: float64
valid_loss:  2.7958170494503447
Learning rate after epoch 13: [1.0393321583597656e-05]
Epoch 14/20: 100%|██████████| 4500/4500 [00:57<00:00, 78.36batch/s]

Model validation...
100%|██████████| 1125/1125 [00:10<00:00, 102.36batch/s]
top1_score    0.838179
top3_score    0.855856
top5_score    0.857682
dtype: float64
valid_loss:  2.7961965588463675
Learning rate after epoch 14: [1.0393321583597656e-05]
Epoch 15/20: 100%|██████████| 4500/4500 [00:57<00:00, 78.20batch/s]

Model validation...
100%|██████████| 1125/1125 [00:11<00:00, 101.31batch/s]
top1_score    0.839204
top3_score    0.854664
top5_score    0.856731
dtype: float64
valid_loss:  2.795105462392171
Learning rate after epoch 15: [1.0393321583597656e-05]
Epoch 16/20: 100%|██████████| 4500/4500 [00:57<00:00, 78.32batch/s]

Model validation...
100%|██████████| 1125/1125 [00:10<00:00, 102.55batch/s]
top1_score    0.838313
top3_score    0.855209
top5_score    0.857251
dtype: float64
valid_loss:  2.7947890521155463
Learning rate after epoch 16: [1.0393321583597656e-05]
Epoch 17/20: 100%|██████████| 4500/4500 [00:57<00:00, 77.70batch/s]

Model validation...
100%|██████████| 1125/1125 [00:10<00:00, 102.71batch/s]
top1_score    0.842410
top3_score    0.857476
top5_score    0.859538
dtype: float64
valid_loss:  2.7919353470272488
Learning rate after epoch 17: [1.0393321583597656e-05]
Epoch 18/20: 100%|██████████| 4500/4500 [00:57<00:00, 78.84batch/s]

Model validation...
100%|██████████| 1125/1125 [00:11<00:00, 101.69batch/s]
top1_score    0.842550
top3_score    0.858277
top5_score    0.860401
dtype: float64
valid_loss:  2.7912586998409696
Learning rate after epoch 18: [1.0393321583597656e-05]
Epoch 19/20: 100%|██████████| 4500/4500 [00:57<00:00, 77.74batch/s]

Model validation...
100%|██████████| 1125/1125 [00:10<00:00, 102.56batch/s]
top1_score    0.837586
top3_score    0.852847
top5_score    0.854993
dtype: float64
valid_loss:  2.795284576839871
Learning rate after epoch 19: [1.0393321583597656e-05]
Epoch 20/20: 100%|██████████| 4500/4500 [00:57<00:00, 78.39batch/s]

Model validation...
100%|██████████| 1125/1125 [00:10<00:00, 103.32batch/s]
[I 2025-06-25 05:22:59,265] Trial 46 finished with value: 0.8425498008728027 and parameters: {'linear_out_1': 256, 'linear_out_2': 256, 'sched_factor': 0.20311778230228472, 'lr': 1.0393321583597656e-05, 'weight_decay': 0.0008759915117442526, 'batch_size': 32, 'sched_patience': 5, 'sched_min_lr': 1e-05, 'a1': 0.44375975686133196, 'a2': 0.5408391125033881, 'a3': 0.687378477744252}. Best is trial 43 with value: 0.8590327501296997.
/home/c611/.local/lib/python3.11/site-packages/optuna/trial/_trial.py:682: RuntimeWarning: Inconsistent parameter values for distribution with name "sched_factor"! This might be a configuration mistake. Optuna allows to call the same distribution with the same name more than once in a trial. When the parameter values are inconsistent optuna only uses the values of the first call and ignores all following. Using these values: {'step': None, 'low': 0.0, 'high': 0.4, 'log': False}
  warnings.warn(
top1_score    0.841630
top3_score    0.855680
top5_score    0.858044
dtype: float64
valid_loss:  2.791565973069933
Learning rate after epoch 20: [1.0393321583597656e-05]
Training finished!
{'lr': 4.523379981702298e-05, 'weight_decay': 0.0008380397918156008, 'num_epochs': 20, 'batch_size': 64, 'only_train_classifier': False, 'sched_patience': 3, 'sched_factor': 0.20056026397172427, 'sched_min_lr': 1e-06, 'train_with_CombinedClassifier': True, 'probabilities': [0.439792006293575, 0.5567541497055554, 0.7837640716527802], 'config_CombinedClassifier': {'linear_out_1': 256, 'linear_out_2': 128, 'dropout': 0.20056026397172427}}


-------------- Start New Training: ---------------


Epoch 1/20: 100%|██████████| 2250/2250 [00:43<00:00, 51.48batch/s]

Model validation...
100%|██████████| 563/563 [00:09<00:00, 59.11batch/s]
top1_score    0.741445
top3_score    0.753280
top5_score    0.754743
dtype: float64
valid_loss:  2.893380568675419
Learning rate after epoch 1: [4.523379981702298e-05]
Epoch 2/20: 100%|██████████| 2250/2250 [00:43<00:00, 51.57batch/s]

Model validation...
100%|██████████| 563/563 [00:09<00:00, 59.25batch/s]
top1_score    0.779941
top3_score    0.796605
top5_score    0.798446
dtype: float64
valid_loss:  2.854360376327542
Learning rate after epoch 2: [4.523379981702298e-05]
Epoch 3/20: 100%|██████████| 2250/2250 [00:43<00:00, 51.46batch/s]

Model validation...
100%|██████████| 563/563 [00:09<00:00, 58.97batch/s]
top1_score    0.782477
top3_score    0.796910
top5_score    0.798830
dtype: float64
valid_loss:  2.8505806956790902
Learning rate after epoch 3: [4.523379981702298e-05]
Epoch 4/20: 100%|██████████| 2250/2250 [00:43<00:00, 51.16batch/s]

Model validation...
100%|██████████| 563/563 [00:09<00:00, 58.88batch/s]
top1_score    0.785853
top3_score    0.797853
top5_score    0.799978
dtype: float64
valid_loss:  2.846968682578783
Learning rate after epoch 4: [4.523379981702298e-05]
Epoch 5/20: 100%|██████████| 2250/2250 [00:44<00:00, 51.13batch/s]

Model validation...
100%|██████████| 563/563 [00:09<00:00, 58.83batch/s]
top1_score    0.798614
top3_score    0.811388
top5_score    0.813122
dtype: float64
valid_loss:  2.8344104616087242
Learning rate after epoch 5: [4.523379981702298e-05]
Epoch 6/20: 100%|██████████| 2250/2250 [00:43<00:00, 51.45batch/s]

Model validation...
100%|██████████| 563/563 [00:09<00:00, 59.38batch/s]
top1_score    0.796347
top3_score    0.811071
top5_score    0.813113
dtype: float64
valid_loss:  2.8379855956319804
Learning rate after epoch 6: [4.523379981702298e-05]
Epoch 7/20: 100%|██████████| 2250/2250 [00:43<00:00, 51.24batch/s]

Model validation...
100%|██████████| 563/563 [00:09<00:00, 58.74batch/s]
top1_score    0.808406
top3_score    0.819430
top5_score    0.820886
dtype: float64
valid_loss:  2.8248264086606447
Learning rate after epoch 7: [4.523379981702298e-05]
Epoch 8/20: 100%|██████████| 2250/2250 [00:43<00:00, 51.71batch/s]

Model validation...
100%|██████████| 563/563 [00:09<00:00, 59.04batch/s]
top1_score    0.804709
top3_score    0.816862
top5_score    0.818744
dtype: float64
valid_loss:  2.8284577476406607
Learning rate after epoch 8: [4.523379981702298e-05]
Epoch 9/20: 100%|██████████| 2250/2250 [00:43<00:00, 51.51batch/s]

Model validation...
100%|██████████| 563/563 [00:09<00:00, 58.93batch/s]
[I 2025-06-25 05:30:59,490] Trial 47 pruned.
/home/c611/.local/lib/python3.11/site-packages/optuna/trial/_trial.py:682: RuntimeWarning: Inconsistent parameter values for distribution with name "sched_factor"! This might be a configuration mistake. Optuna allows to call the same distribution with the same name more than once in a trial. When the parameter values are inconsistent optuna only uses the values of the first call and ignores all following. Using these values: {'step': None, 'low': 0.0, 'high': 0.4, 'log': False}
  warnings.warn(
top1_score    0.801687
top3_score    0.814227
top5_score    0.816087
dtype: float64
valid_loss:  2.8322208181791155
Learning rate after epoch 9: [4.523379981702298e-05]
{'lr': 2.3739063552401983e-05, 'weight_decay': 0.000334104816865929, 'num_epochs': 20, 'batch_size': 32, 'only_train_classifier': False, 'sched_patience': 4, 'sched_factor': 0.2296563865476144, 'sched_min_lr': 1e-05, 'train_with_CombinedClassifier': True, 'probabilities': [0.37108006305163405, 0.5017304181653608, 0.6917290051750025], 'config_CombinedClassifier': {'linear_out_1': 256, 'linear_out_2': 256, 'dropout': 0.2296563865476144}}


-------------- Start New Training: ---------------


Epoch 1/20: 100%|██████████| 4500/4500 [00:57<00:00, 77.98batch/s]

Model validation...
100%|██████████| 1125/1125 [00:11<00:00, 101.70batch/s]
top1_score    0.777794
top3_score    0.804814
top5_score    0.807625
dtype: float64
valid_loss:  2.8591899087693955
Learning rate after epoch 1: [2.3739063552401983e-05]
Epoch 2/20: 100%|██████████| 4500/4500 [00:57<00:00, 77.71batch/s]

Model validation...
100%|██████████| 1125/1125 [00:10<00:00, 102.59batch/s]
top1_score    0.789916
top3_score    0.810725
top5_score    0.814454
dtype: float64
valid_loss:  2.8446351358625623
Learning rate after epoch 2: [2.3739063552401983e-05]
Epoch 3/20: 100%|██████████| 4500/4500 [00:57<00:00, 77.93batch/s]

Model validation...
100%|██████████| 1125/1125 [00:11<00:00, 101.47batch/s]
top1_score    0.808992
top3_score    0.830978
top5_score    0.834799
dtype: float64
valid_loss:  2.8251206029256184
Learning rate after epoch 3: [2.3739063552401983e-05]
Epoch 4/20: 100%|██████████| 4500/4500 [00:57<00:00, 77.86batch/s]

Model validation...
100%|██████████| 1125/1125 [00:10<00:00, 102.93batch/s]
top1_score    0.811855
top3_score    0.832218
top5_score    0.836077
dtype: float64
valid_loss:  2.8213008189731172
Learning rate after epoch 4: [2.3739063552401983e-05]
Epoch 5/20: 100%|██████████| 4500/4500 [00:57<00:00, 77.75batch/s]

Model validation...
100%|██████████| 1125/1125 [00:11<00:00, 101.61batch/s]
top1_score    0.810593
top3_score    0.829900
top5_score    0.832291
dtype: float64
valid_loss:  2.821886933644613
Learning rate after epoch 5: [2.3739063552401983e-05]
Epoch 6/20: 100%|██████████| 4500/4500 [00:57<00:00, 77.59batch/s]

Model validation...
100%|██████████| 1125/1125 [00:11<00:00, 102.13batch/s]
top1_score    0.812923
top3_score    0.831743
top5_score    0.834286
dtype: float64
valid_loss:  2.8199063453674316
Learning rate after epoch 6: [2.3739063552401983e-05]
Epoch 7/20: 100%|██████████| 4500/4500 [00:57<00:00, 77.95batch/s]

Model validation...
100%|██████████| 1125/1125 [00:10<00:00, 102.31batch/s]
top1_score    0.812666
top3_score    0.831613
top5_score    0.834075
dtype: float64
valid_loss:  2.8191333389282227
Learning rate after epoch 7: [2.3739063552401983e-05]
Epoch 8/20: 100%|██████████| 4500/4500 [00:57<00:00, 77.82batch/s]

Model validation...
100%|██████████| 1125/1125 [00:11<00:00, 101.50batch/s]
top1_score    0.811842
top3_score    0.830268
top5_score    0.832356
dtype: float64
valid_loss:  2.8204077137841117
Learning rate after epoch 8: [2.3739063552401983e-05]
Epoch 9/20: 100%|██████████| 4500/4500 [00:57<00:00, 77.69batch/s]

Model validation...
100%|██████████| 1125/1125 [00:10<00:00, 102.28batch/s]
top1_score    0.816984
top3_score    0.833817
top5_score    0.836187
dtype: float64
valid_loss:  2.8150292856428356
Learning rate after epoch 9: [2.3739063552401983e-05]
Epoch 10/20: 100%|██████████| 4500/4500 [00:57<00:00, 78.58batch/s]

Model validation...
100%|██████████| 1125/1125 [00:10<00:00, 102.67batch/s]
top1_score    0.825727
top3_score    0.843612
top5_score    0.845670
dtype: float64
valid_loss:  2.8067973018222383
Learning rate after epoch 10: [2.3739063552401983e-05]
Epoch 11/20: 100%|██████████| 4500/4500 [00:57<00:00, 78.16batch/s]

Model validation...
100%|██████████| 1125/1125 [00:10<00:00, 102.43batch/s]
top1_score    0.828727
top3_score    0.846145
top5_score    0.848428
dtype: float64
valid_loss:  2.8036085414886474
Learning rate after epoch 11: [2.3739063552401983e-05]
Epoch 12/20: 100%|██████████| 4500/4500 [00:57<00:00, 77.91batch/s]

Model validation...
100%|██████████| 1125/1125 [00:11<00:00, 102.10batch/s]
top1_score    0.828738
top3_score    0.844893
top5_score    0.846601
dtype: float64
valid_loss:  2.80281226878696
Learning rate after epoch 12: [2.3739063552401983e-05]
Epoch 13/20: 100%|██████████| 4500/4500 [00:57<00:00, 78.14batch/s]

Model validation...
100%|██████████| 1125/1125 [00:11<00:00, 102.09batch/s]
top1_score    0.831635
top3_score    0.847618
top5_score    0.849373
dtype: float64
valid_loss:  2.8002297700246177
Learning rate after epoch 13: [2.3739063552401983e-05]
Epoch 14/20: 100%|██████████| 4500/4500 [00:57<00:00, 77.96batch/s]

Model validation...
100%|██████████| 1125/1125 [00:10<00:00, 102.47batch/s]
top1_score    0.831960
top3_score    0.846806
top5_score    0.848424
dtype: float64
valid_loss:  2.7998051535288493
Learning rate after epoch 14: [2.3739063552401983e-05]
Epoch 15/20: 100%|██████████| 4500/4500 [00:57<00:00, 78.01batch/s]

Model validation...
100%|██████████| 1125/1125 [00:11<00:00, 101.24batch/s]
top1_score    0.827864
top3_score    0.842888
top5_score    0.845210
dtype: float64
valid_loss:  2.804597114562988
Learning rate after epoch 15: [2.3739063552401983e-05]
Epoch 16/20: 100%|██████████| 4500/4500 [00:57<00:00, 77.60batch/s]

Model validation...
100%|██████████| 1125/1125 [00:10<00:00, 102.50batch/s]
top1_score    0.834313
top3_score    0.849415
top5_score    0.850928
dtype: float64
valid_loss:  2.7971760453118217
Learning rate after epoch 16: [2.3739063552401983e-05]
Epoch 17/20: 100%|██████████| 4500/4500 [00:57<00:00, 77.74batch/s]

Model validation...
100%|██████████| 1125/1125 [00:10<00:00, 103.20batch/s]
top1_score    0.829514
top3_score    0.844308
top5_score    0.846008
dtype: float64
valid_loss:  2.8022310517628988
Learning rate after epoch 17: [2.3739063552401983e-05]
Epoch 18/20: 100%|██████████| 4500/4500 [00:57<00:00, 77.78batch/s]

Model validation...
100%|██████████| 1125/1125 [00:10<00:00, 102.73batch/s]
top1_score    0.830277
top3_score    0.846309
top5_score    0.847625
dtype: float64
valid_loss:  2.8008925274742973
Learning rate after epoch 18: [2.3739063552401983e-05]
Epoch 19/20: 100%|██████████| 4500/4500 [00:57<00:00, 77.83batch/s]

Model validation...
100%|██████████| 1125/1125 [00:10<00:00, 102.35batch/s]
top1_score    0.828303
top3_score    0.843446
top5_score    0.844993
dtype: float64
valid_loss:  2.8029984878963896
Learning rate after epoch 19: [2.3739063552401983e-05]
Epoch 20/20: 100%|██████████| 4500/4500 [00:57<00:00, 78.20batch/s]

Model validation...
100%|██████████| 1125/1125 [00:10<00:00, 102.73batch/s]
[I 2025-06-25 05:53:55,533] Trial 48 finished with value: 0.8343128561973572 and parameters: {'linear_out_1': 256, 'linear_out_2': 256, 'sched_factor': 0.2296563865476144, 'lr': 2.3739063552401983e-05, 'weight_decay': 0.000334104816865929, 'batch_size': 32, 'sched_patience': 4, 'sched_min_lr': 1e-05, 'a1': 0.37108006305163405, 'a2': 0.5017304181653608, 'a3': 0.6917290051750025}. Best is trial 43 with value: 0.8590327501296997.
/home/c611/.local/lib/python3.11/site-packages/optuna/trial/_trial.py:682: RuntimeWarning: Inconsistent parameter values for distribution with name "sched_factor"! This might be a configuration mistake. Optuna allows to call the same distribution with the same name more than once in a trial. When the parameter values are inconsistent optuna only uses the values of the first call and ignores all following. Using these values: {'step': None, 'low': 0.0, 'high': 0.4, 'log': False}
  warnings.warn(
top1_score    0.832965
top3_score    0.847602
top5_score    0.849440
dtype: float64
valid_loss:  2.7980393322838677
Learning rate after epoch 20: [2.3739063552401983e-05]
Training finished!
{'lr': 8.098222289457243e-05, 'weight_decay': 0.0010550241757845906, 'num_epochs': 20, 'batch_size': 64, 'only_train_classifier': False, 'sched_patience': 10, 'sched_factor': 0.18234858444406593, 'sched_min_lr': 1e-06, 'train_with_CombinedClassifier': True, 'probabilities': [0.5411795062960131, 0.7334847552515049, 0.6238395212062005], 'config_CombinedClassifier': {'linear_out_1': 256, 'linear_out_2': 128, 'dropout': 0.18234858444406593}}


-------------- Start New Training: ---------------


Epoch 1/20: 100%|██████████| 2250/2250 [00:43<00:00, 51.26batch/s]

Model validation...
100%|██████████| 563/563 [00:09<00:00, 59.31batch/s]
top1_score    0.749407
top3_score    0.761766
top5_score    0.763262
dtype: float64
valid_loss:  2.8843746985677714
Learning rate after epoch 1: [8.098222289457243e-05]
Epoch 2/20: 100%|██████████| 2250/2250 [00:43<00:00, 51.33batch/s]

Model validation...
100%|██████████| 563/563 [00:09<00:00, 59.46batch/s]
top1_score    0.763498
top3_score    0.776532
top5_score    0.777909
dtype: float64
valid_loss:  2.8688495654619395
Learning rate after epoch 2: [8.098222289457243e-05]
Epoch 3/20: 100%|██████████| 2250/2250 [00:43<00:00, 51.58batch/s]

Model validation...
100%|██████████| 563/563 [00:09<00:00, 58.79batch/s]
[I 2025-06-25 05:56:35,739] Trial 49 pruned.
/home/c611/.local/lib/python3.11/site-packages/optuna/trial/_trial.py:682: RuntimeWarning: Inconsistent parameter values for distribution with name "sched_factor"! This might be a configuration mistake. Optuna allows to call the same distribution with the same name more than once in a trial. When the parameter values are inconsistent optuna only uses the values of the first call and ignores all following. Using these values: {'step': None, 'low': 0.0, 'high': 0.4, 'log': False}
  warnings.warn(
top1_score    0.768176
top3_score    0.787940
top5_score    0.789236
dtype: float64
valid_loss:  2.863353301957911
Learning rate after epoch 3: [8.098222289457243e-05]
{'lr': 6.12366365284977e-05, 'weight_decay': 0.0013677246552371135, 'num_epochs': 20, 'batch_size': 32, 'only_train_classifier': False, 'sched_patience': 5, 'sched_factor': 0.2803748751161641, 'sched_min_lr': 1e-05, 'train_with_CombinedClassifier': True, 'probabilities': [0.763588495627614, 0.8176366324284112, 0.8316135829022383], 'config_CombinedClassifier': {'linear_out_1': 256, 'linear_out_2': 64, 'dropout': 0.2803748751161641}}


-------------- Start New Training: ---------------


Epoch 1/20: 100%|██████████| 4500/4500 [00:57<00:00, 78.14batch/s]

Model validation...
100%|██████████| 1125/1125 [00:10<00:00, 102.85batch/s]
top1_score    0.703372
top3_score    0.716710
top5_score    0.718498
dtype: float64
valid_loss:  2.9291279364691842
Learning rate after epoch 1: [6.12366365284977e-05]
Epoch 2/20: 100%|██████████| 4500/4500 [00:57<00:00, 78.37batch/s]

Model validation...
100%|██████████| 1125/1125 [00:11<00:00, 99.19batch/s]
top1_score    0.710013
top3_score    0.721164
top5_score    0.722582
dtype: float64
valid_loss:  2.9207505486806236
Learning rate after epoch 2: [6.12366365284977e-05]
Epoch 3/20: 100%|██████████| 4500/4500 [00:57<00:00, 78.24batch/s]

Model validation...
100%|██████████| 1125/1125 [00:11<00:00, 102.02batch/s]
[I 2025-06-25 06:00:01,887] Trial 50 pruned.
/home/c611/.local/lib/python3.11/site-packages/optuna/trial/_trial.py:682: RuntimeWarning: Inconsistent parameter values for distribution with name "sched_factor"! This might be a configuration mistake. Optuna allows to call the same distribution with the same name more than once in a trial. When the parameter values are inconsistent optuna only uses the values of the first call and ignores all following. Using these values: {'step': None, 'low': 0.0, 'high': 0.4, 'log': False}
  warnings.warn(
top1_score    0.705611
top3_score    0.716604
top5_score    0.717957
dtype: float64
valid_loss:  2.9248631161583796
Learning rate after epoch 3: [6.12366365284977e-05]
{'lr': 1.0681181064635099e-05, 'weight_decay': 0.0004459501940989067, 'num_epochs': 20, 'batch_size': 32, 'only_train_classifier': False, 'sched_patience': 5, 'sched_factor': 0.14618872277385508, 'sched_min_lr': 1e-05, 'train_with_CombinedClassifier': True, 'probabilities': [0.6500992957401903, 0.6602289323585269, 0.515812299998021], 'config_CombinedClassifier': {'linear_out_1': 512, 'linear_out_2': 256, 'dropout': 0.14618872277385508}}


-------------- Start New Training: ---------------


Epoch 1/20: 100%|██████████| 4500/4500 [00:57<00:00, 78.36batch/s]

Model validation...
100%|██████████| 1125/1125 [00:10<00:00, 102.41batch/s]
top1_score    0.666849
top3_score    0.690984
top5_score    0.696189
dtype: float64
valid_loss:  2.9754342890845407
Learning rate after epoch 1: [1.0681181064635099e-05]
Epoch 2/20: 100%|██████████| 4500/4500 [00:57<00:00, 78.10batch/s]

Model validation...
100%|██████████| 1125/1125 [00:10<00:00, 103.46batch/s]
top1_score    0.743056
top3_score    0.766122
top5_score    0.768706
dtype: float64
valid_loss:  2.9000322250790065
Learning rate after epoch 2: [1.0681181064635099e-05]
Epoch 3/20: 100%|██████████| 4500/4500 [00:57<00:00, 78.72batch/s]

Model validation...
100%|██████████| 1125/1125 [00:10<00:00, 102.38batch/s]
[I 2025-06-25 06:03:27,260] Trial 51 pruned.
/home/c611/.local/lib/python3.11/site-packages/optuna/trial/_trial.py:682: RuntimeWarning: Inconsistent parameter values for distribution with name "sched_factor"! This might be a configuration mistake. Optuna allows to call the same distribution with the same name more than once in a trial. When the parameter values are inconsistent optuna only uses the values of the first call and ignores all following. Using these values: {'step': None, 'low': 0.0, 'high': 0.4, 'log': False}
  warnings.warn(
top1_score    0.761025
top3_score    0.774618
top5_score    0.776851
dtype: float64
valid_loss:  2.876138410144382
Learning rate after epoch 3: [1.0681181064635099e-05]
{'lr': 1.4151785944470403e-05, 'weight_decay': 0.0005704150126605028, 'num_epochs': 20, 'batch_size': 32, 'only_train_classifier': False, 'sched_patience': 6, 'sched_factor': 0.16200081086308543, 'sched_min_lr': 1e-05, 'train_with_CombinedClassifier': True, 'probabilities': [0.22281176656927992, 0.8592002942068719, 0.6785814467938439], 'config_CombinedClassifier': {'linear_out_1': 256, 'linear_out_2': 256, 'dropout': 0.16200081086308543}}


-------------- Start New Training: ---------------


Epoch 1/20: 100%|██████████| 4500/4500 [00:57<00:00, 77.96batch/s]

Model validation...
100%|██████████| 1125/1125 [00:11<00:00, 101.40batch/s]
top1_score    0.732627
top3_score    0.749025
top5_score    0.751696
dtype: float64
valid_loss:  2.9043137474060057
Learning rate after epoch 1: [1.4151785944470403e-05]
Epoch 2/20: 100%|██████████| 4500/4500 [00:57<00:00, 78.27batch/s]

Model validation...
100%|██████████| 1125/1125 [00:10<00:00, 102.56batch/s]
top1_score    0.761444
top3_score    0.778845
top5_score    0.781064
dtype: float64
valid_loss:  2.8729313935173884
Learning rate after epoch 2: [1.4151785944470403e-05]
Epoch 3/20: 100%|██████████| 4500/4500 [00:58<00:00, 77.44batch/s]

Model validation...
100%|██████████| 1125/1125 [00:10<00:00, 102.96batch/s]
top1_score    0.787677
top3_score    0.803568
top5_score    0.805606
dtype: float64
valid_loss:  2.846668887880113
Learning rate after epoch 3: [1.4151785944470403e-05]
Epoch 4/20: 100%|██████████| 4500/4500 [00:57<00:00, 77.97batch/s]

Model validation...
100%|██████████| 1125/1125 [00:11<00:00, 100.90batch/s]
top1_score    0.790317
top3_score    0.805350
top5_score    0.807349
dtype: float64
valid_loss:  2.8435083257887097
Learning rate after epoch 4: [1.4151785944470403e-05]
Epoch 5/20: 100%|██████████| 4500/4500 [00:58<00:00, 77.25batch/s]

Model validation...
100%|██████████| 1125/1125 [00:10<00:00, 103.56batch/s]
top1_score    0.791586
top3_score    0.804930
top5_score    0.806384
dtype: float64
valid_loss:  2.8417663483089872
Learning rate after epoch 5: [1.4151785944470403e-05]
Epoch 6/20: 100%|██████████| 4500/4500 [00:57<00:00, 78.20batch/s]

Model validation...
100%|██████████| 1125/1125 [00:10<00:00, 103.87batch/s]
[I 2025-06-25 06:10:20,351] Trial 52 pruned.
/home/c611/.local/lib/python3.11/site-packages/optuna/trial/_trial.py:682: RuntimeWarning: Inconsistent parameter values for distribution with name "sched_factor"! This might be a configuration mistake. Optuna allows to call the same distribution with the same name more than once in a trial. When the parameter values are inconsistent optuna only uses the values of the first call and ignores all following. Using these values: {'step': None, 'low': 0.0, 'high': 0.4, 'log': False}
  warnings.warn(
top1_score    0.794972
top3_score    0.807066
top5_score    0.808459
dtype: float64
valid_loss:  2.837590806325277
Learning rate after epoch 6: [1.4151785944470403e-05]
{'lr': 1.458597291055656e-05, 'weight_decay': 0.0003024303569242632, 'num_epochs': 20, 'batch_size': 32, 'only_train_classifier': False, 'sched_patience': 5, 'sched_factor': 0.23071051762307093, 'sched_min_lr': 1e-05, 'train_with_CombinedClassifier': True, 'probabilities': [0.5161909566685907, 0.816599312322982, 0.7261816337699887], 'config_CombinedClassifier': {'linear_out_1': 256, 'linear_out_2': 256, 'dropout': 0.23071051762307093}}


-------------- Start New Training: ---------------


Epoch 1/20: 100%|██████████| 4500/4500 [00:57<00:00, 77.85batch/s]

Model validation...
100%|██████████| 1125/1125 [00:10<00:00, 102.54batch/s]
top1_score    0.749279
top3_score    0.770030
top5_score    0.773206
dtype: float64
valid_loss:  2.889226344850328
Learning rate after epoch 1: [1.458597291055656e-05]
Epoch 2/20: 100%|██████████| 4500/4500 [00:57<00:00, 77.72batch/s]

Model validation...
100%|██████████| 1125/1125 [00:10<00:00, 102.37batch/s]
top1_score    0.758405
top3_score    0.773290
top5_score    0.774779
dtype: float64
valid_loss:  2.8760537348853217
Learning rate after epoch 2: [1.458597291055656e-05]
Epoch 3/20: 100%|██████████| 4500/4500 [00:57<00:00, 77.95batch/s]

Model validation...
100%|██████████| 1125/1125 [00:10<00:00, 103.45batch/s]
[I 2025-06-25 06:13:46,929] Trial 53 pruned.
/home/c611/.local/lib/python3.11/site-packages/optuna/trial/_trial.py:682: RuntimeWarning: Inconsistent parameter values for distribution with name "sched_factor"! This might be a configuration mistake. Optuna allows to call the same distribution with the same name more than once in a trial. When the parameter values are inconsistent optuna only uses the values of the first call and ignores all following. Using these values: {'step': None, 'low': 0.0, 'high': 0.4, 'log': False}
  warnings.warn(
top1_score    0.760530
top3_score    0.773408
top5_score    0.774701
dtype: float64
valid_loss:  2.872451823764377
Learning rate after epoch 3: [1.458597291055656e-05]
{'lr': 1.0274174842147042e-05, 'weight_decay': 0.009898604605700677, 'num_epochs': 20, 'batch_size': 32, 'only_train_classifier': False, 'sched_patience': 4, 'sched_factor': 0.20488260952688053, 'sched_min_lr': 1e-05, 'train_with_CombinedClassifier': True, 'probabilities': [0.41521510310285803, 0.1560138290450186, 0.36744006582657673], 'config_CombinedClassifier': {'linear_out_1': 512, 'linear_out_2': 256, 'dropout': 0.20488260952688053}}


-------------- Start New Training: ---------------


Epoch 1/20: 100%|██████████| 4500/4500 [00:58<00:00, 77.43batch/s]

Model validation...
100%|██████████| 1125/1125 [00:10<00:00, 102.75batch/s]
top1_score    0.715658
top3_score    0.733262
top5_score    0.735698
dtype: float64
valid_loss:  2.9208550832536484
Learning rate after epoch 1: [1.0274174842147042e-05]
Epoch 2/20: 100%|██████████| 4500/4500 [00:58<00:00, 77.58batch/s]

Model validation...
100%|██████████| 1125/1125 [00:10<00:00, 102.82batch/s]
top1_score    0.722538
top3_score    0.735100
top5_score    0.737039
dtype: float64
valid_loss:  2.9095308871799044
Learning rate after epoch 2: [1.0274174842147042e-05]
Epoch 3/20: 100%|██████████| 4500/4500 [00:58<00:00, 77.50batch/s]

Model validation...
100%|██████████| 1125/1125 [00:10<00:00, 103.14batch/s]
[I 2025-06-25 06:17:14,219] Trial 54 pruned.
/home/c611/.local/lib/python3.11/site-packages/optuna/trial/_trial.py:682: RuntimeWarning: Inconsistent parameter values for distribution with name "sched_factor"! This might be a configuration mistake. Optuna allows to call the same distribution with the same name more than once in a trial. When the parameter values are inconsistent optuna only uses the values of the first call and ignores all following. Using these values: {'step': None, 'low': 0.0, 'high': 0.4, 'log': False}
  warnings.warn(
top1_score    0.765429
top3_score    0.782028
top5_score    0.784215
dtype: float64
valid_loss:  2.869011566162109
Learning rate after epoch 3: [1.0274174842147042e-05]
{'lr': 2.2945849648019294e-05, 'weight_decay': 0.0008948831112180708, 'num_epochs': 20, 'batch_size': 64, 'only_train_classifier': False, 'sched_patience': 6, 'sched_factor': 0.18000870776719316, 'sched_min_lr': 1e-05, 'train_with_CombinedClassifier': True, 'probabilities': [0.1414971124395532, 0.9592795768199591, 0.5803988312458033], 'config_CombinedClassifier': {'linear_out_1': 256, 'linear_out_2': 256, 'dropout': 0.18000870776719316}}


-------------- Start New Training: ---------------


Epoch 1/20: 100%|██████████| 2250/2250 [00:43<00:00, 51.63batch/s]

Model validation...
100%|██████████| 563/563 [00:09<00:00, 59.47batch/s]
top1_score    0.735516
top3_score    0.757477
top5_score    0.759656
dtype: float64
valid_loss:  2.9008154327026694
Learning rate after epoch 1: [2.2945849648019294e-05]
Epoch 2/20: 100%|██████████| 2250/2250 [00:43<00:00, 51.47batch/s]

Model validation...
100%|██████████| 563/563 [00:09<00:00, 59.27batch/s]
top1_score    0.773607
top3_score    0.794153
top5_score    0.795899
dtype: float64
valid_loss:  2.86101084238259
Learning rate after epoch 2: [2.2945849648019294e-05]
Epoch 3/20: 100%|██████████| 2250/2250 [00:43<00:00, 51.39batch/s]

Model validation...
100%|██████████| 563/563 [00:09<00:00, 59.32batch/s]
top1_score    0.773405
top3_score    0.790716
top5_score    0.792627
dtype: float64
valid_loss:  2.860258840964065
Learning rate after epoch 3: [2.2945849648019294e-05]
Epoch 4/20: 100%|██████████| 2250/2250 [00:43<00:00, 51.27batch/s]

Model validation...
100%|██████████| 563/563 [00:09<00:00, 58.93batch/s]
top1_score    0.791810
top3_score    0.810199
top5_score    0.812391
dtype: float64
valid_loss:  2.842723429732382
Learning rate after epoch 4: [2.2945849648019294e-05]
Epoch 5/20: 100%|██████████| 2250/2250 [00:43<00:00, 51.16batch/s]

Model validation...
100%|██████████| 563/563 [00:09<00:00, 59.45batch/s]
top1_score    0.808766
top3_score    0.830868
top5_score    0.832932
dtype: float64
valid_loss:  2.825035923539428
Learning rate after epoch 5: [2.2945849648019294e-05]
Epoch 6/20: 100%|██████████| 2250/2250 [00:43<00:00, 51.91batch/s]

Model validation...
100%|██████████| 563/563 [00:09<00:00, 59.22batch/s]
top1_score    0.810840
top3_score    0.830247
top5_score    0.832382
dtype: float64
valid_loss:  2.8233964252641317
Learning rate after epoch 6: [2.2945849648019294e-05]
Epoch 7/20: 100%|██████████| 2250/2250 [00:43<00:00, 51.32batch/s]

Model validation...
100%|██████████| 563/563 [00:09<00:00, 59.67batch/s]
top1_score    0.814404
top3_score    0.834277
top5_score    0.836892
dtype: float64
valid_loss:  2.818977230607299
Learning rate after epoch 7: [2.2945849648019294e-05]
Epoch 8/20: 100%|██████████| 2250/2250 [00:43<00:00, 51.36batch/s]

Model validation...
100%|██████████| 563/563 [00:09<00:00, 59.33batch/s]
top1_score    0.821971
top3_score    0.839485
top5_score    0.842314
dtype: float64
valid_loss:  2.8129744085071353
Learning rate after epoch 8: [2.2945849648019294e-05]
Epoch 9/20: 100%|██████████| 2250/2250 [00:43<00:00, 51.24batch/s]

Model validation...
100%|██████████| 563/563 [00:09<00:00, 58.72batch/s]
top1_score    0.827551
top3_score    0.844387
top5_score    0.846809
dtype: float64
valid_loss:  2.8063663428460726
Learning rate after epoch 9: [2.2945849648019294e-05]
Epoch 10/20: 100%|██████████| 2250/2250 [00:43<00:00, 51.35batch/s]

Model validation...
100%|██████████| 563/563 [00:09<00:00, 59.01batch/s]
top1_score    0.841606
top3_score    0.859463
top5_score    0.861546
dtype: float64
valid_loss:  2.793092778055113
Learning rate after epoch 10: [2.2945849648019294e-05]
Epoch 11/20: 100%|██████████| 2250/2250 [00:43<00:00, 51.39batch/s]

Model validation...
100%|██████████| 563/563 [00:09<00:00, 59.37batch/s]
top1_score    0.840516
top3_score    0.859412
top5_score    0.861589
dtype: float64
valid_loss:  2.793580857305713
Learning rate after epoch 11: [2.2945849648019294e-05]
Epoch 12/20: 100%|██████████| 2250/2250 [00:43<00:00, 51.29batch/s]

Model validation...
100%|██████████| 563/563 [00:09<00:00, 59.38batch/s]
top1_score    0.838403
top3_score    0.856961
top5_score    0.859299
dtype: float64
valid_loss:  2.795013849095807
Learning rate after epoch 12: [2.2945849648019294e-05]
Epoch 13/20: 100%|██████████| 2250/2250 [00:43<00:00, 51.61batch/s]

Model validation...
100%|██████████| 563/563 [00:09<00:00, 58.94batch/s]
top1_score    0.844953
top3_score    0.861380
top5_score    0.862763
dtype: float64
valid_loss:  2.788812924024267
Learning rate after epoch 13: [2.2945849648019294e-05]
Epoch 14/20: 100%|██████████| 2250/2250 [00:43<00:00, 51.57batch/s]

Model validation...
100%|██████████| 563/563 [00:09<00:00, 59.06batch/s]
top1_score    0.844850
top3_score    0.860291
top5_score    0.862239
dtype: float64
valid_loss:  2.7883732174895375
Learning rate after epoch 14: [2.2945849648019294e-05]
Epoch 15/20: 100%|██████████| 2250/2250 [00:43<00:00, 51.22batch/s]

Model validation...
100%|██████████| 563/563 [00:09<00:00, 59.26batch/s]
top1_score    0.844673
top3_score    0.860162
top5_score    0.861864
dtype: float64
valid_loss:  2.788524990183422
Learning rate after epoch 15: [2.2945849648019294e-05]
Epoch 16/20: 100%|██████████| 2250/2250 [00:43<00:00, 51.48batch/s]

Model validation...
100%|██████████| 563/563 [00:09<00:00, 59.46batch/s]
top1_score    0.844628
top3_score    0.859777
top5_score    0.861676
dtype: float64
valid_loss:  2.7883620312963457
Learning rate after epoch 16: [2.2945849648019294e-05]
Epoch 17/20: 100%|██████████| 2250/2250 [00:43<00:00, 51.36batch/s]

Model validation...
100%|██████████| 563/563 [00:09<00:00, 59.31batch/s]
top1_score    0.846639
top3_score    0.862021
top5_score    0.863778
dtype: float64
valid_loss:  2.7862096832234737
Learning rate after epoch 17: [2.2945849648019294e-05]
Epoch 18/20: 100%|██████████| 2250/2250 [00:43<00:00, 51.50batch/s]

Model validation...
100%|██████████| 563/563 [00:09<00:00, 58.73batch/s]
top1_score    0.842142
top3_score    0.857432
top5_score    0.858938
dtype: float64
valid_loss:  2.791035968499328
Learning rate after epoch 18: [2.2945849648019294e-05]
Epoch 19/20: 100%|██████████| 2250/2250 [00:43<00:00, 51.24batch/s]

Model validation...
100%|██████████| 563/563 [00:09<00:00, 59.33batch/s]
top1_score    0.840740
top3_score    0.855615
top5_score    0.857671
dtype: float64
valid_loss:  2.7921852819873005
Learning rate after epoch 19: [2.2945849648019294e-05]
Epoch 20/20: 100%|██████████| 2250/2250 [00:43<00:00, 51.59batch/s]

Model validation...
100%|██████████| 563/563 [00:09<00:00, 59.39batch/s]
[I 2025-06-25 06:35:00,345] Trial 55 finished with value: 0.8466387391090393 and parameters: {'linear_out_1': 256, 'linear_out_2': 256, 'sched_factor': 0.18000870776719316, 'lr': 2.2945849648019294e-05, 'weight_decay': 0.0008948831112180708, 'batch_size': 64, 'sched_patience': 6, 'sched_min_lr': 1e-05, 'a1': 0.1414971124395532, 'a2': 0.9592795768199591, 'a3': 0.5803988312458033}. Best is trial 43 with value: 0.8590327501296997.
/home/c611/.local/lib/python3.11/site-packages/optuna/trial/_trial.py:682: RuntimeWarning: Inconsistent parameter values for distribution with name "sched_factor"! This might be a configuration mistake. Optuna allows to call the same distribution with the same name more than once in a trial. When the parameter values are inconsistent optuna only uses the values of the first call and ignores all following. Using these values: {'step': None, 'low': 0.0, 'high': 0.4, 'log': False}
  warnings.warn(
top1_score    0.844524
top3_score    0.861026
top5_score    0.862731
dtype: float64
valid_loss:  2.7876570292518577
Learning rate after epoch 20: [2.2945849648019294e-05]
Training finished!
{'lr': 3.023618863693667e-05, 'weight_decay': 0.0023817647166974906, 'num_epochs': 20, 'batch_size': 64, 'only_train_classifier': False, 'sched_patience': 5, 'sched_factor': 0.1793138246283284, 'sched_min_lr': 1e-06, 'train_with_CombinedClassifier': True, 'probabilities': [0.32813867067084557, 0.9564789632632603, 0.5637728613785387], 'config_CombinedClassifier': {'linear_out_1': 256, 'linear_out_2': 256, 'dropout': 0.1793138246283284}}


-------------- Start New Training: ---------------


Epoch 1/20: 100%|██████████| 2250/2250 [00:43<00:00, 51.47batch/s]

Model validation...
100%|██████████| 563/563 [00:09<00:00, 59.21batch/s]
top1_score    0.761901
top3_score    0.787800
top5_score    0.790309
dtype: float64
valid_loss:  2.874133722820248
Learning rate after epoch 1: [3.023618863693667e-05]
Epoch 2/20: 100%|██████████| 2250/2250 [00:43<00:00, 51.33batch/s]

Model validation...
100%|██████████| 563/563 [00:09<00:00, 59.39batch/s]
top1_score    0.766981
top3_score    0.788319
top5_score    0.790747
dtype: float64
valid_loss:  2.866484426901565
Learning rate after epoch 2: [3.023618863693667e-05]
Epoch 3/20: 100%|██████████| 2250/2250 [00:43<00:00, 51.51batch/s]

Model validation...
100%|██████████| 563/563 [00:09<00:00, 59.68batch/s]
top1_score    0.787979
top3_score    0.814242
top5_score    0.819174
dtype: float64
valid_loss:  2.8467349320067816
Learning rate after epoch 3: [3.023618863693667e-05]
Epoch 4/20: 100%|██████████| 2250/2250 [00:43<00:00, 51.70batch/s]

Model validation...
100%|██████████| 563/563 [00:09<00:00, 59.60batch/s]
top1_score    0.816732
top3_score    0.841048
top5_score    0.843833
dtype: float64
valid_loss:  2.816731159047589
Learning rate after epoch 4: [3.023618863693667e-05]
Epoch 5/20: 100%|██████████| 2250/2250 [00:43<00:00, 51.31batch/s]

Model validation...
100%|██████████| 563/563 [00:09<00:00, 58.87batch/s]
top1_score    0.819329
top3_score    0.841192
top5_score    0.843212
dtype: float64
valid_loss:  2.814354047046885
Learning rate after epoch 5: [3.023618863693667e-05]
Epoch 6/20: 100%|██████████| 2250/2250 [00:43<00:00, 51.69batch/s]

Model validation...
100%|██████████| 563/563 [00:09<00:00, 59.48batch/s]
top1_score    0.818600
top3_score    0.840319
top5_score    0.842579
dtype: float64
valid_loss:  2.8147949181483862
Learning rate after epoch 6: [3.023618863693667e-05]
Epoch 7/20: 100%|██████████| 2250/2250 [00:43<00:00, 51.53batch/s]

Model validation...
100%|██████████| 563/563 [00:09<00:00, 59.69batch/s]
top1_score    0.815460
top3_score    0.836627
top5_score    0.839340
dtype: float64
valid_loss:  2.817255028611178
Learning rate after epoch 7: [3.023618863693667e-05]
Epoch 8/20: 100%|██████████| 2250/2250 [00:43<00:00, 51.80batch/s]

Model validation...
100%|██████████| 563/563 [00:09<00:00, 59.54batch/s]
top1_score    0.824117
top3_score    0.843860
top5_score    0.846312
dtype: float64
valid_loss:  2.808316062229353
Learning rate after epoch 8: [3.023618863693667e-05]
Epoch 9/20: 100%|██████████| 2250/2250 [00:43<00:00, 51.85batch/s]

Model validation...
100%|██████████| 563/563 [00:09<00:00, 59.32batch/s]
top1_score    0.828438
top3_score    0.846412
top5_score    0.848370
dtype: float64
valid_loss:  2.803597395627579
Learning rate after epoch 9: [3.023618863693667e-05]
Epoch 10/20: 100%|██████████| 2250/2250 [00:43<00:00, 51.56batch/s]

Model validation...
100%|██████████| 563/563 [00:09<00:00, 59.13batch/s]
top1_score    0.824133
top3_score    0.843133
top5_score    0.845304
dtype: float64
valid_loss:  2.8080772409218877
Learning rate after epoch 10: [3.023618863693667e-05]
Epoch 11/20: 100%|██████████| 2250/2250 [00:43<00:00, 51.60batch/s]

Model validation...
100%|██████████| 563/563 [00:09<00:00, 59.28batch/s]
top1_score    0.825091
top3_score    0.844743
top5_score    0.846598
dtype: float64
valid_loss:  2.8064697830545753
Learning rate after epoch 11: [3.023618863693667e-05]
Epoch 12/20: 100%|██████████| 2250/2250 [00:43<00:00, 51.51batch/s]

Model validation...
100%|██████████| 563/563 [00:09<00:00, 59.47batch/s]
top1_score    0.831249
top3_score    0.847998
top5_score    0.850495
dtype: float64
valid_loss:  2.800638571513906
Learning rate after epoch 12: [3.023618863693667e-05]
Epoch 13/20: 100%|██████████| 2250/2250 [00:43<00:00, 51.47batch/s]

Model validation...
100%|██████████| 563/563 [00:09<00:00, 59.76batch/s]
top1_score    0.825710
top3_score    0.842488
top5_score    0.844422
dtype: float64
valid_loss:  2.8057517306631032
Learning rate after epoch 13: [3.023618863693667e-05]
Epoch 14/20: 100%|██████████| 2250/2250 [00:43<00:00, 51.51batch/s]

Model validation...
100%|██████████| 563/563 [00:09<00:00, 58.87batch/s]
top1_score    0.830947
top3_score    0.847716
top5_score    0.849719
dtype: float64
valid_loss:  2.8005243791780083
Learning rate after epoch 14: [3.023618863693667e-05]
Epoch 15/20: 100%|██████████| 2250/2250 [00:43<00:00, 51.55batch/s]

Model validation...
100%|██████████| 563/563 [00:09<00:00, 59.27batch/s]
top1_score    0.831806
top3_score    0.847842
top5_score    0.849637
dtype: float64
valid_loss:  2.79965067290921
Learning rate after epoch 15: [3.023618863693667e-05]
Epoch 16/20: 100%|██████████| 2250/2250 [00:43<00:00, 51.35batch/s]

Model validation...
100%|██████████| 563/563 [00:09<00:00, 59.43batch/s]
top1_score    0.832227
top3_score    0.849145
top5_score    0.851265
dtype: float64
valid_loss:  2.798993162744634
Learning rate after epoch 16: [3.023618863693667e-05]
Epoch 17/20: 100%|██████████| 2250/2250 [00:43<00:00, 51.49batch/s]

Model validation...
100%|██████████| 563/563 [00:09<00:00, 58.96batch/s]
top1_score    0.828611
top3_score    0.844993
top5_score    0.846589
dtype: float64
valid_loss:  2.802675614348525
Learning rate after epoch 17: [3.023618863693667e-05]
Epoch 18/20: 100%|██████████| 2250/2250 [00:43<00:00, 51.32batch/s]

Model validation...
100%|██████████| 563/563 [00:09<00:00, 59.30batch/s]
top1_score    0.827066
top3_score    0.844638
top5_score    0.846868
dtype: float64
valid_loss:  2.8041402184095094
Learning rate after epoch 18: [3.023618863693667e-05]
Epoch 19/20: 100%|██████████| 2250/2250 [00:44<00:00, 50.96batch/s]

Model validation...
100%|██████████| 563/563 [00:09<00:00, 59.04batch/s]
top1_score    0.830603
top3_score    0.845735
top5_score    0.847442
dtype: float64
valid_loss:  2.800152735007169
Learning rate after epoch 19: [3.023618863693667e-05]
Epoch 20/20: 100%|██████████| 2250/2250 [00:43<00:00, 51.33batch/s]

Model validation...
100%|██████████| 563/563 [00:09<00:00, 58.88batch/s]
[I 2025-06-25 06:52:44,958] Trial 56 finished with value: 0.8322265148162842 and parameters: {'linear_out_1': 256, 'linear_out_2': 256, 'sched_factor': 0.1793138246283284, 'lr': 3.023618863693667e-05, 'weight_decay': 0.0023817647166974906, 'batch_size': 64, 'sched_patience': 5, 'sched_min_lr': 1e-06, 'a1': 0.32813867067084557, 'a2': 0.9564789632632603, 'a3': 0.5637728613785387}. Best is trial 43 with value: 0.8590327501296997.
/home/c611/.local/lib/python3.11/site-packages/optuna/trial/_trial.py:682: RuntimeWarning: Inconsistent parameter values for distribution with name "sched_factor"! This might be a configuration mistake. Optuna allows to call the same distribution with the same name more than once in a trial. When the parameter values are inconsistent optuna only uses the values of the first call and ignores all following. Using these values: {'step': None, 'low': 0.0, 'high': 0.4, 'log': False}
  warnings.warn(
top1_score    0.824064
top3_score    0.839914
top5_score    0.841337
dtype: float64
valid_loss:  2.8073605178304506
Learning rate after epoch 20: [3.023618863693667e-05]
Training finished!
{'lr': 0.0002963355844206341, 'weight_decay': 0.0033413587887767835, 'num_epochs': 20, 'batch_size': 64, 'only_train_classifier': False, 'sched_patience': 3, 'sched_factor': 0.10697804328366399, 'sched_min_lr': 1e-05, 'train_with_CombinedClassifier': True, 'probabilities': [0.14685481469961506, 0.9623775281851903, 0.6427359930041049], 'config_CombinedClassifier': {'linear_out_1': 256, 'linear_out_2': 64, 'dropout': 0.10697804328366399}}


-------------- Start New Training: ---------------


Epoch 1/20: 100%|██████████| 2250/2250 [00:43<00:00, 51.17batch/s]

Model validation...
100%|██████████| 563/563 [00:09<00:00, 58.72batch/s]
top1_score    0.715245
top3_score    0.729256
top5_score    0.730975
dtype: float64
valid_loss:  2.917794455431706
Learning rate after epoch 1: [0.0002963355844206341]
Epoch 2/20: 100%|██████████| 2250/2250 [00:43<00:00, 51.31batch/s]

Model validation...
100%|██████████| 563/563 [00:09<00:00, 59.01batch/s]
top1_score    0.725377
top3_score    0.743247
top5_score    0.744499
dtype: float64
valid_loss:  2.906011056307154
Learning rate after epoch 2: [0.0002963355844206341]
Epoch 3/20: 100%|██████████| 2250/2250 [00:43<00:00, 51.45batch/s]

Model validation...
100%|██████████| 563/563 [00:09<00:00, 59.23batch/s]
[I 2025-06-25 06:55:25,476] Trial 57 pruned.
/home/c611/.local/lib/python3.11/site-packages/optuna/trial/_trial.py:682: RuntimeWarning: Inconsistent parameter values for distribution with name "sched_factor"! This might be a configuration mistake. Optuna allows to call the same distribution with the same name more than once in a trial. When the parameter values are inconsistent optuna only uses the values of the first call and ignores all following. Using these values: {'step': None, 'low': 0.0, 'high': 0.4, 'log': False}
  warnings.warn(
top1_score    0.727950
top3_score    0.743616
top5_score    0.746648
dtype: float64
valid_loss:  2.903849276090388
Learning rate after epoch 3: [0.0002963355844206341]
{'lr': 4.368851353570315e-05, 'weight_decay': 0.0015534139722471023, 'num_epochs': 20, 'batch_size': 64, 'only_train_classifier': False, 'sched_patience': 4, 'sched_factor': 0.15381847779951732, 'sched_min_lr': 1e-05, 'train_with_CombinedClassifier': True, 'probabilities': [0.28229235658129603, 0.884837604299845, 0.4830114210526353], 'config_CombinedClassifier': {'linear_out_1': 256, 'linear_out_2': 128, 'dropout': 0.15381847779951732}}


-------------- Start New Training: ---------------


Epoch 1/20: 100%|██████████| 2250/2250 [00:43<00:00, 51.52batch/s]

Model validation...
100%|██████████| 563/563 [00:09<00:00, 59.74batch/s]
top1_score    0.768027
top3_score    0.793712
top5_score    0.796446
dtype: float64
valid_loss:  2.8661191713322967
Learning rate after epoch 1: [4.368851353570315e-05]
Epoch 2/20: 100%|██████████| 2250/2250 [00:43<00:00, 51.41batch/s]

Model validation...
100%|██████████| 563/563 [00:09<00:00, 58.78batch/s]
top1_score    0.777012
top3_score    0.798353
top5_score    0.800866
dtype: float64
valid_loss:  2.8557437338583425
Learning rate after epoch 2: [4.368851353570315e-05]
Epoch 3/20: 100%|██████████| 2250/2250 [00:43<00:00, 51.28batch/s]

Model validation...
100%|██████████| 563/563 [00:09<00:00, 59.34batch/s]
top1_score    0.801521
top3_score    0.828689
top5_score    0.832582
dtype: float64
valid_loss:  2.8323349169261927
Learning rate after epoch 3: [4.368851353570315e-05]
Epoch 4/20: 100%|██████████| 2250/2250 [00:43<00:00, 51.37batch/s]

Model validation...
100%|██████████| 563/563 [00:09<00:00, 59.22batch/s]
top1_score    0.811332
top3_score    0.834221
top5_score    0.837615
dtype: float64
valid_loss:  2.821239548932172
Learning rate after epoch 4: [4.368851353570315e-05]
Epoch 5/20: 100%|██████████| 2250/2250 [00:43<00:00, 51.45batch/s]

Model validation...
100%|██████████| 563/563 [00:09<00:00, 59.58batch/s]
top1_score    0.817391
top3_score    0.838921
top5_score    0.842379
dtype: float64
valid_loss:  2.815831551966828
Learning rate after epoch 5: [4.368851353570315e-05]
Epoch 6/20: 100%|██████████| 2250/2250 [00:43<00:00, 51.20batch/s]

Model validation...
100%|██████████| 563/563 [00:09<00:00, 59.16batch/s]
top1_score    0.816752
top3_score    0.838684
top5_score    0.841248
dtype: float64
valid_loss:  2.815818420735388
Learning rate after epoch 6: [4.368851353570315e-05]
Epoch 7/20: 100%|██████████| 2250/2250 [00:43<00:00, 51.32batch/s]

Model validation...
100%|██████████| 563/563 [00:09<00:00, 59.39batch/s]
top1_score    0.821238
top3_score    0.841548
top5_score    0.844679
dtype: float64
valid_loss:  2.8110094239826102
Learning rate after epoch 7: [4.368851353570315e-05]
Epoch 8/20: 100%|██████████| 2250/2250 [00:43<00:00, 51.37batch/s]

Model validation...
100%|██████████| 563/563 [00:09<00:00, 59.22batch/s]
top1_score    0.822736
top3_score    0.839865
top5_score    0.843158
dtype: float64
valid_loss:  2.8087613557202356
Learning rate after epoch 8: [4.368851353570315e-05]
Epoch 9/20: 100%|██████████| 2250/2250 [00:43<00:00, 51.66batch/s]

Model validation...
100%|██████████| 563/563 [00:09<00:00, 58.66batch/s]
top1_score    0.822385
top3_score    0.838165
top5_score    0.841726
dtype: float64
valid_loss:  2.809357248021697
Learning rate after epoch 9: [4.368851353570315e-05]
Epoch 10/20: 100%|██████████| 2250/2250 [00:43<00:00, 51.47batch/s]

Model validation...
100%|██████████| 563/563 [00:09<00:00, 59.78batch/s]
top1_score    0.823964
top3_score    0.841671
top5_score    0.844532
dtype: float64
valid_loss:  2.8072371965614984
Learning rate after epoch 10: [4.368851353570315e-05]
Epoch 11/20: 100%|██████████| 2250/2250 [00:43<00:00, 51.64batch/s]

Model validation...
100%|██████████| 563/563 [00:09<00:00, 59.72batch/s]
top1_score    0.827310
top3_score    0.845106
top5_score    0.846883
dtype: float64
valid_loss:  2.8038423391684333
Learning rate after epoch 11: [4.368851353570315e-05]
Epoch 12/20: 100%|██████████| 2250/2250 [00:43<00:00, 51.29batch/s]

Model validation...
100%|██████████| 563/563 [00:09<00:00, 59.41batch/s]
top1_score    0.823465
top3_score    0.841683
top5_score    0.843649
dtype: float64
valid_loss:  2.8079248004018944
Learning rate after epoch 12: [4.368851353570315e-05]
Epoch 13/20: 100%|██████████| 2250/2250 [00:43<00:00, 51.33batch/s]

Model validation...
100%|██████████| 563/563 [00:09<00:00, 59.60batch/s]
top1_score    0.820335
top3_score    0.837400
top5_score    0.839681
dtype: float64
valid_loss:  2.8108753556680086
Learning rate after epoch 13: [4.368851353570315e-05]
Epoch 14/20: 100%|██████████| 2250/2250 [00:43<00:00, 51.70batch/s]

Model validation...
100%|██████████| 563/563 [00:09<00:00, 59.63batch/s]
top1_score    0.826406
top3_score    0.842405
top5_score    0.844683
dtype: float64
valid_loss:  2.80490809096748
Learning rate after epoch 14: [4.368851353570315e-05]
Epoch 15/20: 100%|██████████| 2250/2250 [00:43<00:00, 51.51batch/s]

Model validation...
100%|██████████| 563/563 [00:09<00:00, 59.42batch/s]
[I 2025-06-25 07:08:44,552] Trial 58 pruned.
/home/c611/.local/lib/python3.11/site-packages/optuna/trial/_trial.py:682: RuntimeWarning: Inconsistent parameter values for distribution with name "sched_factor"! This might be a configuration mistake. Optuna allows to call the same distribution with the same name more than once in a trial. When the parameter values are inconsistent optuna only uses the values of the first call and ignores all following. Using these values: {'step': None, 'low': 0.0, 'high': 0.4, 'log': False}
  warnings.warn(
top1_score    0.821683
top3_score    0.838259
top5_score    0.840663
dtype: float64
valid_loss:  2.8088404391205755
Learning rate after epoch 15: [4.368851353570315e-05]
{'lr': 0.0004460604155981802, 'weight_decay': 0.000884460368699031, 'num_epochs': 20, 'batch_size': 64, 'only_train_classifier': False, 'sched_patience': 7, 'sched_factor': 0.13987773608306567, 'sched_min_lr': 1e-06, 'train_with_CombinedClassifier': True, 'probabilities': [0.20917525867600323, 0.768599878029451, 0.5973705973451422], 'config_CombinedClassifier': {'linear_out_1': 256, 'linear_out_2': 256, 'dropout': 0.13987773608306567}}


-------------- Start New Training: ---------------


Epoch 1/20: 100%|██████████| 2250/2250 [00:43<00:00, 51.53batch/s]

Model validation...
100%|██████████| 563/563 [00:09<00:00, 59.40batch/s]
top1_score    0.571581
top3_score    0.610768
top5_score    0.618869
dtype: float64
valid_loss:  3.0603011497172328
Learning rate after epoch 1: [0.0004460604155981802]
Epoch 2/20: 100%|██████████| 2250/2250 [00:43<00:00, 51.32batch/s]

Model validation...
100%|██████████| 563/563 [00:09<00:00, 59.18batch/s]
top1_score    0.593730
top3_score    0.620353
top5_score    0.629912
dtype: float64
valid_loss:  3.0384121267240807
Learning rate after epoch 2: [0.0004460604155981802]
Epoch 3/20: 100%|██████████| 2250/2250 [00:43<00:00, 51.51batch/s]

Model validation...
100%|██████████| 563/563 [00:09<00:00, 59.02batch/s]
[I 2025-06-25 07:11:24,584] Trial 59 pruned.
top1_score    0.651351
top3_score    0.678289
top5_score    0.684630
dtype: float64
valid_loss:  2.978207716509885
Learning rate after epoch 3: [0.0004460604155981802]
Beste Hyperparameter: {'linear_out_1': 256, 'linear_out_2': 256, 'sched_factor': 0.17352834622559538, 'lr': 1.436504053514967e-05, 'weight_decay': 0.00042474032760911497, 'batch_size': 32, 'sched_patience': 4, 'sched_min_lr': 1e-05, 'a1': 0.10573110587830598, 'a2': 0.8399009736482208, 'a3': 0.6165562109062541}
Beste Leistung (Maximale top1_prediction): 0.8590327501296997